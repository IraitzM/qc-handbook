[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantum Computing Handbook",
    "section": "",
    "text": "Preface\nYou may wonder, another book about Quantum Computing? More documentation? We already have:\nWell, by telling it from a different perspective it may help people coming from different fields (meaning, non-physicists) grasp the idea behind quantum computing. This is also a work for myself, as it will definitely help me better understand some of the concepts. Just by explaining things I found it is the best way to challenge ones knowledge about a field you are trying to master.\nAlso, it costs nothing so, no harm done. Code snippets are easier to maintain so hopefully, everything will work with the latest versions. I hope you enjoy reading it as much as I have learned creating it.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Quantum Computing Handbook",
    "section": "About me",
    "text": "About me\nMi name is Iraitz Montalb√°n, Computer Scientist by background, I started in the field of Quantum Computing by chance, while doing an internship in 2010 there were some guys doing something called quantum computing, which was a mathematical framework back then. I kept my interest in the field and ended up doing a masters on Quantum Computing Technologies at UPM and working with some startups in the field.\nFeel free to keep in touch at iraitz.info.\n\n\n\n\nNielsen, Michael A, and Isaac L Chuang. 2010. Quantum Computation and Quantum Information. Cambridge university press.\n\n\nWolf, Ronald de. 2023. ‚ÄúQuantum Computing: Lecture Notes.‚Äù https://arxiv.org/abs/1907.09415.\n\n\nYanofsky, Noson S, and Mirco A Mannucci. 2008. Quantum Computing for Computer Scientists. Cambridge University Press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Well, quantum computing. Really trendy topic. Although it is not the only unconventional computing paradigm out there:\n\nQuantum Computing\nNeuromorphic computing\nAnalog computing\nProbabilistic computing\nThermodynamic computing\nBiological computing\n\nFancy as it might be there are some really cool things we can learn from trying to challenge the status quo. It is not the first paradigm aiming to dethrone classical computing - yes, we will need to do that distinction from now on - but it is certainly the one that has progressed the most. We do have quantum computers, computers the open public can use and that are already fighting in some critical areas making people nervous about it.\nLet‚Äôs see if we can get our heads around what quantum computing is and how to work with it. Let‚Äôs start üéâ",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "parts/gettingstarted/qc-intro.html",
    "href": "parts/gettingstarted/qc-intro.html",
    "title": "Getting started",
    "section": "",
    "text": "What is quantum computing? Well, nothing but a new paradigm to perform computation. It is based on the formalism of quantum physics and even though it might be challenging to grasp at first, just some basic linear algebra might help you understanding the basics. I will try to show it by examples in Python so that it is less challenging and more hands-on.\nClassical computing also embraces physics but at a much higher abstraction level. By being able to manipulate electricity the first devices, analog devices, were able to perform first algorithms. Programming analog devices it is definitely a challenging task, so operation digitization was able to ease that task also making easier for machines to interoperate given that even though different manufacturers where involved, by adopting the same standards in terms of digital operations (time-bounded actions) it made it a lot easier to understand each other.\nBoolean algebra made this possible, providing an universal set of operations to produce the algorithms we do use on a daily basis. Being able to manufacture bits as voltages made possible to build machines capable of following that algebra and abstracting it further to higher level programming languages. The rest is history.\nThe main idea behind quantum computing is that, instead of working at a macroscopic level as we are used to, by using quantum states on some physical mean we could benefit from the effects inherent to quantum physics to perform computation in different ways. By leveraging the those effects some really complex calculations could be performed more efficiently. Don‚Äôt worry, we will be using higher level programming languages as well but we need to understand what is going on under the hood.\nSuperposition, interference and entanglement are the key to most of the speedups claimed in the literature, and we will try to understand how we can work with those in order to produce algorithms for optimization or machine learning tasks.\nHere goes a little roadmap on what has happened in the field in the last two decades:\n\n\n\n\nQC timeline\n\n\n\nThere are plenty of good references on the basic stuff so we will try to link those as well as we cover some specifics around optimization and machine learning more in detail. But let‚Äôs start with the basics: quantum states and qubits.",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "parts/gettingstarted/basics.html",
    "href": "parts/gettingstarted/basics.html",
    "title": "Basics",
    "section": "",
    "text": "Qubits and states\nThe main concept leveraged by quantum computers is the ability to act over quantum states. A quantum state is no other thing than a mathematical description of the probability distribution for the potential outcome after performing a measurement upon a system. It sounds tricky but it is simpler than it sounds, bare with me.\nA quantum computer is a device capable of manipulating quantum states in a specific manner. A quantum state is like a complete description of everything you could possibly know about a tiny physical system, a particle, such as an electron or photon, at any given moment. Think of it this way: imagine you have a coin spinning in the air. While it‚Äôs spinning, you can‚Äôt say it‚Äôs definitively heads or tails - it‚Äôs in a state of being ‚Äúboth‚Äù until it lands. A quantum state is similar, but for particles that are unimaginably small.\nThese states are represented as a normed vector (vector of length 1) as the probability of outcomes should add up to one (meaning, certainty). Let‚Äôs try with the most basic one, a system of a single unit state. The quantum version of a coin looks like:\n\\[\n|\\phi\\rangle = \\left[\n\\begin{array}{c}\n\\alpha \\\\\n\\beta\n\\end{array}\n\\right]\n\\]\nbeing this the minimum representation of a quantum state: a unit norm vector \\(\\in \\mathbb{C}^2\\) whose physical realization is known as the qubit. Qubits do not exists (like bit as a matter of fact), these concepts are pure mathematical constructions that have a specific meaning. In our case, the minimum unit we can manipulate in our quantum computer.\nIt represents the two extremes the system can be at and their probability amplitudes. It means we will observe one or the other if we measure the system and those probabilities must add up to one - \\(|\\alpha|^2+|\\beta|^2 = 1\\) - given that there is certainty that the system exists in one of those options.\nThis state can be decomposed into basic components, the two extreme cases previously mentioned or mathematically speaking, basis states. This is relevant as when it comes to quantum computers, our minimal unit will be expressed on its computational basis: \\[\n|0\\rangle = \\left[\n\\begin{array}{c}\n1 \\\\\n0\n\\end{array}\n\\right]\n\\quad\n|1\\rangle = \\left[\n\\begin{array}{c}\n0 \\\\\n1\n\\end{array}\n\\right]\n\\]\nWe use this simplified Dirac notation just to make it shorter to write, otherwise we will take much of the space writing vectors an matrices. It is funny because those two column vectors will be translated to the already known 0 and 1 bit when we read the state from a classical computer. Classical computers only work with bits so no matter what we do at a quantum level, everything will be then reduced to a set of 0s and 1s. But we will deal with this later when we talk about measurements.\nThus previous state can be described in our qubit unit as:\n\\[\n|\\phi\\rangle =\n\\alpha |0\\rangle +\n\\beta |1\\rangle = \\left[\n\\begin{array}{c}\n\\alpha \\\\\n\\beta\n\\end{array}\n\\right],\n\\]\nmeaning our quantum state‚Äôs potential outcomes are \\(|0\\rangle\\) with probability \\(|\\alpha|^2\\) and \\(|\\beta|^2\\) odds for \\(|1\\rangle\\). How do we know the actual outcome of our state? Well, we need to measure. This is done by projecting our state over one of those basis states, so the complex conjugate of previous vectors is used to know how likely each option is.\n\\[\n\\langle 0 | = \\left[\n\\begin{array}{cc}\n1 & 0\n\\end{array}\n\\right]\n\\quad\n\\langle 1 | = \\left[\n\\begin{array}{cc}\n0 & 1\n\\end{array}\n\\right]\n\\]\nBy projecting our state into this basis vectors we would obtain:\n\\[\n|\\langle 0 |\\psi\\rangle|^2 = \\left|\\left[\n\\begin{array}{cc}\n1 & 0\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n\\alpha \\\\\n\\beta\n\\end{array}\n\\right]\\right|^2 = |\\alpha|^2\n\\]\nSo we don‚Äôt actually evaluate the state, but we use a basis vector to see ‚Äúhow close‚Äù our state is to this option that is part of the basis states we are considering.\nAnd here we found one of the first strange things about qubits in comparison to bits. Being 0 and 1 the basic states of a bit, its physical realization can only be in on of those states while qubits can be partially in more than one state creating a superposition of states.",
    "crumbs": [
      "Getting started",
      "Basics"
    ]
  },
  {
    "objectID": "parts/gettingstarted/basics.html#superposition",
    "href": "parts/gettingstarted/basics.html#superposition",
    "title": "Basics",
    "section": "Superposition",
    "text": "Superposition\nThis is one of the trickiest things to get when first approaching quantum computing as it is not so intuitive from our macroscopic perspective. It is also often misunderstood, we will try to make it clear. This is an example of a superposed state\n\\[\n|\\phi\\rangle =\n\\alpha |0\\rangle +\n\\beta |1\\rangle.\n\\]\nThe state is a superposed, meaning is a coin spinning in the air and we do not know which state it will reveal when we stop it to look. Our ability to perceive it is limited by the measurement procedure itself, which also alters the condition of the system (stops the coin from spinning), but it is in a set of probabilities during the spinning that is not entirely tied to one specific option (heads or tails).\nWe need to project our quantum state into one of the two potential states we can obtain (\\(|0\\rangle\\) or \\(|1\\rangle\\)). Sadly, when measuring only classical bits can be obtained (0 or 1) into our conventional machines (like the laptop or phone you are using right now), so no probability amplitude, no phase‚Ä¶ quantum information gets lost in the measurement process and we end up with classical (boring) information.\nIn order to fully understand the state while spinning we toss the coin several times and check each time which state we get. With that we plot the outcome of probabilities in a graph. The probability histogram is created after a sequence of quantum state creations and measurements we often call SHOTS. With that we get an idea on how the coin look while spinning.\n\n\n\n\nCoin toss\n\n\n\nFor a series of measurements we have seen \\(|0\\rangle\\) state 50.6% and \\(|1\\rangle\\) state 49.4%. Definitely, looks superposed to me. It is difficult to see from were we stand, but quantum state holds a lot more information than this simple binary choice. In fact, both \\(\\alpha\\) and \\(\\beta\\) are complex numbers, that is why bloch sphere renders a more in detail expression of the state of a qubit. Sadly for us this is something we will never perceive from a real device but it does help while building the formalism or creating algorithms in our heads.\nAll posible states will be part of the surface of that sphere, this is the potential of a single qubit. Not bad.\n\n\n\n\nBloch sphere\n\n\n\nQuantum states can therefore be expressed also as if we consider the polar coordinate:¬°s\n\\[\n|\\phi\\rangle = \\cos (\\theta/2) |0\\rangle + e^{i\\phi} \\sin (\\theta/2)|1\\rangle\n\\]\nwhich represent the sphere coordinates on the surface of the bloch sphere. It is more evident in this case that rendering simple bits will actually remove most of the information (like phase) contained in the states when operating with them in the quantum regime.",
    "crumbs": [
      "Getting started",
      "Basics"
    ]
  },
  {
    "objectID": "parts/gettingstarted/basics.html#measurement",
    "href": "parts/gettingstarted/basics.html#measurement",
    "title": "Basics",
    "section": "Measurement",
    "text": "Measurement\nMeasuring quantum states is a necessary condition to extract information out of it, but by doing so we affect the system due to the collapse of the wave function. I know, too technical.\nOk, so when we observe a quantum state we can only see one of the potential outcomes, the one we observe actualy. And this affects the system, meaning when we look at it it collapses to the state we have observed and no longer exsits as a probability ditribution but as a certainty.\nThe action we use to observe the quantum state is by means of Observables (self-adjoint operators). These are used to project the measurement of a given quantum state to one of its eigenvalues. Using the closeness between quantum state and eigenvectors of the observable we can measure what is the overlap between those. The eigenvectors of the observable form an orthonormal basis for the Hilbert space, therefore each measure corresponds to a mapping to the eigenvectors comprising the basis. I know, this is a little bit too technical but just think about the observable as a pair of tinted glasses, if glasses have a green-ish color, everything will look some sort of green: dark green for black or light green for white. It is kind of like that. Kind of.\nWe will see that a common observable used is the Pauli \\(\\sigma_z\\) matrix\n\\[\n\\sigma_z = \\left[\n\\begin{array}{cc}\n1 & 0 \\\\\n0 & -1\n\\end{array}\n\\right]\n\\]\nWe can decompose it on its eigenvectors and eigenvalues so that\n\\[\n\\sigma_z|0\\rangle = 1|0\\rangle \\rightarrow \\sigma_z|1\\rangle = -1|1\\rangle.\n\\]\nTherefore, any potential measurement will be mapped to one of those two eigenvectors and its eigenvalue used as the outcome of the measurement. The basis set shown above is often known as the computational basis and it will become relevant when we move to actual implementations.\nFrom our previous state \\(|\\phi\\rangle = \\alpha |0\\rangle + \\beta |1\\rangle\\) and perform a measurement over the computational basis (\\(|0\\rangle,|1\\rangle\\)) where evidently the outcome would be \\(|0\\rangle\\) with \\(|\\alpha|^2\\) probability or \\(|1\\rangle\\) with \\(|\\beta|^2\\) probability. But more importantly, a single measurement, no matter how complex, entangled or superposed our state might be, will only be able to recover classical bits mapping two previously seen states (\\(|0\\rangle \\rightarrow 0\\) and \\(|1\\rangle \\rightarrow 1\\)).\nWhat happens after measurement is performed? Well, that is the trickiest part as the process of measuring obliges to our quantum state to position itself and make a choice. This means our quantum state after measurement will no longer be in a superposition state and will be the outcome of the previous measurement \\(|0\\rangle\\) for the remaining time period.\nIn order to fully grasp the statistics of the state we should find way to reproduce the state and perform enough measurements, so we can characterize its actual condition as much as we can out of the measurement statistics it presents. Measuring in the basis \\(\\{|0\\rangle, |1\\rangle\\}\\) (also called computational basis) may not be enough to capture the actual state. Therefore, a basis change is needed, but we will see that quantum devices only offer computational basis as the measuring basis. That means in certain cases we will need to rotate our qubits so that the basis change is artificially done.\nIf we would like to measure in the \\(\\{|+\\rangle, |-\\rangle\\}\\) basis then a rotation of \\(-\\pi / 2\\) in the Y axis will be needed. \\(\\{|i\\rangle, |-i\\rangle\\}\\) basis will require then a rotation over X axis of \\(\\pi / 2\\). That way we get to shift the reference point and measure according to our target axis.\nNext thing is to operate on those things, let‚Äôs see how gates differ from our knowledge from classical computing.",
    "crumbs": [
      "Getting started",
      "Basics"
    ]
  },
  {
    "objectID": "parts/gettingstarted/gates.html",
    "href": "parts/gettingstarted/gates.html",
    "title": "Quantum gates",
    "section": "",
    "text": "The Hall of Gates\nNow that we know how our states might look like\n\\[\n|\\phi\\rangle = \\left[\n\\begin{array}{c}\n\\alpha \\\\\n\\beta\n\\end{array}\n\\right]\n\\]\nwould be relevant to understand how we can operate on them. Quantum computers, like classical computers, use logical gates to do their work. In this case, gates are operations that take our initial state and convert it into a different quantum state. Classical regime is poor in this sense as allowed changes are only between 0 and 1, but quantum states and qubits allow for much more information to be represented. That makes quantum logical gates also more complex and rich.\n\\[\nU|\\psi\\rangle \\rightarrow |\\phi\\rangle\n\\]\nBut in this case, these gates are specific to the quantum regime. We will find some similarities like in the case of the not gate and some particular cases like the Hadamard gate.\nA quantum gate is a frame to describe the quantum process taking a quantum state at time \\(t\\) to the one at \\(t+1\\). For a closed quantum system the operator that describes the evolution of the system can be described with \\(e^{-i\\int H dt/\\hbar}\\), which is a unitary operator based on the Hamiltonian that describes the dynamics of the system. What is a Hamiltonian you ask? Well, this video may be of interest then:\nJust take into account that, if we ignore the global phase of the quantum state, we can suppose all the quantum gates to be special unitary operations, i.e., the determinant of them is 1. This is so that the obtained state falls into the surface of the bloch sphere (otherwise we enter a much more complex territory).\nAs in classical computing, some key gates need to be known. The \\(NAND\\) being an universal boolean logic gate as it can create all the other options required to render boolean logic, we should introduce some of the core gates when talking about quantum computing and the universality of those gates.",
    "crumbs": [
      "Getting started",
      "Quantum gates"
    ]
  },
  {
    "objectID": "parts/gettingstarted/gates.html#the-hall-of-gates",
    "href": "parts/gettingstarted/gates.html#the-hall-of-gates",
    "title": "Quantum gates",
    "section": "",
    "text": "NOT-gate (X)\nThe not gate (\\(X\\) as the operator gate and \\(\\sigma_x\\) if expressed as a Pauli matrix) is one of the most basic concepts in computing. A classical operation that takes the bit in a position (0 or 1) to its opposite. But, quantum not extends that basic concept to the complexity of the quantum regime. The operator can be represented by a matrix like the following one:\n\\[\nX = \\left[\n\\begin{array}{cc}\n0 & 1 \\\\\n1 & 0\n\\end{array}\n\\right]\n\\]\nThen, the operation \\(X|0\\rangle = |1\\rangle\\) can be validated by the simple multiplication of the matrix of the gate by the quantum state:\n\\[\nX|0\\rangle = \\left[\n\\begin{array}{cc}\n0 & 1 \\\\\n1 & 0\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n1 \\\\\n0\n\\end{array}\n\\right] = \\left[\n\\begin{array}{c}\n0 \\\\\n1\n\\end{array}\n\\right] = |1\\rangle\n\\]\nBut if we perform this very simple example with the generic case for \\(|\\psi\\rangle\\)\n\\[\nX|\\psi\\rangle = X(\\alpha|0\\rangle + \\beta |1\\rangle) = \\left[\n\\begin{array}{cc}\n0 & 1 \\\\\n1 & 0\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n\\alpha \\\\\n\\beta\n\\end{array}\n\\right] = \\left[\n\\begin{array}{c}\n\\beta \\\\\n\\alpha\n\\end{array}\n\\right] = \\beta|0\\rangle + \\alpha |1\\rangle = |\\phi\\rangle\n\\]\nwe have basically created a new state where its outcome probabilities have been shifted. Therefore, the \\(X\\) operation can be used to shift the state ratter than perform a mere coin flipping. This gets more interesting looking at some specific gates like the Hadamard gate.\n\n\nHadamard (H)\nIts matrix representation is\n\\[\nH = \\frac{1}{\\sqrt{2}}\\left[\n\\begin{array}{cc}\n1 & 1 \\\\\n1 & -1\n\\end{array}\n\\right]\n\\]\nAnd its effect‚Ä¶\n\\[\nH|0\\rangle = \\frac{1}{\\sqrt{2}}\\left[\n\\begin{array}{cc}\n1 & 1 \\\\\n1 & -1\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n1 \\\\\n0\n\\end{array}\n\\right] = \\frac{1}{\\sqrt{2}}\\left[\n\\begin{array}{c}\n1 \\\\\n1\n\\end{array}\n\\right] = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle = |+\\rangle\n\\]\nbasically it allows as to switch the axis of the bloch sphere and work on the superposition of the states on the \\(Z\\) axis.\n\n\n\n\nBloch sphere",
    "crumbs": [
      "Getting started",
      "Quantum gates"
    ]
  },
  {
    "objectID": "parts/gettingstarted/gates.html#rotating-quantum-states",
    "href": "parts/gettingstarted/gates.html#rotating-quantum-states",
    "title": "Quantum gates",
    "section": "Rotating quantum states",
    "text": "Rotating quantum states\nIn essence, we will see that many gates what they offer us is simply rotations over the three main axes of the bloch sphere:\n\\[\nR_x(\\theta) = \\left(\n    \\begin{array}{cc}\n        \\cos(\\frac{\\theta}{2}) & -i\\sin(\\frac{\\theta}{2}) \\\\\n        -i\\sin(\\frac{\\theta}{2}) & \\cos(\\frac{\\theta}{2})\n    \\end{array}\n    \\right)\n\\]\n\\[\nR_y(\\theta) = \\left(\n    \\begin{array}{cc}\n        \\cos(\\frac{\\theta}{2}) & -\\sin(\\frac{\\theta}{2}) \\\\\n        \\sin(\\frac{\\theta}{2}) & \\cos(\\frac{\\theta}{2})\n    \\end{array}\n    \\right)\n\\]\n\\[\nR_z(\\theta) = \\left(\n    \\begin{array}{cc}\n        e^{-i\\frac{\\theta}{2}} & 0 \\\\\n        0 & e^{i\\frac{\\theta}{2}}\n    \\end{array}\n    \\right)\n\\]\nmaking the general case \\(R_M(\\theta) = \\exp^{(‚àíi\\theta M/2)}\\) where \\(M \\in \\{X,Y,Z\\};\\) being those three the Pauli gates the Pauli matrices \\(\\sigma_x, \\sigma_y \\text{ and } \\sigma_z\\). Actually the literature will quote a more generic case which will be given by the following gate operator\n\\[\nU(\\theta, \\phi, \\lambda) = \\left( \\begin{array}{cc}\n        \\cos(\\frac{\\theta}{2}) & -e^{i\\lambda}\\sin(\\frac{\\theta}{2})\\\\\n        e^{i\\phi}\\sin(\\frac{\\theta}{2}) & e^{i(\\phi+\\lambda)}\\cos(\\frac{\\theta}{2})\n    \\end{array} \\right)\n\\]\nwhich maps to previous gates following the relationship \\(U(\\theta,0,0) = R_y(\\theta)\\), \\(U(0,0,\\lambda) = R_z(\\lambda)\\) and \\(U(\\theta,-\\frac{\\pi}{2},\\frac{\\pi}{2}) = R_x(\\theta)\\)",
    "crumbs": [
      "Getting started",
      "Quantum gates"
    ]
  },
  {
    "objectID": "parts/gettingstarted/gates.html#native-gates",
    "href": "parts/gettingstarted/gates.html#native-gates",
    "title": "Quantum gates",
    "section": "Native gates",
    "text": "Native gates\nQuantum Computing is a mathematical framework that allows for all kind of gates to be designed but in many cases depending on the manufacturer of the device the set of gates we can use may differ. We will need to find the combination of gates that will produce our desired action by using the gates each device is equipped with. Also called native gates.\nThe native gate set is the gate set, one and two-qubit gates, that should be used to translate any theoretical operator or action from the mathematical framework to be physical action that can be performed, decomposing the original action into different operations or better expressed, transpiling the circuit.\n\n\n\n\n\n\nYou won‚Äôt believe this but‚Ä¶\n\n\n\n\n\nActually, two of the most basic gates, the Hadamard and CNOT gate, do not exist. We always need to find translations for those fundamental operations when dealing with hardware.\n\n\n\nOur circuit:\n\n\n\n\nSome circuit\n\n\n\n\\(H\\) represents the Hadamard gate and the other two blue ones, the Control-NOT gate as the basis for generating entangled states.\nNo device can implement natively at least to the best of our knowledge. Therefore, in order to apply that gate own needs to apply native gates that produce the same action or unitary matrix.\nWhat can run on IBM devices if we convert it to\n\n\n\n\nTranspiled circuit\n\n\n\nWe will see this in more detail later when we review hardware providers and their actual implementation.",
    "crumbs": [
      "Getting started",
      "Quantum gates"
    ]
  },
  {
    "objectID": "parts/gettingstarted/gates.html#control-not-cnot",
    "href": "parts/gettingstarted/gates.html#control-not-cnot",
    "title": "Quantum gates",
    "section": "Control-NOT (CNOT)",
    "text": "Control-NOT (CNOT)\nOne interesting gate to be implemented in this regime is the control not or CNOT gate. This acts on the target qubit only if the first qubit is at \\(|1\\rangle\\) state. It does not require any classical measurement or observation, meaning the control qubit still holds its quantumness after gate operation. This is critical to the good functioning of the system otherwise any classical action would affect the quantum state making it purely classical.\n\\[\nCNOT|00\\rangle = \\left[\n\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\n\\right] = \\left[\n\\begin{array}{c}\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\n\\right] = |00\\rangle\n\\]\n\\[\nCNOT|10\\rangle = \\left[\n\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n0 \\\\\n0 \\\\\n1 \\\\\n0\n\\end{array}\n\\right] = \\left[\n\\begin{array}{c}\n0 \\\\\n0 \\\\\n0 \\\\\n1\n\\end{array}\n\\right] = |11\\rangle\n\\]\nBut what does there happen when it gets applied to a superposed state.\n\\[\nCNOT|+0\\rangle = \\left[\n\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n\\frac{1}{\\sqrt{2}} \\\\\n0 \\\\\n\\frac{1}{\\sqrt{2}} \\\\\n0\n\\end{array}\n\\right] = \\left[\n\\begin{array}{c}\n\\frac{1}{\\sqrt{2}} \\\\\n0 \\\\\n0 \\\\\n\\frac{1}{\\sqrt{2}}\n\\end{array}\n\\right] = \\frac{1}{\\sqrt{2}}(|00\\rangle + |11\\rangle)\n\\]\nQuite interesting. Have you tried decomposing this state into the tensor product of the quantum state of the two qubits?\nIt is fine, because it is not possible. This is what it is called an entangled state. One particular benefit of this states is that by measuring only one of those qubits we know the value of the second one without any need for measurement. We cannot act on one party without affecting the whole system, but we also know the condition of the other party by observing just one. This is what Einstein called spooky actions at a distance and are critical to the scalability of the quantum algorithms.",
    "crumbs": [
      "Getting started",
      "Quantum gates"
    ]
  },
  {
    "objectID": "parts/gettingstarted/gates.html#entanglement",
    "href": "parts/gettingstarted/gates.html#entanglement",
    "title": "Quantum gates",
    "section": "Entanglement",
    "text": "Entanglement\nEntangled states are critical for some of the quantum algorithms allowing us to outperform (theoretically at least) classical computing. The most basic (and probably best known) entangled states are the Bell states or EPR pairs.\n\\[\n|\\Phi^{+}\\rangle = \\frac{1}{\\sqrt{2}}(|00\\rangle + |11\\rangle) \\quad |\\Phi^{-}\\rangle = \\frac{1}{\\sqrt{2}}(|00\\rangle - |11\\rangle)\n\\]\n\\[\n|\\Psi^{+}\\rangle = \\frac{1}{\\sqrt{2}}(|01\\rangle + |10\\rangle) \\quad |\\Psi^{-}\\rangle = \\frac{1}{\\sqrt{2}}(|10\\rangle - |10\\rangle)\n\\]\nEntangled states are behind some of the key algorithms in communication protocols such as Quantum Teleportation or Superdense coding or behind the post-quantum cryptography proposals, like the Quantum Key Distribution. But they are also relevant for computations as we will see in the following sections.\nI know, too much theory. Let‚Äôs try to execute some examples to make it more clear.",
    "crumbs": [
      "Getting started",
      "Quantum gates"
    ]
  },
  {
    "objectID": "parts/gettingstarted/simulations.html",
    "href": "parts/gettingstarted/simulations.html",
    "title": "Simulations",
    "section": "",
    "text": "From scratch\nNow, do we have to create our own set of gates and operations? Well, it might be a good practice as the formalism can be easily reproduced by setting the basic needs:\nWe can then create our own set of functions and objects to simulate those computations:\nWe would easily create basic vector structures for our quantum framework. The minimum unit is the qubit and in order to frame the potential quantum states it may hold we would need to create the computational basis set \\(\\{|0\\rangle, |1\\rangle \\}\\).\nimport numpy as np\nfrom qiskit.visualization import array_to_latex\n\nzero = [[1], [0]]\n\narray_to_latex(array=zero, prefix='|0\\\\rangle = ', max_size=(10,10))\n\n\\[\n|0\\rangle =\n\\begin{bmatrix}\n1  \\\\\n0  \\\\\n\\end{bmatrix}\n\\]\none = [[0], [1]]\n\narray_to_latex(array=one, prefix='|1\\\\rangle = ', max_size=(10,10))\n\n\\[\n|1\\rangle =\n\\begin{bmatrix}\n0  \\\\\n1  \\\\\n\\end{bmatrix}\n\\]\nNow lets try with some gates.\n\\[\nX = \\left[\n\\begin{array}{cc}\n0 & 1 \\\\\n1 & 0\n\\end{array}\n\\right] \\quad\nH = \\frac{1}{\\sqrt{2}}\\left[\n\\begin{array}{cc}\n1 & 1 \\\\\n1 & -1\n\\end{array}\n\\right]\n\\]\nX = [[0,1],[1,0]]\n\narray_to_latex(array=X, prefix='X = ', max_size=(10,10))\n\n\\[\nX =\n\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}\n\\]\nhadamard = np.dot((1/(np.sqrt(2))), [[1, 1], [1, -1]])\n\narray_to_latex(array=hadamard, prefix='H = ', max_size=(10,10))\n\n\\[\nH =\n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}  \\\\\n\\frac{\\sqrt{2}}{2} & - \\frac{\\sqrt{2}}{2}  \\\\\n\\end{bmatrix}\n\\]\nWell, it is already taking shape. We can test if the outcome matches our expectations.\nsuperposition = np.dot(hadamard, zero)\n\narray_to_latex(array=superposition, prefix='H|0\\\\rangle = |+\\\\rangle = ', max_size=(10,10))\n\n\\[\nH|0\\rangle = |+\\rangle =\n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2}  \\\\\n\\frac{\\sqrt{2}}{2}  \\\\\n\\end{bmatrix}\n\\]\none = np.dot(X, zero)\n\narray_to_latex(array=one, prefix='X|0\\\\rangle = |1\\\\rangle = ', max_size=(10,10))\n\n\\[\nX|0\\rangle = |1\\rangle =\n\\begin{bmatrix}\n0  \\\\\n1  \\\\\n\\end{bmatrix}\n\\]\nWe can scale it to a couple of qubits to see what we get. Let‚Äôs try to create on of the bell states we saw during class.\n\\[\n|\\Phi^{+}\\rangle = \\frac{1}{\\sqrt{2}}(|00\\rangle + |11\\rangle) \\quad |\\Phi^{-}\\rangle = \\frac{1}{\\sqrt{2}}(|00\\rangle - |11\\rangle)\n\\] \\[\n|\\Psi^{+}\\rangle = \\frac{1}{\\sqrt{2}}(|01\\rangle + |10\\rangle) \\quad |\\Psi^{-}\\rangle = \\frac{1}{\\sqrt{2}}(|10\\rangle - |10\\rangle)\n\\]\nWe will need the CNOT gate for this.\nCNOT = [[1,0,0,0],[0,1,0,0],[0,0,0,1],[0,0,1,0]]\n\narray_to_latex(array=CNOT, prefix='CNOT = ', max_size=(10,10))\n\n\\[\nCNOT =\n\\begin{bmatrix}\n1 & 0 & 0 & 0  \\\\\n0 & 1 & 0 & 0  \\\\\n0 & 0 & 0 & 1  \\\\\n0 & 0 & 1 & 0  \\\\\n\\end{bmatrix}\n\\]\nWith this we will create a two qubit system, apply the Hadamard gate to the first one and the CNOT gate with the conrol over the first qubit as well.\n# Initial state\ninit_state = np.kron(zero, zero)\n\n# (I tensor Hadamard)\nHI = np.kron(hadamard, np.eye(2))\nWith that we can perform the full operation\n\\[\nCNOT (I\\otimes H)|00\\rangle = |\\Phi^{+}\\rangle = \\frac{1}{\\sqrt{2}}(|00\\rangle + |11\\rangle\n\\]\npsi_1 = np.dot(HI, init_state)\npsi = np.dot(CNOT, psi_1)\n\narray_to_latex(array=psi, prefix='|\\\\psi\\\\rangle = ', max_size=(10,10))\n\n\\[\n|\\psi\\rangle =\n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2}  \\\\\n0  \\\\\n0  \\\\\n\\frac{\\sqrt{2}}{2}  \\\\\n\\end{bmatrix}\n\\]\nThere you go. This is our entangled 2-qubit state. Building the whole formalism from scratch might be tedious, but it helps us understand every detail of it.\nOf course, in order to continue forward, we will take advantage of the collective effort and use some existing tools to ease our way into quantum computing.",
    "crumbs": [
      "Getting started",
      "Simulations"
    ]
  },
  {
    "objectID": "parts/gettingstarted/simulations.html#qutip",
    "href": "parts/gettingstarted/simulations.html#qutip",
    "title": "Simulations",
    "section": "QuTip",
    "text": "QuTip\nOf course, we are not the first ones with this need, so there do exist some quite useful libraries in this domain. QuTiP might be one of the most used ones (at least for us, Python enthusiasts).\n\nimport scipy\nimport numpy as np\nfrom qutip import Qobj, mesolve\nfrom qutip import basis, tensor\nfrom qutip.qip.operations import cnot\n\n# Basis states\nzero = basis(2,0)\none = basis(2,1)\n\n# |10&gt;\none_zero = tensor(one, zero)\n\n# CNOT\nhamiltonian = cnot().full()\n\n# e^itH\nu_generator = Qobj(1j * scipy.linalg.logm(hamiltonian), dims=[[2] * 2] * 2)\n\n# Time range 0.0 -&gt; 1.0\ntimes = np.arange(0, 1.1, 0.1)\n\n# \\psi = H\\psi_0\nevolution = mesolve(u_generator, one_zero, times)\n\n\nevolution.states[0]\n\nQuantum object: dims=[[2, 2], [1]], shape=(4, 1), type='ket', dtype=Dense\nQobj data =\n[[0.]\n [0.]\n [1.]\n [0.]]\n\n\n\nevolution.states[1]\n\nQuantum object: dims=[[2, 2], [1]], shape=(4, 1), type='ket', dtype=Dense\nQobj data =\n[[0.        +0.j        ]\n [0.        +0.j        ]\n [0.97552824+0.15450855j]\n [0.02447175-0.15450855j]]\n\n\n\nevolution.states[-1]\n\nQuantum object: dims=[[2, 2], [1]], shape=(4, 1), type='ket', dtype=Dense\nQobj data =\n[[0.00000000e+00+0.00000000e+00j]\n [0.00000000e+00+0.00000000e+00j]\n [1.07316277e-06+9.88099162e-07j]\n [1.00000000e+00-9.88099162e-07j]]\n\n\nLet‚Äôs go by steps.\n\nfrom qutip import basis, tensor\n\none = basis(2,1)\nzero = basis(2,0)\n\none_zero = tensor(one, zero) # |10&gt;\none_zero\n\nQuantum object: dims=[[2, 2], [1]], shape=(4, 1), type='ket', dtype=Dense\nQobj data =\n[[0.]\n [0.]\n [1.]\n [0.]]\n\n\n\nfrom qutip.qip.operations import cnot\n\ncnot_matrix = cnot().full()\ncnot_matrix\n\narray([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n       [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n       [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n       [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]])\n\n\nIn fact, it allows us to do more than just the simple operations we envisioned. For example, we could generate the whole evolution over a period of time of the \\(U\\) unitary generated for our Hamiltonian.\n\nfrom qutip import identity\nfrom qutip.qip.operations import hadamard_transform\n\n# hamiltonian\nhamiltonian =  cnot() * tensor(hadamard_transform(1), identity(2))\nhamiltonian\n\nQuantum object: dims=[[2, 2], [2, 2]], shape=(4, 4), type='oper', dtype=Dense, isherm=False\nQobj data =\n[[ 0.70710678  0.          0.70710678  0.        ]\n [ 0.          0.70710678  0.          0.70710678]\n [ 0.          0.70710678  0.         -0.70710678]\n [ 0.70710678  0.         -0.70710678  0.        ]]\n\n\n\nimport numpy as np\nimport scipy\nfrom qutip import Qobj, mesolve\n\n# Initial state\ninit_state = tensor(zero, zero) # |00&gt;\n\n# e^itH\nu_generator = Qobj(1j * scipy.linalg.logm(hamiltonian.full()), dims=[[2] * 2] * 2)\n\n# Time range\ntimes = np.arange(0, 1.1, 0.1)\nevolution = mesolve(u_generator, init_state, times)\n\n\nevolution.states[0]\n\nQuantum object: dims=[[2, 2], [1]], shape=(4, 1), type='ket', dtype=Dense\nQobj data =\n[[1.]\n [0.]\n [0.]\n [0.]]\n\n\n\nevolution.states[1]\n\nQuantum object: dims=[[2, 2], [1]], shape=(4, 1), type='ket', dtype=Dense\nQobj data =\n[[ 0.99487486+0.02262725j]\n [-0.00204247+0.02262725j]\n [-0.03057749-0.05462701j]\n [ 0.04788161-0.05462701j]]\n\n\n\nevolution.states[-1]\n\nQuantum object: dims=[[2, 2], [1]], shape=(4, 1), type='ket', dtype=Dense\nQobj data =\n[[ 7.07106734e-01-2.77290950e-08j]\n [-2.74299654e-08-2.77290950e-08j]\n [ 6.61796919e-08+6.69439574e-08j]\n [ 7.07106828e-01+6.69439574e-08j]]\n\n\n\npsi = np.round(evolution.states[-1].full(), decimals = 5)\n\narray_to_latex(array=psi, prefix='|\\\\psi\\\\rangle = ', max_size=(10,10))\n\n\\[\n|\\psi\\rangle =\n\\begin{bmatrix}\n0.70711  \\\\\n0  \\\\\n0  \\\\\n0.70711  \\\\\n\\end{bmatrix}\n\\]\n\n\nLooks like \\(\\frac{1}{\\sqrt{2}}(|00\\rangle + |11\\rangle)\\) but due to numerical imprecisions we wil get this type of ugly structures in between. We will need to familiarize with those artifacts as noise when using hardware will produce similar numerical artifacts.\nWell, having a theoretical framework may be a good option for simulating and doing some local experimentation but that will reach soon limitations when trying to scale it up. Our classical device won‚Äôt be able to perform the whole system calculations and we might need to switch to actual quantum computers doing those. Therefore, we need to find a way to o so.\nThis is why some manufacturers have invested time and effort on creating open-source frameworks to be adopted by the community (and position themselves). Companies such as IBM or AWS have leveraged their own version that also allows for these programs to be sent to an end device that will perform the set of operations.\n\nfrom qiskit import QuantumCircuit\n\nqc = QuantumCircuit(2, 2)\nqc.x(0)\nqc.cx(0, 1)\nqc.measure([0,1], [0,1])\n\nqc.draw('mpl')",
    "crumbs": [
      "Getting started",
      "Simulations"
    ]
  },
  {
    "objectID": "parts/gettingstarted/simulations.html#qiskit",
    "href": "parts/gettingstarted/simulations.html#qiskit",
    "title": "Simulations",
    "section": "Qiskit",
    "text": "Qiskit\nQiskit is IBM‚Äôs quantum computing toolkit the enables interaction with their devices. Let‚Äôs first start replicating the theoretical basis and then move forward up to device simulation.\nOpflow is the framework section dedicated to provide the pieces to perform previous computations.\n\nfrom qiskit.quantum_info import Statevector\n\nZero = Statevector.from_label('0')\nOne = Statevector.from_label('1')\n\n\nprobs = Zero.probabilities()\nprint('Probability of measuring 0: {}'.format(probs[0]))\n\nProbability of measuring 0: 1.0\n\n\n\nprint('Probability of measuring 1: {}'.format(probs[1]))\n\nProbability of measuring 1: 0.0\n\n\n\nPlus = Statevector.from_label('+')\n\nprint(Plus)\n\nStatevector([0.70710678+0.j, 0.70710678+0.j],\n            dims=(2,))\n\n\n\nPlus.probabilities()\n\narray([0.5, 0.5])\n\n\nQiskit tends to understand everything in terms of circuits. But in essence we can request the actual operation being performed there and check that the Hadamard action over a \\(|0\\rangle\\) state (the initial state) provides the \\(|+\\rangle\\) state as expected.\n\narray_to_latex(array=Plus.data, prefix='|+\\\\rangle = ', max_size=(10,10))\n\n\\[\n|+\\rangle =\n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}  \\\\\n\\end{bmatrix}\n\\]\n\n\nWhat would be the outcome of it?\n\nprobs = Plus.probabilities()\nprint('Probability of measuring 0: {}'.format(probs[0]))\n\nProbability of measuring 0: 0.4999999999999999\n\n\nThis is the expected outcome for the \\(\\langle 0 | \\psi \\rangle\\) operation. But we can mimic it as well.\n\nprint(abs(np.dot(Zero.data, Plus.data.T)**2))\n\n0.4999999999999999\n\n\nSimilarly gate operations can be used.\n\nfrom qiskit.quantum_info import Pauli\n\nX = Pauli('X')\n\nAnd what is the amplitude of \\(|1\\rangle\\) after the \\(X|0\\rangle\\) operation?\n\nZero.evolve(X) == One\n\nTrue\n\n\nQiskit orders bits in a specific manner so some gates may look different but is just a matter of ordering when applying the operations.\n\nfrom qiskit.circuit.library import UnitaryGate\n\nmatrix = [[1., 0., 0., 0.],\n          [0., 0., 0., 1.],\n          [0., 0., 1., 0.],\n          [0., 1., 0., 0.]]\nCNOT = UnitaryGate(matrix)\n\nLet play around with the Bell state we produced before\n\\[\nCNOT (I\\otimes H)|00\\rangle = |\\Phi^{+}\\rangle = \\frac{1}{\\sqrt{2}}(|00\\rangle + |11\\rangle\n\\]\n\nprint(Zero ^ Zero)\n\nStatevector([1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n            dims=(2, 2))\n\n\n\nfrom qiskit import QuantumCircuit\n\nqc = QuantumCircuit(2)\nqc.h(0)\n\nqc.draw()\n\n     ‚îå‚îÄ‚îÄ‚îÄ‚îê\nq_0: ‚î§ H ‚îú\n     ‚îî‚îÄ‚îÄ‚îÄ‚îò\nq_1: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n          \n\n\n\nfrom qiskit.quantum_info import Operator\n\nprint(Operator(qc).data)\n\n[[ 0.70710678+0.j  0.70710678+0.j  0.        +0.j  0.        +0.j]\n [ 0.70710678+0.j -0.70710678+0.j  0.        +0.j  0.        +0.j]\n [ 0.        +0.j  0.        +0.j  0.70710678+0.j  0.70710678+0.j]\n [ 0.        +0.j  0.        +0.j  0.70710678+0.j -0.70710678+0.j]]\n\n\n\n(Zero ^ Zero).evolve(qc).data\n\narray([0.70710678+0.j, 0.70710678+0.j, 0.        +0.j, 0.        +0.j])\n\n\n\nbell_state = (Zero ^ Zero).evolve(qc).evolve(CNOT)\nbell_state.data\n\narray([0.70710678+0.j, 0.        +0.j, 0.        +0.j, 0.70710678+0.j])\n\n\n\narray_to_latex(array=bell_state.data, prefix='|\\\\psi\\\\rangle = ', max_size=(10,10))\n\n\\[\n|\\psi\\rangle =\n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} & 0 & 0 & \\frac{\\sqrt{2}}{2}  \\\\\n\\end{bmatrix}\n\\]\n\n\nOf course the gate-based nature of IBM devices makes it more natural to directly code our approach as a gate based circuit.\n\nfrom qiskit import QuantumCircuit\n\nqc = QuantumCircuit(2)\nqc.h(0)\nqc.cx(0,1)\n\nqc.draw('mpl')\n\n\n\n\n\n\n\n\nMost likelly moving forward, this pictorical approach will ease the abstraction but is good to know that, the formalism is still there.\n\nprint('Math:', (Zero ^ Zero).evolve(qc).probabilities())\n\nMath: [0.5 0.  0.  0.5]\n\n\nOf course qiskit offers some other nice ways to simulate and visualize the results.\n\nfrom qiskit.quantum_info import Statevector\n\npsi  = Statevector.from_instruction(qc)\n\n\nfrom qiskit.visualization import plot_state_qsphere\n\nplot_state_qsphere(psi)\n\n\n\n\n\n\n\n\n\nfrom qiskit.visualization import plot_bloch_multivector\n\nplot_bloch_multivector(psi)\n\n\n\n\n\n\n\n\nIn order to simulate the actual action of the circuit we will need to add some classical registers and measurement operations.\n\ncircuit = QuantumCircuit(2, 2)\n\ncircuit = circuit.compose(qc)\n\ncircuit.measure([0,1],[0,1])\n\ncircuit.draw('mpl')\n\n\n\n\n\n\n\n\n\nfrom qiskit_aer import AerSimulator\n\n# execute the quantum circuit\nsimulator = AerSimulator()\n\nresult = simulator.run(circuit, shots=1000).result()\ncounts  = result.get_counts(circuit)\nprint(counts)\n\n{'00': 483, '11': 517}\n\n\n\nfrom qiskit.visualization import plot_histogram\n\nplot_histogram(counts)",
    "crumbs": [
      "Getting started",
      "Simulations"
    ]
  },
  {
    "objectID": "parts/gettingstarted/simulations.html#exercise",
    "href": "parts/gettingstarted/simulations.html#exercise",
    "title": "Simulations",
    "section": "Exercise",
    "text": "Exercise\nCould we create a state that superposes all potential basis states for a 2-qubit configuration?\n\\[\n\\frac{1}{2}|00\\rangle + \\frac{1}{2}|01\\rangle + \\frac{1}{2}|10\\rangle + \\frac{1}{2}|11\\rangle\n\\]\n\nqc = QuantumCircuit(2)\n\n# HERE GOES YOUR CIRCUIT\n\n\npsi  = Statevector.from_instruction(qc)\n\nplot_bloch_multivector(psi)\n\n\n\n\n\n\n\n\n\nplot_state_qsphere(psi)\n\n\n\n\n\n\n\n\n\ncircuit = QuantumCircuit(2, 2)\ncircuit = circuit.compose(qc)\ncircuit.measure([0,1],[0,1])\n\n# execute the quantum circuit\nsimulator = AerSimulator()\n\nresult = simulator.run(circuit, shots=1000).result()\ncounts  = result.get_counts(circuit)\n\nplot_histogram(counts)",
    "crumbs": [
      "Getting started",
      "Simulations"
    ]
  },
  {
    "objectID": "parts/gettingstarted/classicalqc.html",
    "href": "parts/gettingstarted/classicalqc.html",
    "title": "Classical resources",
    "section": "",
    "text": "Simulators\nSo, now you know how a simulator works. Essentially performs the mathematical operations we expect a quantum computer will do. but there are quite some nuances on the way in which we do this. For example, it is not the same to use a simulator or an emulator.\nA simulator performs specifically the actions of the mathematical framework, without any restriction or special case. Simply put, it was what the ideal machine should do. An emulator on the other hand, performs the same set of actions but considering some of the limitations a particular device could have. Essentially it is trying to show how the same actions performed by a phisical machine would look like.\nThe actions we performed in previous exercise were mimicking the state vector of our system and performing the unitary operations that drive the state. This is often known as State Vector simulator (dah!) and it is common on some of the most used SDKs.\nA less common approach is the usage of the Density Matrix simulator, which renders the whole evolution considering the effect of noise and the potential of obtaining a mixed-state (which in reality happens a lot but unless we work trying to emulate the work of a device, we can skip it).\nThe Matrix Product State simulator is a method derived from Tensor Networks, a way to efficiently deal with large matrix operations, at least up to a precision level. In many cases this method allows us to emulate larger circuits as it is the most resource efficient method.\nimport numpy as np\n\n# Import Qiskit\nfrom qiskit import QuantumCircuit, transpile\nfrom qiskit_aer import AerSimulator\nfrom qiskit.visualization import plot_histogram, plot_state_city\nimport qiskit.quantum_info as qi\n\n# Create circuit\ncirc = QuantumCircuit(2)\ncirc.h(0)\ncirc.cx(0, 1)\ncirc.measure_all()\n\n# Increase shots to reduce sampling variance\nshots = 10000\n\n# Statevector simulation method\nsim_statevector = AerSimulator(method='statevector')\njob_statevector = sim_statevector.run(circ, shots=shots)\ncounts_statevector = job_statevector.result().get_counts(0)\n\n# Density Matrix simulation method\nsim_density = AerSimulator(method='density_matrix')\njob_density = sim_density.run(circ, shots=shots)\ncounts_density = job_density.result().get_counts(0)\n\n# Matrix Product State simulation method\nsim_mps = AerSimulator(method='matrix_product_state')\njob_mps = sim_mps.run(circ, shots=shots)\ncounts_mps = job_mps.result().get_counts(0)\n\nplot_histogram([counts_statevector, counts_density, counts_mps],\n               title='Counts for different simulation methods',\n               legend=[ 'statevector',\n                       'density_matrix',\n                       'matrix_product_state'])\nQiskit is not the only SDK we will be using so it is good to know this very same simulation techniques exist when, for example, using Pennylane you will have access to several devices including:\nAnd many more. They also include nice simulators for GPU based simulation, based on NVIDIA cuQuantum SDK via Lightning which will be really helpful when we start talking about training circuits.\nIf you are interested in Tensor Networks, this is not the right place, but I can recommend some resources like TensorNetwork.org to look into useful resources and some nice references to get deeper into the field (Biamonte and Bergholm 2017).\nLet‚Äôs keep it simple for now and deal with algorithm implementations.",
    "crumbs": [
      "Getting started",
      "Classical resources"
    ]
  },
  {
    "objectID": "parts/gettingstarted/classicalqc.html#simulators",
    "href": "parts/gettingstarted/classicalqc.html#simulators",
    "title": "Classical resources",
    "section": "",
    "text": "default.qubit for state vector simulation\ndefault.mixed for dealing with mixed states\ndefault.tensor for MPS type of simulation using QUIMB under the hood\n\n\n\n\n\n\n\n\nBiamonte, Jacob, and Ville Bergholm. 2017. ‚ÄúTensor Networks in a Nutshell.‚Äù https://arxiv.org/abs/1708.00006.",
    "crumbs": [
      "Getting started",
      "Classical resources"
    ]
  },
  {
    "objectID": "parts/computers/firstrealizations.html",
    "href": "parts/computers/firstrealizations.html",
    "title": "First realizations",
    "section": "",
    "text": "Building the thing\nMotivated by Shor‚Äôs algorithm (Shor 1994), during late 90s there was quite some motivation to provide a physical realization of what up to that moment was mostly theoretical work.\nAny two-level physical system could be a good candidate for a qubit but we need to a find a physical realization that allows for operation and performing the set of tasks we could consider an algorithm.\nIn 1998, using nuclear magnetic resonance (NMR) both Oxford University and Stanford achieved the first implementations of two qubit algorithms (Chuang, Gershenfeld, and Kubinec 1998). More or less in the same period, Yasunobu Nakamura and Jaw-Shen Tsai demonstrated how a superconducting chip can be used as a qubit by controlling the current through it (Nakamura, Pashkin, and Tsai 2001).\n2007 is known to be a key date as the first transmon was born. It is the equivalent of the transistor in the quantum regime (Koch et al. 2007). So first qubits exist since early 2000 which is not much but from manufacturing a qubit to building a whole computer‚Ä¶ that is a long road.\nMany metrics will be used to compare different devices and realizations in following sections so better get them clear now.\nT1 time\nAlso known as the relaxation time, identifies how much time it would take a qubit to transition from an excited state (\\(|1\\rangle\\)) to its ground state (\\(|0\\rangle\\)). T1 time is not an actual time from state to state but rather identifies the time in which a state might transition and flips states once exited (\\(P(|1\\rangle) = e^{-\\frac{t}{T1}}\\)).\nT2 time\nKnown as the phase coherence time. Basically quantifies the time in which the phase associated to a state is stable.\n\\[\n|+\\rangle = \\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}} \\quad \\xrightarrow[T2]{} \\quad |-\\rangle = \\frac{|0\\rangle -|1\\rangle}{\\sqrt{2}}\n\\]\nThese two time-metrics characterize our qubits coherence time, the time in which our qubits are actually qubits and not classical bits playing tricks on us.\nAnother measure we will often refer to is the fidelity. Basically it measures the gap between theoretical state and physical outcome as a qubit quality metric. Fidelity attends to the more general state of closeness between two quantum states but in this following sections it will provide a measure to understand how well each operation renders the expected result.\n\\[\nF(\\rho, \\sigma) = |\\langle \\psi | \\phi \\rangle | ^2\n\\]\nThere is more than one type of quantum computer. Actually we yet don‚Äôt know the one that will win the podium, but the race is interesting at this point. The physical realization conditions many aspects of the whole machine so it is relevant to understand a bit of their inner workings so that we can select the most appropriate option.\nThere is one general definition of how a quantum computer may look like and it is given by the DiVincenzo criteria (DiVincenzo 1997).\nWith this recipe many platforms have been proposed using different physical realizations for the technology.",
    "crumbs": [
      "Quantum Computers",
      "First realizations"
    ]
  },
  {
    "objectID": "parts/computers/firstrealizations.html#building-the-thing",
    "href": "parts/computers/firstrealizations.html#building-the-thing",
    "title": "First realizations",
    "section": "",
    "text": "Must be scalable and well characterized qubits.\nSystem may be able to be initialized (\\(|00...\\rangle\\) state is the common choice)\nQubit coherence time must be much larger than the actuation time\nEnable a universal set of gates/operations\nCapable of measuring from specific qubits\n\n\n\n\n\n\nChuang, Isaac L, Neil Gershenfeld, and Mark Kubinec. 1998. ‚ÄúExperimental Implementation of Fast Quantum Searching.‚Äù Physical Review Letters 80 (15): 3408.\n\n\nDiVincenzo, David P. 1997. ‚ÄúTopics in Quantum Computers.‚Äù In Mesoscopic Electron Transport, 657‚Äì77. Springer.\n\n\nKoch, Jens, Terri M Yu, Jay Gambetta, Andrew A Houck, David I Schuster, Johannes Majer, Alexandre Blais, Michel H Devoret, Steven M Girvin, and Robert J Schoelkopf. 2007. ‚ÄúCharge-Insensitive Qubit Design Derived from the Cooper Pair Box.‚Äù Physical Review A‚ÄîAtomic, Molecular, and Optical Physics 76 (4): 042319.\n\n\nNakamura, Y, Yu A Pashkin, and Jaw Shen Tsai. 2001. ‚ÄúRabi Oscillations in a Josephson-Junction Charge Two-Level System.‚Äù Physical Review Letters 87 (24): 246601.\n\n\nShor, Peter W. 1994. ‚ÄúAlgorithms for Quantum Computation: Discrete Logarithms and Factoring.‚Äù In Proceedings 35th Annual Symposium on Foundations of Computer Science, 124‚Äì34. Ieee.",
    "crumbs": [
      "Quantum Computers",
      "First realizations"
    ]
  },
  {
    "objectID": "parts/computers/technology.html",
    "href": "parts/computers/technology.html",
    "title": "Technology",
    "section": "",
    "text": "Superconducting chips\nIn order to create a machine that is able to execute the mathematical framework described by quantum computation, different technologies can be used.\nSuperconducting qubits are implemented using macroscopic elements build using a LC electrical circuit. The effect of an electrical resonator is observed at specific work temperatures below 15mK which requires building quite large dilution refrigerators. One of the main disadvantages of this technology is their susceptibility to environment noise (sources being dielectric of surrounding metal or surrounding energy radiations) that cause short coherence times.\nProbably one of the most mature implementations when it comes to qubit realization. Companies like IBM, Intel, Google or D-Wave base their devices in this technology. The most popular qubit, the transmon, offers coherence times between 50 and 100 \\(\\mu\\)s with operation times in the order of nanoseconds (fastest: 18 ns demonstration in 2019). This leaves an operation ratio of \\(10^{4}\\) satisfying 3rd DiVincenzo criteria. Qubit fidelity reach the 99.95% with experiments reporting 99.66% for two qubit gate in 2016.\nWorks by controlling the current flow through the superconducting circuit and those current flows can be operated using microwave pulses. Different pulse configurations (length, amplitude or phase) represent the potential digitized gates one could find.\nCircuit topology is often limited to nearest-neighbor couplings, which limits the implementation of densely connected algorithms.\nThese devices can handle up to 433 qubits (IBM‚Äôs Osprey architecture) on their digital version but D-Wave‚Äôs annealing machine counts with up to 8000 qubits to be used.\nTo know a bit more about the inner workings, go to Xanadu‚Äôs thorough explanation.",
    "crumbs": [
      "Quantum Computers",
      "Technology"
    ]
  },
  {
    "objectID": "parts/computers/technology.html#superconducting-chips",
    "href": "parts/computers/technology.html#superconducting-chips",
    "title": "Technology",
    "section": "",
    "text": "Superconducting\n\n\n\n\n\n\n\n\n\n\n\nProvider\nMax. Qubits\n\n\n\n\nD-Wave\n8000\n\n\nIBM\n433\n\n\nRigetti\n80\n\n\nOQC\n32",
    "crumbs": [
      "Quantum Computers",
      "Technology"
    ]
  },
  {
    "objectID": "parts/computers/technology.html#trapped-ions",
    "href": "parts/computers/technology.html#trapped-ions",
    "title": "Technology",
    "section": "Trapped ions",
    "text": "Trapped ions\nInitially proposed by Cirac and Zoller (Cirac and Zoller 1995), ions are confined into traps making up the quantum computer composed of those.\n\n\n\n\nIon trap\n\n\n\nPairs of internal electronic states of ions are implemented as the base qubit states \\(|0\\rangle\\) and \\(|1\\rangle\\). They can be any combination of long-lived levels, differentiated by the separation of their energy levels. The 2 most predominant ones are hyperfine levels, separated by gigahertz frequencies; and optical levels, separated by frequencies of hundreds of terahertz.\nAfter the ions are loaded into their traps, the qubits are optically pumped into the fiducial state of \\(|1\\rangle\\). The methodology of gate operation via state transition is dependent on the type of trapped-ion qubit. For optical qubits, a laser with a resonant frequency equal to the state transition is used whereas gate operations on hyperfine qubits can be performed either with lasers using stimulated Raman Coupling or with microwaves directly addressing the giga-hertz energy level splitting.\nThe state-dependent readout is carried out by lasers as well. Using contemporary methods, readout fidelities of over 99.9% have been achieved within microseconds (Myerson et al. 2008; Crain et al. 2019).\nIon-traps count with a virtual all-to-all connectivity that allows for any qubit to qubit interaction. Shortcomings can be attributed to the decrease of the speed and fidelity of 2 qubit gates as the number of ions in the chain increases though. As the size of the ion chain increases, its mass increases as does the average distance between ions. This in turn leads to a decrease in the coupling strength between arbitrary ions, causing a decrease in speed of 2 qubit gates. Fidelity decreases due to the increased susceptibility among the normal modes used to mediate the 2-qubit interaction to unwanted spectral crosstalk interaction, as well as to noise-induced heating in the system if the gates take longer.\nA way to circumvent this problem is to break up the long linear chain into smaller modules, and it is still the barrier many device manufacturers in this regime need to work on.\n\n\n\n\nProvider\nMax. Qubits\n\n\n\n\nIonQ\n23\n\n\nQuantinuum\n32\n\n\n\n\nMore on their current specs can be found in their sites or cloud provider sites:\n\nIonQ\nQuantinuum\n\nMore in detail information thanks to Xanadu.",
    "crumbs": [
      "Quantum Computers",
      "Technology"
    ]
  },
  {
    "objectID": "parts/computers/technology.html#neutral-atoms",
    "href": "parts/computers/technology.html#neutral-atoms",
    "title": "Technology",
    "section": "Neutral atoms",
    "text": "Neutral atoms\nProposed around 2000, the basic concept lies in using atoms or neutral alkali metals like Rubidium (Rb) trapped in optical arrays. By laser cooling the atoms in an array to near absolute zero temperatures, traps using magnetic fields or lasers allows for rearrangement of the atoms within the lattice. Atom‚Äôs own structure represents the two level system we are looking for therefore a one-to-one mapping is done between the atom and the qubit we would like to manufacture.\n\n\n\n\nNeutral atoms\n\n\n\nSingle-qubit gate operations are achieved by driving atomic transitions using laser beams tightly focused on a single atom, or by microwaves, for which the targeted atom‚Äôs resonance needs to be shifted using magnetic fields or laser beams. Two-qubit gates would seem to be a hindrance for neutral atom qubits due to their weak interactions with each other; however, this problem is solved by exciting atoms to Rydberg states. A Rydberg atom exhibits a strong dipolar interaction and produces a phenomenon known as the Rydberg blockade, which prohibits more than one atom in a small volume from being simultaneously excited to a Rydberg state. This can be used to produce entanglement between two atoms using a three pulse sequence. The readout of the neutral atom array is typically done by taking a fluorescence image at the end of the computational process. It is performed such that each atom in the state \\(|0\\rangle\\) will appear bright, whereas atoms in the state \\(|1\\rangle\\) remain dark.\nThe arrangement of atoms in any type of 2D or even 3D structures makes problem mapping and qubit coupling flexible enough so that potentially any problem may fit into those devices. The main problem comes on the realization of two-qubit gates with high enough fidelity. Single-qubit operations reach 99.9% fidelity while two-qubit gates have been extensively studied boosting the initial 80% achieved in early 2016 to the reported 96.5% in 2019 (Levine et al. 2019). This is still an ongoing effort that limits the operability and universality of coding as most of the platforms require some kind of analog encoding as opposed to the logical abstraction others can provide.\n\n\n\n\nPasqal\n\n\n\nQubit coherence times are in the order of seconds, with and operation regime of \\(\\mu\\)seconds (approx. ratio \\(10^7\\)). They also count with the ability to operate simultaneously in different atom clusters, enabling operation overlapping as opposed to ion-traps.\n\n\n\n\nProvider\nMax. Qubits\n\n\n\n\nPasqal\n~300\n\n\nQuEra\n256\n\n\n\n\nSome specs on both hardware providers:\n\nPasqal\nQuEra\n\nAnd easy to read and in detail vision of this technology, once again, thanks to Xanadu.\n\n\n\n\nCirac, Juan I, and Peter Zoller. 1995. ‚ÄúQuantum Computations with Cold Trapped Ions.‚Äù Physical Review Letters 74 (20): 4091.\n\n\nCrain, Stephen, Clinton Cahall, Geert Vrijsen, Emma E Wollman, Matthew D Shaw, Varun B Verma, Sae Woo Nam, and Jungsang Kim. 2019. ‚ÄúHigh-Speed Low-Crosstalk Detection of a 171Yb+ Qubit Using Superconducting Nanowire Single Photon Detectors.‚Äù Communications Physics 2 (1): 97.\n\n\nLevine, Harry, Alexander Keesling, Giulia Semeghini, Ahmed Omran, Tout T Wang, Sepehr Ebadi, Hannes Bernien, et al. 2019. ‚ÄúParallel Implementation of High-Fidelity Multiqubit Gates with Neutral Atoms.‚Äù Physical Review Letters 123 (17): 170503.\n\n\nMyerson, AH, DJ Szwer, SC Webster, DTC Allcock, MJ Curtis, G Imreh, JA Sherman, DN Stacey, AM Steane, and DM Lucas. 2008. ‚ÄúHigh-Fidelity Readout of Trapped-Ion Qubits.‚Äù Physical Review Letters 100 (20): 200502.",
    "crumbs": [
      "Quantum Computers",
      "Technology"
    ]
  },
  {
    "objectID": "parts/computers/machinetypes.html",
    "href": "parts/computers/machinetypes.html",
    "title": "Machine types",
    "section": "",
    "text": "Analog devices\nWe have talked about how the hardware works but not all Quantum Computers work in the same way. Just talking about pure quantum computers three main types of computers can be found.\nLike in the very beginning of computers, managing analog signals is the natural way to start when trying to domesticate physical phenomena to reproduce a particular set of steps. There do exist machines with well characterized qubits but requiring a specific way to interact with them. This is the case of neutral atom platforms, a Hamiltonian is provided that acts on the system and our duty is to define the signals that will drive the system towards the solution of our problem. These atoms are well defined qubits but manipulating them according to the mathematical framework can be challenging, therefore it is easier (in some sense) to drive the whole system by signal manipulation (hence analog computation).\nThe Hamiltonian on those devices looks as follows,\n\\[\n\\frac{\\mathcal{H}(t)}{\\hbar} = \\sum_j \\frac{\\Omega_j(t)}{2} \\left( e^{i \\phi_j(t) } | 0_j \\rangle  \\langle 1_j | + e^{-i \\phi_j(t) } | 1_j \\rangle  \\langle 0_j | \\right) - \\sum_j \\Delta_j(t) \\hat{n}_j + \\sum_{j &lt; k} V_{jk} \\hat{n}_j \\hat{n}_k,\n\\]\nwhere \\(|0_i\\rangle\\), \\(|1_i\\rangle\\), and \\(\\hat{n}_i = 0|0_i\\rangle\\langle0_i|+1|1_i\\rangle\\langle1_i|\\) refer to qubits \\(i\\) position for a given space. Defining algorithms requires manipulating the time-traces of the Hamiltonian parameters \\(\\Omega_i(t)\\), \\(\\phi_i(t)\\), and \\(\\Delta_i(t)\\). The Rabi term \\(\\Omega\\) and detuning \\(\\Delta\\) introduce energy scales that compete with the geometrically controlled scale \\(V_{ij}\\).\nThese machines natively solve a mathematical problem called, maximum independent set. Each atom has a blockade radius that prevents to atoms (qubits) being active at the same time (both being \\(|1\\rangle\\)). If we are able to encode our problem, the machine could without effort compute the most efficient points in a map where one should place specific assets based on their surroundings. More on about this on QuEra‚Äôs documentation site\nLook at this reference on how to interact with QuEra devices using Amazon Braket service and their SDK. It is easier than it looks but one needs to definitely know how to translate the problem to be solved to that formulation. These devices are often limited to a set of mathematical problems that requires users to think how their actual business problem may fit into those setups. As an example, here is a candidate problem on where to place antennas in Boston to minimize the cost but maximize coverage.\nMost likely, as it has happened with the rest of the machines, they will grow into the direction of digitized operation. A hint on previously mentioned native gates: defining a set of parameters that translated our device Hamiltonian to the gates we would like to use, is part of the magic many hardware providers perform through programming frameworks so that we don‚Äôt have to worry too much about it.",
    "crumbs": [
      "Quantum Computers",
      "Machine types"
    ]
  },
  {
    "objectID": "parts/computers/machinetypes.html#analog-devices",
    "href": "parts/computers/machinetypes.html#analog-devices",
    "title": "Machine types",
    "section": "",
    "text": "Boston Antenna placement\n\n\n\n\nExample for above picture\nGeneral on MIS for neutral atoms: https://arxiv.org/pdf/2109.03517.pdf",
    "crumbs": [
      "Quantum Computers",
      "Machine types"
    ]
  },
  {
    "objectID": "parts/computers/machinetypes.html#quantum-annealers",
    "href": "parts/computers/machinetypes.html#quantum-annealers",
    "title": "Machine types",
    "section": "Quantum Annealers",
    "text": "Quantum Annealers\nIn 1998, Hideyoshi Nishimoru from Tokyo University demonstrated that quantum annealing could outperform classical annealing techniques used up to that moment for optimization and combinatorial tasks. Quantum Annealers work by starting the system at a know state and by performing little perturbations to the system provide the solution to a target state. Its universality has been proven\nAdiabatic quantum computing (AQC) is a model of computation that uses quantum mechanical processes operating under adiabatic conditions. As a form of universal quantum computation, AQC employs the principles of superposition, tunneling, and entanglement that manifest in quantum physical systems.\n\n\n\n\nAdiabatic evolution of ground state\n\n\n\nD-Wave, was one of the first ones to produce a commercially available quantum computer with thousands of qubits. A clear milestone in the path towards commercially making it viable.\nA big caveat of this machine, is that it can only work using the adiabatic evolution as the main process. This means we can only govern the inner workings of the machine up to a point. Annealers work in precise manner so they do not allow for the flexibility digitized computers do, making them closer to an analog machine with restricted programming ability.\nThese machines are able to solve a complex problem indeed. In general any Quadratic Unconstrained Binary Optimization (QUBO) problem fits into the general framework of using annealers to solve the minimum energy state for a Ising type of problem:\n\\[\nH = -\\sum_{\\langle i j \\rangle} J s_i s_j - \\sum_j h s_j.\n\\]\nWe will see this more in detail when covering the inner workings of annealers but, think that if your problem fits into this type of mathematical formulation, you may find an ally when it comes to Quantum Annealers.",
    "crumbs": [
      "Quantum Computers",
      "Machine types"
    ]
  },
  {
    "objectID": "parts/computers/machinetypes.html#digital-quantum-computers",
    "href": "parts/computers/machinetypes.html#digital-quantum-computers",
    "title": "Machine types",
    "section": "Digital quantum computers",
    "text": "Digital quantum computers\nIn order to provide certain level of universality when it comes to operations, digitization of the interactions with the quantum computer and the states it encodes has become a critical path towards quantum device commercialization. Managing Hamiltonians is still necessary but the way in which these need to be introduced into the machine could be handled by atomic operations (logical gates) with which we can compose higher level operators or create more complex dynamics taking us closer to how classical computers operate, enabling incremental instruction addition and higher level abstractions.\nDigitized computers work by enabling a set of digitized operations so that we can implement our routines. The initial state of the system would still be \\(|00...0\\rangle\\) so if we would like to transform this state into \\(|11..1\\rangle\\) we should find the Hamiltonian that does it.\nA not operation, like in classical computers, is one of the basic actions: \\[\nX^{\\otimes n} |0\\rangle^{\\otimes n} = |1\\rangle^{\\otimes n}\n\\]\nA simple composition of \\(X\\) (not) operation on each qubit. Therefore, we could express it in a pictorial way:\n\n\n\n\n\\(\\sigma_x\\) circuit\n\n\n\nThat way it is quite simple to understand what are the atomic operations required and have a programming language that abstracts us from signals and pulses.\nAs an example, a unitary transformation characterized by \\(U = Z_1 \\otimes Z_2\\) leads to a simple implementation\nOPENQASM 2.0;\ninclude \"qelib1.inc\";\n\nqreg q[2];\nrz(pi/2) q[0];\nrz(pi/2) q[1];\n\n\n\n\n\\(\\sigma_z\\) circuit\n\n\n\nThis approach makes programming these machines really close to the way in which classical devices were programmed in the early days.\nWe should pay attention though that this logical framework is often complicated to represent by physical means, therefore we will see some discrepancies when we start looking into what are called native gates. This will become clear when we start with our exercises against real hardware. Let‚Äôs keep the joy of playing with quantum simulators for now.",
    "crumbs": [
      "Quantum Computers",
      "Machine types"
    ]
  },
  {
    "objectID": "parts/computers/serviceproviders.html",
    "href": "parts/computers/serviceproviders.html",
    "title": "Service providers",
    "section": "",
    "text": "Direct service providers\nMost of these machines cannot be purchased; or their cost would be way beyond any potential return of investment. They are quite expensive and require active maintenance so, lucky for us, we do count with the ability to interact with remote machines from our workstations. That way we can rent their usage and let others do the nasty work of maintenance. Most hardware providers offer their quantum computing machines as a service, so we can ask for computation time and send our circuits/algorithms to be executed on those. This was really common for Universities and research institutions when requesting time on a supercomputing or HPC resource but since the creation of the cloud it is also a well extended practice between data and analytics departments where ephemeral resources may make sense to be used due to the experimental nature of a given initiative.\nWe will guide on how these services can be enabled from players providing themselves the service as well as cloud providers where a wider variety of machines can be found.",
    "crumbs": [
      "Quantum Computers",
      "Service providers"
    ]
  },
  {
    "objectID": "parts/computers/serviceproviders.html#direct-service-providers",
    "href": "parts/computers/serviceproviders.html#direct-service-providers",
    "title": "Service providers",
    "section": "",
    "text": "D-Wave\nD-Wave is known to be one of the first companies to make their quantum computers commercially available (2011). This Canadian niche player built quantum annealing machines so that people could‚Ä¶ buy them? Well probably that was part of their intention, but these are big expensive machines yet to prove their usefulness. Luckily in 2018 they released their cloud solution DWave Leap.\nTheir cloud access allows some free time on these machines so that users can evaluate their usefulness and advantage to solve specific problems.\n\n\n\n\nDWave Leap\n\n\n\nThe dashboard offers information about the remaining available usage time and there are options to open a browser based Integrated Development Environment (IDE) or access the documentation for use cases that may help get started with this service.\nWe already discussed that Quantum Annealers are a different type of beast, so they might require different ways to instruct what to do. That is why DWave had to leverage also Python based Software Development Kits so that people could interact with their machines (exclusively).\nOcean software stack\nBy attaching a GitHub account the initial 20 min. access is extended to a monthly time allowance where their purely quantum devices as well as hybrid solvers will be available.\n\n\n\n\nDWave devices\n\n\n\n\n\nIBM\nIBM has been involved since the early days in the development of quantum computers. Superconducting chips are at the core of what they can provide, and probably you have already seen some images on how their machines look like, so now you know what is inside this huge cylinder.\n\n\n\n\nIBM System One\n\n\n\nIt is said that IBM representatives asked at a conference how many of the attendees would actually interact with a quantum computing service to send their experiments if IBM would allow for such access. Apparently a huge amount of people raised their hands and that gave birth to IBM‚Äôs Quantum Computing platform https://www.ibm.com/quantum) and the framework that enabled the communication and experiment implementation, Qiskit.\n\n\n\n\nIBM Quantum\n\n\n\nWe could either use online resources available there, such as the Quantum Lab (a Jupyter based IDE) or get the API token to interact from our local machine by means of Qiskit. Same devices will be available for our experiments in both cases.\n\n\n\n\nIBM devices",
    "crumbs": [
      "Quantum Computers",
      "Service providers"
    ]
  },
  {
    "objectID": "parts/computers/serviceproviders.html#cloud-hosted-services",
    "href": "parts/computers/serviceproviders.html#cloud-hosted-services",
    "title": "Service providers",
    "section": "Cloud hosted services",
    "text": "Cloud hosted services\nNot all companies have the ability to develop and maintain an online customer facing service as their focus may be more on the hardware development side, and they could benefit from partnerships where this is done by a third party. Cloud providers already aware of this, decided to step in and create a service so that others could host their quantum computing services.\n\nAmazon Braket\nAWS created a service called Amazon Braket so that people already using their cloud services could simply add quantum computing resources as an extra to their current stack. It offers similar experience to IBM‚Äôs or DWave, but the main benefit is the access to devices other than IBM‚Äôs or DWave‚Äôs.\n\n\n\n\nAWS devices\n\n\n\nOne can use their Software Development Kit (Braket SDK) which could be necessary for some devices not providing digitized gates but in general, due to the agreement on digitized instructions there it does exist an integration between both languages Amazon Braket SDK and IBM‚Äôs Qiskit.\n\n\nMicrosoft Azure\nSimilarly, Azure created a service that provides access to a set of devices some of them only being accessible in this platform (Quantinuum for example). Quantum Workspaces is the name, and it also offers a Jupyter based IDE for online experimentation.\n\n\n\n\nAzure devices\n\n\n\nMicrosoft created their own quantum computing framework called Q# but given the maturity of Qiskit, plugins do exist so that programmers won‚Äôt need to switch frameworks.\n\n\nDirect providers\nSome companies, like IonQ, do provide direct access to their simulators, emulators and devices via their cloud hosted service. This is the case of IonQ‚Äôs Cloud service. Anyone can sign up and request access to the simulators and QPUs available there.\n\n\n\n\nIonQ Cloud",
    "crumbs": [
      "Quantum Computers",
      "Service providers"
    ]
  },
  {
    "objectID": "parts/computers/serviceproviders.html#third-party-providers",
    "href": "parts/computers/serviceproviders.html#third-party-providers",
    "title": "Service providers",
    "section": "Third party providers",
    "text": "Third party providers\nThere do exist some intermediary providers that position themselves between the hardware access and the developer experience. Definitely interesting to look at their offering as it evolves:\n\nStrangeworks\nqBraid",
    "crumbs": [
      "Quantum Computers",
      "Service providers"
    ]
  },
  {
    "objectID": "parts/computers/challenges.html",
    "href": "parts/computers/challenges.html",
    "title": "Challenges",
    "section": "",
    "text": "Considerations\nThis is a tricky one, even though we love working with quantum computers, the key aspect is being able to solve efficiently a given problem. And, at this stage, there are many competitors doing a great job at solving problems without the need for quantum computers, and it is important that while this field evolves we make the most out of their colleagues. All those, quantum-inspired and quantum-adjacent technologies.\nFrom this family we have focused until now into the last bit, the Noisy Intermediate-Scale Quantum (NISQ) bit, the one related to actual quantum hardware but that does not mean those techniques and devices inspired by the challenges quantum mechanical systems pose are less than the actual hardware. In fact, one of the main problems to claim quantum advantage is the constant improvement and refinement of classical devices solving the same problems.\nWe highly recommend looking into classical and quantum-inspired solutions before jumping into the complexities of dealing with quantum hardware. There are really interesting results when it comes to classically emulating this formalism in the range of 10s to 100s of qubits and also hybrid approaches benefiting from specific aspects of the quantum domain, but heavily relying on classical processes (like Shor‚Äôs algorithm in fact).\nYou have been warned!\nBefore we enter into more cumbersome and obscure techniques, lets understand the context in which we are trying to use Quantum Computing to outperform classical means. It is not a fair comparison as we count with almost 70 years of development in the classical regime while quantum computers even though have been used for almost 25 years since the first realization of the qubit, might require some extra work until a similar status of maturity and confidence is reached.\nFollowing section will try to make the reader conscious about some of the issues they might face and how to tackle them, with a particular interest on Digital Quantum Computers.",
    "crumbs": [
      "Quantum Computers",
      "Challenges"
    ]
  },
  {
    "objectID": "parts/computers/challenges.html#considerations",
    "href": "parts/computers/challenges.html#considerations",
    "title": "Challenges",
    "section": "",
    "text": "Quantum devices\nWe have seen many different devices exist but, even between the ones that are based on same technology we will find discrepancies on how they implement their operations. When this operations are fully parameterized we call them Gates but even though engineering makes a substantial effort bridging the gap between theoretical framework and physical reality, we might need to make our part as well.\n\nCoupling maps\nCoupling map refers to the connectivity between qubits. Most of our problems will be mapped to those qubits and we will need to define the strength of the action with respect to neighboring qubits. This ability to encode the interactions is reflected in the physical connection between the qubits on a given device and it is called the coupling map.\nAs an example, this is the coupling map of Rigetti:\n\n\n\nRigetti Aspen topology\n\n\nAnd this one belongs to IonQ‚Äôs Aria:\n\n\n\nIonQ Aria topology\n\n\nSeems like ino-traps may be a better fit to combinatorial optimization problems as they already count with all-to-all connectivity, but they lack the scale of superconducting devices. So, can we make anything to virtually connect those qubits on Rigetti‚Äôs device all-to-all?\nSWAP gates\nThe SWAP gate affects two qubits by interchanging their state, such that\n\\[\nSWAP|\\phi\\rangle|\\psi\\rangle = |\\psi\\rangle|\\phi\\rangle,\n\\]\nand its matrix form looks as follows\n\\[\nSWAP = \\left[\n\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\n\\right].\n\\]\nBy iteratively applying this gates we can come up with a logical all-to-all connectivity even on sparsely connected devices. Swap strategies are quite common in the literature in particular when using superconducting devices. Different approaches can be found in the literature, and we will see in practice how it ends up being on reality, but in 2022 (Weidenfeller et al. 2022) they showed how adding interchanging SWAP layers produces an virtual all to all connectivity with minimal depth increase.\n\n\n\nSwap strategy\n\n\n\n\n\nSwaping for full connectivity\n\n\nThey we will simply need to introduce an appropriate amount of SWAP gates when needed according to our target device coupling map. The downside of it is that our circuit depth will increase, more operations are added, but that is not bad, is it?\n\n\nNative gates\nOur logical framework for quantum computing lands into different devices. As we already mentioned when discussing different device technologies, the actuation over the qubits is performed in different ways:\n\nSuperconducting qubits use microwave pulses\nIon traps use lasers\nNeutral atoms use lasers as well, in a slightly different manner\n\nTherefore all operations need to find a translation that maps logical gates with the expected outcome of it, meaning a calibrated pulse needs to be found that renders the expected result. Following is the example of the Hadamard gate on a given qubit\n\n\n\nHadamard pulse\n\n\nA Gaussian shape pulse of 128 system cycle unit duration and an amplitude of 0.1. If this was the level required to work, probably less people would embrace quantum computing, therefore most companies offer an already calibrated set of operations that even though they are not the whole set of gates of the formalism, sets a basic and universal set of operations so that any logical circuit can be transform to those native gates.\nIf we access any of the IBM Quantum Computing devices we can see how they already show relevant information where we can encounter those natively accessible gates named as the basis gates\n\n\n\nIBM Perth information\n\n\n\nID is the identity operation\nX is the NOT gate and SX, the \\(\\sqrt{X}\\)\nRZ is the rotation over the \\(Z\\) axis given an angle \\(\\theta\\)\nCX is our only two-qubit gate and represents the CNOT\n\nThen, if we only have a CNOT gate how could we implement the SWAP gates we needed in order to create an all-to-all connected circuit? Tricky as it might be we need to find a way to decompose our logical gates into the gates we have available (Gokhale et al. 2021).\n\n\n\nTranspilation\nTranspilation refers to the systematic approach of changing a logical circuit to fit under the basis gates and coupling map for a given device. Reminds to the old compilation step needed in computer programs to fit a given architecture but in this case, given that still converts our source code in a different source-code, transpilation is the appropriate name.\nThese logical steps, once transpiled can be interpreted by the device knowing the type of pulse that needs to be sent to each qubit to represent the requested operation.\nIBM counts with one of the most mature transpilation modules out there and it is often used to translate the logical circuits to specifications for other devices. It is composed by different circuit passes focusing on fixing specific issues between logical circuit and physical device.\n\n\n\nPasses for circuit transpilation using Qiskit\n\n\nAt the end you will see the representation fo the pulsed schedule that is finally submitted to the deive following the architecture layer as in the image below.\n\n\n\nLayers from logical circuit to actual pulses\n\n\nMore on this can be found on IBM‚Äôs documentation\n\n\nNoise and errors\nAnd if that wasn‚Äôt enough, quantum devices are far from perfect when performing operations. Operations not always turn out as expected due to:\n\nMeasurement or Readout error\nOperation error rates\n\nAnd given the limited coupling ability and native gate translation, more operations than initially expected ones need to be added to the final circuit. The more operations we have, the more error we will add at the end state.\n\n\n\nIBM Jakarta\n\n\nAll this goes against the effort we have put when trying to solve our problem accurately using quantum computers. Lucky for us, there is people researching on how these effects can be minimized or suppressed, so let‚Äôs see a couple of examples. It is also relevant to clarify the lingo as it might get messy. They are trying to take us to the Fault-Tolerant Quantum Computing era. Some of the areas people are working on will be highlighted below.\nError suppression\nError suppression aims to leverage knowledge about the system and its fundamental errors to suppress their effect when someone runs a given circuit on those devices. It is often found at a fundamental level, really close o the hardware itself and should be easy to abstract the final user from these actions as they mostly require little knowledge about the executing circuit.\nA clear example of these techniques is Dynamical Decoupling. A techniques that introduces operations when qubits are idling so that they do not get affected by the surrounding qubtis and operations taking place.\n\n\n\nPulse scheme for dynamical decoupling\n\n\nDerivative Removal by Adiabatic Gate (DRAG) as an additional example, adds a component to the standard pulse shape to reduce qubits entering states higher than the 0 and 1 states we use for calculations. These techniques aim to fix fundamental issues related to the physical implementation.\nError mitigation\nThese techniques try to diminish the effect of noise during the execution by compensating for the systematic issues of the device itself. We could statistically characterize the systematic error upon redout and compensate for it after execution of the circuit.\n\n\n\nRedout error\n\n\nOr when expectation value is the obtained value, characterize the noise level and extrapolate the value we would have obtained in absence of noise for a given device.\n\n\n\nZero noise extrapolation (ZNE)\n\n\nSimple actions like layout selection, trying to select those qubits with less error on them or surrounding qubits might be simple and powerful enough for small circuits but it gets challenging on real use cases where depth and connectivity increase significantly.\nError correction\nError correction refers to the ultimate goal of many quantum computing companies, in particular hardware providers, where a logical implementation using redundant physical actions may lead to fault-tolerant device. The trick here is that our qubits will be a logical representation of what we have seen until now\n\\[\n|0\\rangle_L = |000000000\\rangle\n\\]\nAnd therefore, operations should also be modified to represent the actions on all physical qubits behind our logical actions. Below example is one of the canonical examples of error correcting codes designed by Peter Shor.\n\n\n\nShor‚Äôs error code\n\n\nFault Tolerant Quantum Computing would be the ideal scenario where a final user would only care about the logical implementation and the machine would deal with redundancy to fix errors and return the actual execution of the code in absence of noise. Sadly, we are still far from this future as it requires between hundreds and thousands of physical qubits to implement single logical qubits, increasing the scale up to million qubits needed for meaningful computation (see Google‚Äôs journey map).\nTherefore, we need to invest some time on manual error fixing and mitigation. Even though these techniques may sound complex, current status of frameworks such as Qiskit may help bridge the gap.\n\n\nAdditional means\nWell, I hope this wasn‚Äôt too intense. We got down to the inner workings of available Quantum Services and listed some of the impediments we might face when moving to actual devices. We did focus on Digital Quantum Computers being the ones that allow to move beyond the service offered by Quantum Annealers but does not require so much understanding as Analog Quantum Computers.\nBeing aware of the techniques that can be used we also showed how easy it is to implement those on actual devices on IBM‚Äôs Qiskit Runtime. Other cloud providers may enable other devices but will require additional knowledge on how to implement those techniques. We can benefit of some of the methods made available by Unitary Fund‚Äôs open-source community or niche players like Q-Ctrl:\n\nMitiq\nFire Opal\nSuperstaq\n\nMost likely quantum computing service providers will increase the abstraction making easier to implement algorithms in real hardware but for the moment, some work still needs to be done on our end as we saw. Let‚Äôs stick to simulators and emulators for now.\n\n\n\n\nGokhale, Pranav, Teague Tomesh, Martin Suchara, and Frederic T. Chong. 2021. ‚ÄúFaster and More Reliable Quantum SWAPs via Native Gates.‚Äù https://arxiv.org/abs/2109.13199.\n\n\nWeidenfeller, Johannes, Lucia C. Valor, Julien Gacon, Caroline Tornow, Luciano Bello, Stefan Woerner, and Daniel J. Egger. 2022. ‚ÄúScaling of the Quantum Approximate Optimization Algorithm on Superconducting Qubit Based Hardware.‚Äù Quantum 6 (December): 870. https://doi.org/10.22331/q-2022-12-07-870.",
    "crumbs": [
      "Quantum Computers",
      "Challenges"
    ]
  },
  {
    "objectID": "parts/algorithms/complexity.html",
    "href": "parts/algorithms/complexity.html",
    "title": "Types of algorithms",
    "section": "",
    "text": "Computational complexity\nWe do have a mathematical formalism to perform computation and devices capable of processing these instructions for us. Now we only need a set of relevant hard to solve problems that could benefit from those resources.\nA relevant aspect of quantum computing is that given the low level at which we will work, the words circuit and algorithm interchange. Classical computing algorithms are way more abstract nowadays than in the original days, closer to the actual electronics. This is where quantum computing still may confuse you as the steps of an algorithms are represented in blocks of logical gates composing a circuit.\nIn general by the term algorithm we refer to a set of operations that transform our initial quantum state into a final state. In this regard many types of algorithms can be found.\nCommunication\nQuantum Key Distribution (QKD), Superdense Coding and Quantum Teleportation are protocols involving quantum resources that help us communicate better, safer or faster.\nBuilding blocks\nCertain circuits/algorithms are also building blocks of more complex algorithms. For example, Quantum Fourier Transform (QFT) is used to map a given input to a phase change on the \\(Z\\) axis.\nIts inverse, makes the opposite, any operation that only rotates the status of a qubit on \\(Z\\) axis will render a superposition state when measured. Therefore, iQFT transforms this amplitude into a final state triggering the phase change of a given quantum state.\nThese are fundamental blocks for basic algorithms such as Quantum Phase Estimation which takes part in the famous factoring algorithm by Shor.\nCanonical examples\nThere are some algorithms every quantum computing specialist should be familiar with.\nMost of them are hard to realize due to the ancillary qubits required for the implementation. Other than basic examples, as soon as the problem grows in size it becomes challenging to implement those even using local simulators.\nOur main focus for this following section will be dedicated to combinatorial optimization. In this regime some specific quantum computing forms will be visited and how existing services can be used to actually implement those algorithms for specific purposes.\nIf one looks at the media it looks that ENTER THE NEW TREND HERE will be much better than what whatever we might be using. This is something Quantum Computing has also been affected by. Thus, it is important to frame the potential gain QC may introduce to our processes as depending on the complexity of it could well be there is no justification to adopt a new paradigm given the effort it requires.\nBQP (bounded error, quantum, polynomial time) is a superset of polynomial complexity class within NP (nondeterministic polynomial) problem space. Several problems fall into this category and are subject to be solved using quantum computing solutions (Wocjan and Zhang 2006).\nSo, let‚Äôs be aware that some problems can as well be efficiently solved by existing classical resources. Others, might be not even suitable for quantum computers being too hard to solve as well. So there is a sweet-spot where a set of problems may benefit from being solved using QC. Let‚Äôs not assume superiority by default.\nWe will start with a critical process for many companies and part of the key processes when it comes to Machine Learning.",
    "crumbs": [
      "Algorithms",
      "Types of algorithms"
    ]
  },
  {
    "objectID": "parts/algorithms/complexity.html#computational-complexity",
    "href": "parts/algorithms/complexity.html#computational-complexity",
    "title": "Types of algorithms",
    "section": "",
    "text": "Complexity depiction Taken from https://arxiv.org/pdf/1611.04471\n\n\n\n\n\n\n\n\n\nWocjan, Pawel, and Shengyu Zhang. 2006. ‚ÄúSeveral Natural BQP-Complete Problems.‚Äù https://arxiv.org/abs/quant-ph/0606179.",
    "crumbs": [
      "Algorithms",
      "Types of algorithms"
    ]
  },
  {
    "objectID": "parts/algorithms/optimization.html",
    "href": "parts/algorithms/optimization.html",
    "title": "Optimization",
    "section": "",
    "text": "Types of Optimization\nOptimization and operations research (OR) represent powerful analytical approaches that help businesses make better decisions by finding the best possible solutions to complex problems. At its core, optimization involves maximizing or minimizing an objective (such as profit, cost, or efficiency) while satisfying various constraints and limitations that real-world business situations impose.\nIn the business world, optimization problems are everywhere. Companies need to determine the most cost-effective way to distribute products across multiple warehouses, allocate limited budgets across marketing channels, schedule employees to meet service requirements, or decide which products to manufacture given resource constraints. Operations research provides the mathematical framework and computational tools to tackle these challenges systematically.\nThe value proposition for businesses is compelling: optimization can reduce costs, increase revenues, improve customer service, and enhance operational efficiency. For instance, airlines use optimization to set prices and manage seat inventory, logistics companies optimize delivery routes to minimize fuel costs and time, and manufacturers optimize production schedules to maximize throughput while minimizing waste.\nDifferent business problems require different optimization techniques, each suited to specific mathematical structures. These are particularized depending on the shape of the problem and its constraints.\nFor example, if our problem has a convex shape (meaning it looks like a bowl), the there is an entire field dedicated to it.\nBut depending on the variables to be used, for example restricting to a specific set of values, other specifications can be found:\nThese last problems often required a relaxed version of the problem where we solve using solvers suited for previous optimization techniques and then we iterate adding restrictions to approximate our goal. This technique is known as Branch-and-Bound.\nThe choice of optimization approach depends on the problem structure, data availability, and computational requirements. Modern businesses increasingly rely on these techniques as competitive advantages, using sophisticated software and algorithms to solve problems that would be impossible to address through intuition or simple heuristics alone. The key is matching the right optimization tool to the specific business challenge at hand.\nKnowing binary optimization is a challenging task and quantum computers took a chance as binary problems fall already in their nature.",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "parts/algorithms/optimization.html#types-of-optimization",
    "href": "parts/algorithms/optimization.html#types-of-optimization",
    "title": "Optimization",
    "section": "",
    "text": "Convex Optimization: studies the case when the objective function is convex (minimization) or concave (maximization) and the constraint set is convex. This can be viewed as a particular case of nonlinear programming or as generalization of linear or convex quadratic programming.\n\nLinear Programming (LP) handles problems where both the objective function and constraints are linear relationships. This is ideal for resource allocation problems, such as determining the optimal product mix to maximize profit given limited raw materials and production capacity.\nQuadratic Programming (QP) addresses situations where the objective function contains quadratic terms, often appearing in portfolio optimization where risk (measured as variance) needs to be balanced against expected returns.\n\nNonlinear Programming tackles problems with nonlinear objective functions or constraints, common in engineering design, pricing strategies, and situations where economies or diseconomies of scale exist.\nStochastic Programming addresses uncertainty by incorporating probabilistic elements, useful for supply chain planning under demand uncertainty or financial planning with market volatility.\n\n\n\nInteger Programming (IP) where all variables must be integer values.\nMixed-Integer Programming (MIP) combines continuous and discrete decision variables, making it perfect for problems involving yes/no decisions alongside quantity decisions. Examples include facility location problems (whether to open a warehouse) combined with distribution decisions (how much to ship).\nBinary Optimization (BO) is a special case of Integer programming where only to values are allowd for our variables, 0 or 1, hence the name binary.",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "parts/algorithms/adiabatic.html",
    "href": "parts/algorithms/adiabatic.html",
    "title": "Adiabatic Theorem",
    "section": "",
    "text": "Adiabatic Quantum Computing (AQC) is a model of computation that uses quantum-mechanical processes operating under adiabatic conditions. It is the main mechanism quantum computers use to obtain the minimum energy or ground state for a given problem, and it is the main approach we use to solve hard combinatorial optimization problems (Albash and Lidar 2018).\nThis type of quantum computing is based on continuous-time evolution of a quantum state \\(|\\psi(t)\\rangle\\) from a well-defined initial value to compute a final observed value. The evolution is modeled by the time-dependent Schr√∂dinger equation\n\\[\ni\\hbar \\frac{\\partial|\\psi(t)\\rangle}{\\partial t}= H(t)|\\psi(t)\\rangle\n\\]\noperating in the presence of adiabatic changes to the governing Hamiltonian \\(H(t)\\) over the range \\(t\\in\\left[0,T\\right]\\), where \\(\\hbar\\) is Planck‚Äôs constant. Adiabatic computing is computationally equivalent to all other quantum computing models, including the circuit and topological models, and it can efficiently solve any problem in BQP (Aharonov et al. 2008). However, it was originally proposed as a method for solving satisfiability problems (Farhi, Goldstone, and Gutmann 2000) and it has received attention for the simplicity by which combinatorial optimization problems can be cast in Hamiltonian forms (Lucas 2014).\nThe time dependent Hamiltonian for this type of computation is given by the following formula\n\\[\nH(t) = A(t)H_{A} + B(t)H_B\n\\]\nwhere \\(t\\in[0, T]\\) for the total time evolution \\(T\\). The temporal schedules (\\(A(t)\\) and \\(B(t)\\)) control the level of interpolation between the initial and final Hamiltonian (\\(H_A\\) and \\(H_B\\)). These Hamiltonians have some special characteristics:\n\n\\(H_A\\) is known, by known it also means we know its ground state and that we will be able to make it so that \\(|\\psi(t_0)\\rangle\\) matches that state.\n\\(H_B\\) is our target Hamiltonian. A mapping that represents our combinatorial optimization problem. We would like to obtain its ground state.\n\nBy making the schedule functions meet the conditions \\(A(t_0) = 1\\), \\(B(t_0)  =0\\), \\(A(T) = 0\\) and \\(B(T) = 1\\) we are able to produce an interpolated mixture of both Hamiltonians so that if the evolution is slow enough and the gap between ground state and other existed states remains the state produced at the end of the evolution should match the ground state of our final Hamiltonian (\\(H(T) = H_B\\)).\n\n\n\n\nAnnealing schedules, credit to DWave‚Äôs documentation\n\n\n\nA critical issue when performing this type of computation and selecting the right scheduling function is that the minimum spectral gap, energy spectrum gap between ground state and remaining excited states, is kept non-zero during the whole transition. Otherwise, no guarantee of obtaining the target state can exist. In particular, we should care about the gap expressed as\n\\[\ng_{min} = \\min_{0\\le t \\le T} \\min_{j\\ne0} |E_j(t) - E_0(t)|\n\\]\nVisually, you can see the issues.\n\n\n\n\nEnergy gap (https://arxiv.org/pdf/1611.04471)\n\n\n\nIt might be risky if at those points the evolution moves too fast. Therefore, we should select a scheduling function that apart from initial conditions also takes into consideration the compensation of the gap evolution to minimize transitions to higher energy states. Therefore, ending on local optima instead of maintaining in the global minima.\n\n\n\n\nScheduling function\n\n\n\nAs long as this evolution is above zero, we should be quite confident of the result at the end of the process (\\(T\\)).\nA common form for the initial Hamiltonian is \\(H_A = -\\sum_i^n \\sigma_{x_i}\\) being \\(\\sigma_x\\) the Pauli operator (previously mentioned \\(X\\) operator) applied to each \\(i\\) index qubit. This hamiltonian is chosen given that we do know its ground state \\(|+\\rangle^n\\) and how to prepare it in our systems (remember the Hadamard gate). One common target Hamiltonian form we can find in the literature is defined as the Ising model. It uses variables \\(s_i = \\pm 1\\) to characterize the magnetic dipole moment in order to characterize ferromagnetism. Its mathematical form is of the shape\n\\[\nH = -\\sum_{\\langle i j \\rangle} J s_i s_j - \\sum_j h s_j\n\\]\nwhere \\(h\\) characterizes the spin energy as well as their position preference (spin up or down) and \\(J\\) is the coupling strength between neighboring spins. It is a general form that might be used to characterize the contribution of individual and collective variables involved in a given process. The appropriate selection of \\(s_i \\in S\\) should render the option that minimizes/maximizes the energy of our Hamiltonian.\n\n\n\n\nIsing\n\n\n\nIt is the general case and many providers will likely have some decision already made for us (scheduling functions and initial Hamiltonian for example) but there are other cases where initial state and hamiltonian can be wisely selected so that the starting point is already close to the target state we would like to obtain. This example is quite common in cases like chemistry where HartreeFock states could be a better initialization or some relaxed version of the target problem could also be solved (classically) and its solution used to approximate the global optima (warm-starting).\n\n\n\n\nAharonov, Dorit, Wim Van Dam, Julia Kempe, Zeph Landau, Seth Lloyd, and Oded Regev. 2008. ‚ÄúAdiabatic Quantum Computation Is Equivalent to Standard Quantum Computation.‚Äù SIAM Review 50 (4): 755‚Äì87.\n\n\nAlbash, Tameem, and Daniel A. Lidar. 2018. ‚ÄúAdiabatic Quantum Computation.‚Äù Reviews of Modern Physics 90 (1). https://doi.org/10.1103/revmodphys.90.015002.\n\n\nFarhi, Edward, Jeffrey Goldstone, and Sam Gutmann. 2000. ‚ÄúA Numerical Study of the Performance of a Quantum Adiabatic Evolution Algorithm for Satisfiability.‚Äù arXiv Preprint Quant-Ph/0007071.\n\n\nLucas, Andrew. 2014. ‚ÄúIsing Formulations of Many NP Problems.‚Äù Frontiers in Physics 2: 5.",
    "crumbs": [
      "Optimization",
      "Adiabatic Theorem"
    ]
  },
  {
    "objectID": "parts/algorithms/aqc.html",
    "href": "parts/algorithms/aqc.html",
    "title": "AQC in practice",
    "section": "",
    "text": "We will make a simple example so that the formalism of this algorithm is clear to everyone. We will need for this exercise:\n\nA initial Hamiltonian and its ground state\nA final Hamiltonian\nA Scheduling function that governs the evolution and mixture between the two\n\nSo let‚Äôs start by selecting our initial Hamiltonian:\n\\[\nH_{init} = - \\sum_i^N \\sigma_i^x\n\\]\n\nN = 3 # 4 qubit system\n\n\nimport numpy as np\n\nsigma_x = np.matrix([[0, 1],\n                     [1, 0]])\n\nH_init = 0\nfor j in range(N):\n    H_init += -1.0 * np.kron( np.kron(np.identity(2**j), sigma_x), np.identity(2**(N-j-1)) )\n\nH_init\n\nmatrix([[ 0., -1., -1.,  0., -1.,  0.,  0.,  0.],\n        [-1.,  0.,  0., -1.,  0., -1.,  0.,  0.],\n        [-1.,  0.,  0., -1.,  0.,  0., -1.,  0.],\n        [ 0., -1., -1.,  0.,  0.,  0.,  0., -1.],\n        [-1.,  0.,  0.,  0.,  0., -1., -1.,  0.],\n        [ 0., -1.,  0.,  0., -1.,  0.,  0., -1.],\n        [ 0.,  0., -1.,  0., -1.,  0.,  0., -1.],\n        [ 0.,  0.,  0., -1.,  0., -1., -1.,  0.]])\n\n\nNow we will use a couple of functions to compute the Eigenspectra (set of eigenvalues for a given Hamiltonian) and the ground-state (minimum energy state).\n\nfrom math import sqrt\nfrom numpy.linalg import eig\n\ndef get_eigenspectra(h_mat):\n    \"\"\"\n    Computes the eigenspectra\n    \"\"\"\n    evals, evecs = eig(h_mat)\n    sort_index = np.argsort(evals)\n\n    return evals [sort_index], evecs[:, sort_index]\n\ndef get_gs(h_mat):\n    \"\"\" Computes the ground state \"\"\"\n    evals, evecs = eig(h_mat)\n    sort_index = np.argsort(evals)\n\n    stat_gs = evecs[:, sort_index[0]]\n    gs_val = evals[sort_index[0]]\n    \n    num = 1\n    for idx in sort_index[1:]:\n        if evals[idx] == gs_val:\n            stat_gs += evecs[:, idx]\n            num += 1\n        else:\n            break\n\n    return np.dot((1/sqrt(num)), stat_gs)\n\nSo, by computing the ground state of our initial Hamiltonian we can check that it is the superposition of all possible states with equal probability.\n\nget_gs(H_init)\n\nmatrix([[-0.35355339],\n        [-0.35355339],\n        [-0.35355339],\n        [-0.35355339],\n        [-0.35355339],\n        [-0.35355339],\n        [-0.35355339],\n        [-0.35355339]])\n\n\nNow for our target Hamiltonian we will select a random instance of the Ising model that looks like:\n\\[\nH_{problem} = \\sum_j^N J_{j,j+1}\\sigma_j^z\\sigma_{j+1}^z\n\\]\n\nJ = -1\n\nsigma_z = np.matrix([[1, 0],\n                     [0, -1]])\n\nH_problem = 0\nfor i in range(N-1):\n    H_problem = J* np.kron( np.kron(np.identity(2**i), sigma_z), np.identity(2**(N-i-1)) ) * np.kron( np.kron(np.identity(2**(i+1)), sigma_z), np.identity(2**(N-(i+1)-1)) )\n\nH_problem\n\nmatrix([[-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.]])\n\n\n\nget_gs(H_problem)\n\nmatrix([[0.5],\n        [0. ],\n        [0. ],\n        [0.5],\n        [0.5],\n        [0. ],\n        [0. ],\n        [0.5]])\n\n\nWe can see our Ising model has a ground state looking like\n\\[\n|\\psi\\rangle = \\frac{1}{2}|000\\rangle + \\frac{1}{2}|011\\rangle + \\frac{1}{2}|100\\rangle + \\frac{1}{2}|111\\rangle.\n\\]\nNow we would only need to define a scheduling function to mix both Hamiltonians. Just for simplicity we will use a single scheduling function \\(\\lambda(t)\\) and use its complementary for the decaying of the initial Hamiltonian.\n\\[\nH(t) = (1-\\lambda(t))H_{init} + \\lambda(t)H_{problem}\n\\]\n\ne0 = []\ne1 = []\ntime_range = np.arange(0.0, 1.0, 0.1)\n\nfor lambda_t in time_range:\n\n    H = (1-lambda_t)*H_init + lambda_t*H_problem\n\n    vals, stats = get_eigenspectra(H)\n    e0.append(vals[0])\n    e1.append(vals[1])\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(time_range, e0, time_range, e1)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(np.subtract(e1,e0))\nplt.show()\n\n\n\n\n\n\n\n\n\nstats[:, 0]\n\nmatrix([[0.49701447],\n        [0.05455839],\n        [0.05455839],\n        [0.49701447],\n        [0.49701447],\n        [0.05455839],\n        [0.05455839],\n        [0.49701447]])\n\n\nThere you go, the obtained state corresponds with the desired\n\\[\n|\\psi\\rangle = \\frac{1}{2}|000\\rangle + \\frac{1}{2}|011\\rangle + \\frac{1}{2}|100\\rangle + \\frac{1}{2}|111\\rangle.\n\\]\nup to a precision.",
    "crumbs": [
      "Optimization",
      "AQC in practice"
    ]
  },
  {
    "objectID": "parts/algorithms/problemposing.html",
    "href": "parts/algorithms/problemposing.html",
    "title": "Problem posing",
    "section": "",
    "text": "QUBO and Ising models\nLet‚Äôs explore a bit how D-Wave provides this service at scale. D-Wave‚Äôs technology is based on superconducting qubits, so they must keep their machines at 12mk temperature regime in order to maintain the quantumness of the system. Their machines are governed by an already fixed Hamiltonian that looks like:\n\\[\nH_{dwave} = -\\frac{A(s)}{2}\\left( \\sum_i \\sigma_x^i \\right) + \\frac{B(s)}{2}\\left( \\sum_i h_i \\sigma_z^i + \\sum_{i&gt;j} J_{ij} \\sigma_z^i \\sigma_z^j \\right)\n\\]\nwhere \\(\\sigma_{x,z}^i\\) are the Pauli matrices operating on qubit \\(i\\) and \\(h\\) and \\(J\\) are problem related variables. So as you can see most of the selection has already been done. We won‚Äôt enter into more details for the moment but for those interested in how this is done more in detail do not hesitate goring through the additional information.\nBut how do you transform any classical combinatorial problem into a Hamiltonian that fits into this form? This is what we are going to focus on for the next section.\nWe already discussed the Ising model and its potential limitations in terms of only managing quadratic relationships between variables. This is also limited by the hardware, so we will need to understand the way in which our classically formulated optimization problem may fit into this somehow fixed formulations.\nLet‚Äôs start with a simple example of the following form:\n\\[\n\\max_x \\sum_i a_i x_i \\quad\ns.t. \\quad \\sum_i b_i x_i = c\n\\]\nwhere \\(a_i\\) and \\(b_i\\) are free variables and \\(c\\) is a hard constraint. \\(x_i \\in \\{0, 1\\}\\) will be a mask over the selection on which variables should be selected to maximize our objective function. Why binary variables, well, in the end we can only obtain bit strings when reading our quantum states, so it seems sensible to make those the target variables of our problem.\nImmediately comes up is the management of restrictions as we will only be able to evaluate the energy level of the system so every bit of information will need to be handled within the objective function itself. That requires us to introduce constraints as regularization terms affecting the objective function in case of not being satisfied\n\\[\n\\sum_i b_i x_i = c \\quad \\rightarrow \\quad \\left( \\sum_i x_i b_i - c\\right)^2,\n\\]\nbeing the power needed to value any deviation (negative or positive) from our target value \\(c\\). These terms need to be conditioned by weight operators so that the problem is regularized, balancing between cost and regularization terms as follows\n\\[\n\\max_x \\theta_1 \\left( \\sum_i a_i x_i \\right) - \\theta_2 \\left( \\sum_i x_i b_i - c\\right)^2.\n\\]\nTherefore, unconstrained.\nAdditionally, our solvers will only be able to minimize the energy of the Hamiltonian so we should pose it as a minimization problem.\n\\[\n\\min_x \\theta_2 \\left( \\sum_i x_i b_i - c\\right)^2 - \\theta_1 \\left( \\sum_i a_i x_i \\right)\n\\]\nWith this it should be easy to see hot we can convert this problem into its Quadratic Unconstrained Binary Optimization (QUBO) form\n\\[\n\\min_x x^TQx + k.\n\\]\nHere \\(Q\\) represents the value matrix between each pair of variables (with the notion of \\(x_i^2 = x_i\\)) and the offset constant \\(k\\) (which will be added to the final result but does not affect the optimality search).\nOne interesting fact about this formulation is that it contains linear relations (\\(Q_{ii}x_i\\)) and quadratic for off diagonal terms (\\(Q_{ij}x_ix_j \\text{ for } i\\ne j\\)).We could pose it on its scalar form as well\n\\[\n\\min_x \\sum_i a_i x_i + \\sum_i \\sum_{j\\ne i} b_{ij} x_i x_j + k\n\\]\nwhich can be directly related to previously seen Ising type of Hamiltonian by the transformation \\(x_i \\in \\{0,1\\}\\) to \\(s_i = \\pm 1\\).\nOne clear limitation from the Ising formulation (or the native Hamiltonian of the target device) is the limitation for quadratic terms (\\(x_ix_j\\)) so no high-order relationship between variables can be implemented as a general norm. Also, binary characterization is needed, no continuous variable can be introduced within the problem as it is. Of course, there are ways to tackle problems that may require this modifications but it may need an extra effort when mapping our non-QUBO problem to the device.\nD-Wave already envisioned a way to ease this tasks, so it provides libraries and functionalities that should help with part of the work to be done. Particularly, transforming from one shape to another and submitting problems to their devices (or trying to solve them locally).\nPyQUBO documentation\ndimod documentation\nBeing the first always give some advantage, so may of the companies that came after DWave have adopted this libraries and offer certain interoperability. You may find some examples in B Hybrid Solvers",
    "crumbs": [
      "Optimization",
      "Problem posing"
    ]
  },
  {
    "objectID": "parts/algorithms/problemposing.html#qubo-and-ising-models",
    "href": "parts/algorithms/problemposing.html#qubo-and-ising-models",
    "title": "Problem posing",
    "section": "",
    "text": "from pyqubo import Spin\n\ns1, s2, s3, s4 = Spin(\"s1\"), Spin(\"s2\"), Spin(\"s3\"), Spin(\"s4\")\n\n# Hamiltonian\nH = (4*s1 + 2*s2 + 7*s3 + s4)**2\n\n# model creation\nmodel = H.compile()\n\n# QUBO request (to_ising() exists as well )\nqubo, offset = model.to_qubo()\npprint(qubo)\n\n{('s1', 's1'): -160.0,\n('s1', 's2'): 64.0,\n('s1', 's3'): 224.0,\n('s1', 's4'): 32.0,\n('s2', 's2'): -96.0,\n('s2', 's3'): 112.0,\n('s2', 's4'): 16.0,\n('s3', 's3'): -196.0,\n('s3', 's4'): 56.0,\n('s4', 's4'): -52.0}\n\nimport dimod\ndimod.qubo_to_ising({('x1', 'x1'): -22, ('x2', 'x2'): -6, ('x3', 'x3'): -14,\n                     ('x1', 'x2'): 20, ('x1', 'x3'): 28},\n                     offset=9)\n\n({'x1': 1.0, 'x2': 2.0, 'x3': 0.0}, {('x1', 'x2'): 5.0, ('x1', 'x3'): 7.0}, 0.0)",
    "crumbs": [
      "Optimization",
      "Problem posing"
    ]
  },
  {
    "objectID": "parts/algorithms/problemposing.html#mapping-the-problem",
    "href": "parts/algorithms/problemposing.html#mapping-the-problem",
    "title": "Problem posing",
    "section": "Mapping the problem",
    "text": "Mapping the problem\nIdeally, after this initial conversion has been done, the only piece missing is the submission of our job to one of those solvers enabled by our preferred quantum annealing service provider. But hardware might require additional tweaks.\nWe will need to get a little bit used to the transformation between different formulations for a given problem.\n\n\n\n\nModel comparison\n\n\n\nBut more importantly, as those may represent any \\(i\\) \\(j\\) indexes to be connected, the hardware may not present all required connections, in particular in all-to-all cases. Trapped ions make it easy but the rest, require tweaking.\nEmbedding\nLooking at the connectivity of the chip, if an index represents a red ball and each line gets tuned by the values in our problem \\(q_{ij}\\), how can we tune those that have no red line between them?\n\n\n\n\nDWave chip architecture\n\n\n\nQPU Architecture\nWe might need to find ways to connect qubits that are not directly connected and in D-Wave services we can chain connected qubits so that we could place the potential value of a distant qubit in linked different locations of a chip. This is done by artificially introducing additional variables that are mapped to qubits whose value should align to the qubit they are chained to. A visual representation of this would look like\n\n\n\n\nDWave chain\n\n\n\nThat way if we look to solve our three variable problem into a 4 qubit chip, we might see more than three qubit related readout being reported.\n\n\n\n\nChained solution\n\n\n\nIn above example, 0 and 5 qubits are linked to the same problem variable so they should report same bit readout but qubit chains can break and therefore we will need to play with the chain strength to avoid any invalid solution.\nSadly for us, the embedding of densely connected problems comes with the overhead of chaining a lot of physical qubits so we can make all those all-to-all connections that are not native to the device. In fact, we can check that the encoding they have made (by they we mean the people from D-Wave) we can see the maximum number of qubits of the problems encoded in hardware is in the order of 60 to 100. But is this using thousands of qubits? Yes, this is how bad the overhead can get (Cai, Macready, and Roy 2014).\n\n\n\n\nCai, Jun, William G. Macready, and Aidan Roy. 2014. ‚ÄúA Practical Heuristic for Finding Graph Minors.‚Äù https://arxiv.org/abs/1406.2741.",
    "crumbs": [
      "Optimization",
      "Problem posing"
    ]
  },
  {
    "objectID": "parts/algorithms/quboformulations.html",
    "href": "parts/algorithms/quboformulations.html",
    "title": "QUBO formulation",
    "section": "",
    "text": "Binary Linear Programming\nAs we saw during class, one important part of being able to use those quantum computing devices is posing our problem as a matrix representation following the QUBO formulation. We could then convert it to Ising type if needed but we will see some of the most common frameworks already provide a translation mechanism between the too.\nTherefore a critical aspect of it will be knowing how to transform our penalty terms so that the fit into the objective function to be minimized.\nKnown penalties\nbeing \\(P\\) a positive scalar characterizing the effect of the penalty over the cost function.\nWe will see how some classical problems can be mapped to their QUBO form so that this problems can be solved by Quantum Algorithms.\nTaken from (Lucas 2014), Binary Linear Programming problem tries to solve a problem of the shape \\(Sx = b\\) where \\(x\\) is the unknown vector of binary variables to be solved. It is a simple example as it already provides the shape we need being variables binary, no higher order relation exists and has no constraints to care about. In order to solve this problem we will need to reformulate our previous form into:\n\\[\n\\arg \\min_{x} \\| Sx - b \\|\n\\]\n\\[\nH_A = \\sum_j^m \\left[b_j - \\sum_i^N S_{ji}x_ix_j \\right]^2\n\\]\naiming to obtain our solution for \\(x\\) when \\(H_A = 0\\) so the minimization is done over \\(H_A\\) like follows\n\\[\n\\min_x \\sum_j^m \\left[b_j - \\sum_i^N S_{ji}x_ix_j \\right]^2\n\\]\nLet‚Äôs select a particular instance for \\(S\\) and \\(b\\).\nimport numpy as np\n\nS = np.array([\n    [3, 4, 1, 1, 2],\n    [1, 9, 2, 2, 1],\n    [6, 8, 2, 2, 1]\n])\nb = np.array([10, 13, 17])\nSo, now we can start composing our \\(Q\\) matrix.\nimport itertools\n\nn = S.shape[1]\nm = b.shape[0]\n\nQ = np.zeros((n, n))\n\nfor j in range(m):\n    # Diagonal terms\n    for i in range(n):\n        Q[i][i] -= 2 * b[j] * S[j][i]\n\n    tuples = itertools.product(range(n), repeat=2)\n    for tuple_ in tuples:\n        x = tuple_[0]\n        y = tuple_[1]\n        # Divided as we are adding same value to ij and ji\n        Q[x][y] += S[j][x] * S[j][y] / 2.\n        Q[y][x] += S[j][x] * S[j][y] / 2.\n\nQ\n\narray([[-244.,   69.,   17.,   17.,   13.],\n       [  69., -425.,   38.,   38.,   25.],\n       [  17.,   38., -131.,    9.,    6.],\n       [  17.,   38.,    9., -131.,    6.],\n       [  13.,   25.,    6.,    6.,  -94.]])\nAs you can see the matrix is symetric as there is a natural symmetry between \\(x_ix_j\\) and \\(x_jx_i\\). We could brute force our solution.\nmin_val = 0\nmin_sol = []\n\nfor x in np.c_[tuple(i.ravel() for i in np.mgrid[:2, :2, :2, :2, :2])]:  # noqa\n    if x @ Q @ x.T &lt; 0:\n        if (x @ Q @ x.T) &lt; min_val:\n            min_val = x @ Q @ x.T\n            min_sol = x\n\nprint(min_sol, \"|\", min_val)\n\n[1 1 0 1 1] | -558.0\nSo our solution should be [1 1 0 1 1], let‚Äôs check.\nnp.matmul(S, [1, 1, 0, 1, 1])\n\narray([10, 13, 17])\nb\n\narray([10, 13, 17])\nThere you go, it is the actual solution.",
    "crumbs": [
      "Optimization",
      "QUBO formulation"
    ]
  },
  {
    "objectID": "parts/algorithms/quboformulations.html#maxcut",
    "href": "parts/algorithms/quboformulations.html#maxcut",
    "title": "QUBO formulation",
    "section": "MaxCut",
    "text": "MaxCut\nMaximum cut is another canonical example often use to characterize the flow between the nodes of a network mapping a distribution network, electricity or telecommunication networks or even circuit design. The main idea comes from splitting the vertices of a graph into two complementary sets such that the number of edges is maximally cut.\n\n\n\n\n\n\nNote\n\n\n\nNetworkX package will need to be added to your Python environment for this example_\n\n\n\nimport networkx as nx\nfrom networkx.generators.random_graphs import gnm_random_graph\n\ndef gen_graph(size, seed=None):\n    if seed is not None:\n        return gnm_random_graph(size, size, seed=seed)\n    return gnm_random_graph(size, size)\n\n\nG = gen_graph(5, None)\nnx.draw(G)\n\n\n\n\n\n\n\n\nWe can model this problem by identifying each node \\(j\\) in one or other set of the two possible options (\\(x_j = 0\\) or \\(x_j = 1\\)). With that we can set the formula \\(x_i + x_j - 2x_ix_j\\) so that identifies whether edge \\((ij)\\) is in the cut (if only one \\(x\\) is active then it will equal two 1).\nThat let‚Äôs us with the general formula\n\\[\n\\max \\sum_{(i,j)} (x_i + x_j - 2x_ix_j)\n\\]\nwhich we can convert to a minimization problem by changing the sign.\n\nn = G.order()\nnodes = list(G.nodes)\n\nQ = np.zeros((n, n), dtype=np.dtype(np.int32))\nfor edge in G.edges:\n    idx1 = nodes.index(edge[0])\n    idx2 = nodes.index(edge[1])\n    Q[idx1][idx1] += 1\n    Q[idx2][idx2] += 1\n    \n    # Half the value as it is introduced twice\n    Q[idx1][idx2] -= 1\n    Q[idx2][idx1] -= 1\n\nQ = -1.0*Q # Min\n\n\nQ\n\narray([[-0., -0., -0., -0., -0.],\n       [-0., -2.,  1., -0.,  1.],\n       [-0.,  1., -3.,  1.,  1.],\n       [-0., -0.,  1., -2.,  1.],\n       [-0.,  1.,  1.,  1., -3.]])\n\n\n\nmin_val = 0\nmin_sol = []\n\nfor x in np.c_[tuple(i.ravel() for i in np.mgrid[:2, :2, :2, :2, :2])]:  # noqa\n    if x @ Q @ x.T &lt; 0:\n        if (x @ Q @ x.T) &lt; min_val:\n            min_val = x @ Q @ x.T\n            min_sol = x\n\nprint(min_sol, \"|\", min_val)\n\n[0 0 1 0 1] | -4.0\n\n\nThis case is a little bit more complicatd as more than one option can be selected. Ideally we would like to obtain all potential solutions.\n\nsolution = np.array(min_sol)\nred_team = np.where(solution == 1) \nblue_team = np.where(solution == 0) \n\n\npos = nx.spring_layout(G)  # positions for all nodes\n\n# nodes\noptions = {\"edgecolors\": \"tab:gray\", \"node_size\": 800, \"alpha\": 0.9}\nnx.draw_networkx_nodes(G, pos, nodelist=red_team[0], node_color=\"tab:red\", **options)\nnx.draw_networkx_nodes(G, pos, nodelist=blue_team[0], node_color=\"tab:blue\", **options)\n\nnx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5);",
    "crumbs": [
      "Optimization",
      "QUBO formulation"
    ]
  },
  {
    "objectID": "parts/algorithms/quboformulations.html#knapsack-problem-quadratic",
    "href": "parts/algorithms/quboformulations.html#knapsack-problem-quadratic",
    "title": "QUBO formulation",
    "section": "Knapsack problem (quadratic)",
    "text": "Knapsack problem (quadratic)\nFitting a set of assets according to the size of the bag we want to use can be a hard combinatorial problem. Not that we are going to fo the full fledge example but it is interesting to look at it from an implementation perspective.\nQuadratic Knapsack corresponds to the problem where interaction exists into the variables we would like to model:\n\\[\nmax \\sum_{i=1}^{n-1} \\sum_{j=i}^n w_{ij}x_i x_j\n\\]\nconsidering there is a budget constraint so that\n\\[\n\\sum_{j=1}^n a_j x_j \\le b.\n\\]\nVariables \\(x_i\\) represent the mask of selecting or not \\(i\\) element to be considered. \\(w_{ij}\\) sets the value of taking in two elements at the same time, \\(a_j\\) represents the weight associated to \\(j\\) and \\(b\\) sets the available budget for the whole thing (the size of our Knapsack indeed). As can be seen many different types of problems can be mapped to this canonical example.\nOn relevant aspect of inequalities is that we will need to add slack variables to take the excess of the constraint $a_j x_j + S = b $. In this case \\(S\\) will be a value between 0 and \\(b\\) so we would need to characterize its binary representation so that the problem is still a binary quadratic problem still (\\(S = 2^ns_n + \\dots + 2^0s_0\\))\nWe can start by adding the restriction to the objective function followed by a penalty term.\n\\[\nmax \\sum_{i=1}^{n-1} \\sum_{j=i}^n w_{ij}x_i x_j - P\\left(\\sum_{j=1}^n a_j x_j + S - b\\right)^2\n\\]\n\n\n\n\n\n\nNote\n\n\n\nTest case from: https://arxiv.org/pdf/1811.11538.pdf\n\n\n\nweight = np.array([\n            [2, 4, 3, 5],\n            [4, 5, 1, 3],\n            [3, 1, 2, 2],\n            [5, 3, 2, 4]\n        ])\ncosts = [8, 6, 5, 3]\nbudget = 16\n\nFor this case 2 slack variables might be enough to characterize the excess of \\(S\\).\n$$\n8x_1 + 6x_2 + 5x_3 + 3x_4 + 1s_0 + 2s_1 = 16\n$$\n\ncosts.append(1)\ncosts.append(2)\ncosts\n\n[8, 6, 5, 3, 1, 2]\n\n\nAnd penalty, well, we need to decide on a figure.\n\nP = 10\n\n\nn = len(costs)\n\nQ = np.zeros((n, n))\n# First term\nfor i in range(len(weight)):\n    for j in range(len(weight)):\n        Q[i][j] += weight[i][j]\n\n# Constraint section\nfor i in range(n):\n    for j in range(n):\n        Q[i][j] -= P * costs[i] * costs[j]\n    Q[i][i] += P * 2 * budget * costs[i]\n\nQ = -1.0*Q # min\n\n\nQ\n\narray([[-1922.,   476.,   397.,   235.,    80.,   160.],\n       [  476., -1565.,   299.,   177.,    60.,   120.],\n       [  397.,   299., -1352.,   148.,    50.,   100.],\n       [  235.,   177.,   148.,  -874.,    30.,    60.],\n       [   80.,    60.,    50.,    30.,  -310.,    20.],\n       [  160.,   120.,   100.,    60.,    20.,  -600.]])\n\n\n\nmin_val = 0\nmin_sol = []\n\nfor x in np.c_[tuple(i.ravel() for i in np.mgrid[:2, :2, :2, :2, :2, :2])]:  # noqa\n    if x @ Q @ x.T &lt; 0:\n        if (x @ Q @ x.T) &lt; min_val:\n            min_val = x @ Q @ x.T\n            min_sol = x\n\nprint(min_sol, \"|\", min_val)\n\n[1 0 1 1 0 0] | -2588.0\n\n\nPlay around with the penalty term to check its effect in the final solution. Change costs so we can see the evolution of the solution and slack variables being used.",
    "crumbs": [
      "Optimization",
      "QUBO formulation"
    ]
  },
  {
    "objectID": "parts/algorithms/quboformulations.html#traveling-salesperson-problem",
    "href": "parts/algorithms/quboformulations.html#traveling-salesperson-problem",
    "title": "QUBO formulation",
    "section": "Traveling Salesperson Problem",
    "text": "Traveling Salesperson Problem\nAnother example problem that has been covered in the literature. It is a problem that tries to identify the optimal route to cover a set of cities given that there is a cost associated to going from point \\(a\\) to point \\(b\\).\n\n\n\nLet‚Äôs start by defining our variables \\(x_{ij}\\) which will take the positive value when the route is connected by node \\(i\\) to \\(j\\).\n\\[\nx_{i \\, j} =\n    \\begin{cases}\n        1, & \\text{$i$ is conected to $j$} \\\\\n        0, & \\text{otherwise}\n    \\end{cases}\n\\]\nwith a distance/cost of \\(c_{ij}\\). The idea is to visit all nodes but without going over the same route more than once. An ideally doing it at a minimum cost. Therefore,\n\\[\n\\min \\sum_i^n \\sum_{j, j \\ne i}^n c_{ij}x_{ij}\n\\]\nBut a set of conditions need to be met, in particular there is a temporal axis we will also need to consider so that by starting at a given position each time step requires movements forward. This will add a \\(t\\) for each step of the whole route to our previous variables (cost won‚Äôt be affected much) so that we can identify the start and ending point (\\(x_{ijt}\\)). Therefore, previous case will require a \\(2^4 \\times 2^4\\) matrix to characterize the whole set of possible edges (those that exist and those which don‚Äôt).\nConstraints to have in mind, the salesperson should use each road only once. That means we should limit \\(\\forall i \\in \\{0...n\\} \\sum_t \\sum_i^n \\sum_{j, j \\ne i} x_{ijt} = 1\\) both row and column-wise.\n\nG = nx.DiGraph()\nG.add_weighted_edges_from({\n    (\"A\", \"B\", 20), (\"A\", \"C\", 42), (\"A\", \"D\", 35), (\"B\", \"A\", 20),\n    (\"B\", \"C\", 30), (\"B\", \"D\", 34), (\"C\", \"A\", 42),(\"C\", \"B\", 30),\n    (\"C\", \"D\", 12), (\"D\", \"A\", 35), (\"D\", \"B\", 34), (\"D\", \"C\", 12)\n})\n\nnx.draw(G)\n\n\n\n\n\n\n\n\n\n# Usually a good estimate for a lagrange parameter is between 75-150%\n# of the objective function value, so we come up with an estimate for \n# tour length and use that.\nlagrange = 100\n\n\nfrom collections import defaultdict\n\n# Creating the QUBO\nQ = defaultdict(float)\n\n# number of steps\nN = G.number_of_nodes()\n\n# Constraint that each row has exactly one 1\nfor node in G:\n    for pos_1 in range(N):\n        Q[((node, pos_1), (node, pos_1))] -= lagrange\n        for pos_2 in range(pos_1+1, N):\n            Q[((node, pos_1), (node, pos_2))] += lagrange\n\n# Constraint that each col has exactly one 1\nfor pos in range(N):\n    for node_1 in G:\n        Q[((node_1, pos), (node_1, pos))] -= lagrange\n        for node_2 in set(G)-{node_1}:\n            # QUBO coefficient is 2*lagrange, but we are placing this value \n            # above *and* below the diagonal, so we put half in each position.\n            Q[((node_1, pos), (node_2, pos))] += lagrange\n\n# Objective that minimizes distance\nfor u, v in G.edges:\n    for pos in range(N):\n        nextpos = (pos + 1) % N\n\n        # going from u -&gt; v\n        try:\n            value = G[u][v][\"weight\"]\n        except KeyError:\n            value = lagrange\n\n        Q[((u, pos), (v, nextpos))] += value\n\n        # going from v -&gt; u\n        try:\n            value = G[v][u][\"weight\"]\n        except KeyError:\n            value = lagrange\n\n        Q[((v, pos), (u, nextpos))] += value\n\nThis challenges have been also classically solved using simulated annealing procedures.\n\nfrom networkx.algorithms import approximation as approx\n\ncycle = approx.simulated_annealing_tsp(G, \"greedy\", source=\"D\")\ncost = sum(G[n][nbr][\"weight\"] for n, nbr in nx.utils.pairwise(cycle))\nprint(f\"Cost for route {cycle} is {cost}\")\n\nCost for route ['D', 'C', 'B', 'A', 'D'] is 97\n\n\nAnd can be also solved by traversing the whole solution space (if possible).\n\nimport dimod\n\nsampler = dimod.ExactSolver()\n\nresponse = sampler.sample_qubo(Q)\n\nprint(response.slice(0, 10))\n\n  ('A', 0) ('A', 1) ('A', 2) ('A', 3) ('B', 0) ... ('D', 3) energy num_oc.\n0        1        0        0        0        0 ...        0 -606.0       1\n1        0        0        1        0        0 ...        1 -606.0       1\n2        0        1        0        0        1 ...        0 -606.0       1\n3        0        0        1        0        0 ...        0 -606.0       1\n4        0        0        0        1        1 ...        0 -606.0       1\n5        0        1        0        0        0 ...        0 -606.0       1\n6        0        0        0        1        0 ...        0 -606.0       1\n7        1        0        0        0        0 ...        1 -606.0       1\n8        1        0        0        0        0 ...        0 -584.0       1\n9        0        1        0        0        0 ...        1 -584.0       1\n['BINARY', 10 rows, 10 samples, 16 variables]\n\n\n\nresponse.first.sample\n\n{('A', 0): np.int8(1),\n ('A', 1): np.int8(0),\n ('A', 2): np.int8(0),\n ('A', 3): np.int8(0),\n ('B', 0): np.int8(0),\n ('B', 1): np.int8(0),\n ('B', 2): np.int8(0),\n ('B', 3): np.int8(1),\n ('C', 0): np.int8(0),\n ('C', 1): np.int8(0),\n ('C', 2): np.int8(1),\n ('C', 3): np.int8(0),\n ('D', 0): np.int8(0),\n ('D', 1): np.int8(1),\n ('D', 2): np.int8(0),\n ('D', 3): np.int8(0)}\n\n\n\nroute = response.first.sample\n\ncycle = [None]*5\nfor key in route:\n    if route[key] == 1:\n        cycle[key[1]] = key[0]\ncycle[-1] = cycle[0]\n\ncost = sum(G[n][nbr][\"weight\"] for n, nbr in nx.utils.pairwise(cycle))\nprint(f\"Cost for route {cycle} is {cost}\")\n\nCost for route ['A', 'D', 'C', 'B', 'A'] is 97\n\n\n\nimport neal\n\nsampler = neal.SimulatedAnnealingSampler()\nresponse = sampler.sample_qubo(Q)\nprint(response)\n\n  ('A', 0) ('A', 1) ('A', 2) ('A', 3) ('B', 0) ... ('D', 3) energy num_oc.\n0        0        0        1        0        0 ...        0 -584.0       1\n['BINARY', 1 rows, 1 samples, 16 variables]\n\n\n\nroute = response.first.sample\n\ncycle = [None]*5\nfor key in route:\n    if route[key] == 1:\n        cycle[key[1]] = key[0]\ncycle[-1] = cycle[0]\n\ncost = sum(G[n][nbr][\"weight\"] for n, nbr in nx.utils.pairwise(cycle))\nprint(f\"Cost for route {cycle} is {cost}\")\n\nCost for route ['D', 'C', 'A', 'B', 'D'] is 108\n\n\n\n\n\n\nLucas, Andrew. 2014. ‚ÄúIsing Formulations of Many NP Problems.‚Äù Frontiers in Physics 2: 5.",
    "crumbs": [
      "Optimization",
      "QUBO formulation"
    ]
  },
  {
    "objectID": "parts/algorithms/usingexistingframeworks.html",
    "href": "parts/algorithms/usingexistingframeworks.html",
    "title": "1¬† D-Wave and its ecosystem",
    "section": "",
    "text": "1.1 Portfolio optimization\nFollowing notebook will dig deeper into the existing tools that D-Wave and its ecosystem enables when tackling combinatorial problems.\nFor this exercises following libraries will be used: PyQubo as well as dimod and dwave-ocean-sdk from the DWave Ocean suite\nOften the installation of the SDK already installs the other two dependencies, but just in case make sure all are present.\nPyQubo offers a simple way to come up with those formulations, express them in a simple way and then materialize the QUBO matrix or Ising coefficients.\nPortfolio optimization is a simple exercise that matches pretty nicely the type of formulation we would like to produce. Comes from the definition of Markowitz‚Äôs seminal work.\n\\[\n\\max \\sum_i r_ix_i - \\sum_i \\sum_j c_{ij}x_ix_j \\\\\n\\text{ s. t. } \\sum_i b_ix_i \\le B\n\\]\nwhere \\(r_i\\) represents the return of an asset, the \\(c_{ij}\\) is the risk of both assets being driven by the same factors. We should diversify when it comes to investment portfolios. \\(b_i\\) is the cost of investing in an asset and \\(B\\) is all the budget you have.\nQUBO\n\\[\\begin{equation}\n\\begin{split}\n\\min_{x_{i}\\in{\\{0,1\\}}} \\;\\; & \\theta_{1}\\sum_{i=1}^{n}-x_{i}r_i \\\\\n\\;\\;  +\\;&\\theta_{2}\\sum_{i,j=1}^{n}x_{i}x_{j}c_{ij} \\\\\n+\\;&\\theta_{3}\\left(\\sum_{i=1}^{n}x_{i}b_{i}-B\\right)^{2}\n\\end{split}\n\\end{equation}\\]\nThe parameters \\(0\\leq\\theta_{1},\\theta_{2},\\theta_{3}&lt;\\infty\\) represent the relative importance of each term to the decision maker, and she is free to change these parameters to best reflect that.\nLet‚Äôs select a set of assets and their potential (revenue and covariance).\nimport numpy as np\n\nr_i = np.array([77.60417796, 29.71772711, 19.06733564, 32.14568747, 95.12069674, 32.7466954 , 56.48788447])\n\nb_i = np.array([75, 23, 20, 33, 95, 29 , 25])\n\nB = sum(b_i)*0.66\n\nc_ij = np.array([[162.22030329,  16.96480901,  53.33392587, -61.19096975, -21.76232416, -93.48033116,  19.59907177],\n                 [ 16.96480901,  69.33332072,  16.523284  ,  10.15842664, -10.78956293,  24.00854339, -14.78713228],\n                 [ 53.33392587,  16.523284  ,  43.90334576, -11.43885741, -21.68843841, -22.09673565,  -5.10928035],\n                 [-61.19096975,  10.15842664, -11.43885741,  46.42412482, -2.8970541 ,  49.90788498, -31.74061021],\n                 [-21.76232416, -10.78956293, -21.68843841,  -2.8970541 , 23.59596426,   4.87211145,  16.74718009],\n                 [-93.48033116,  24.00854339, -22.09673565,  49.90788498, 4.87211145,  81.35591517, -26.58473057],\n                 [ 19.59907177, -14.78713228,  -5.10928035, -31.74061021, 16.74718009, -26.58473057,  40.90234169]])\nfrom pyqubo import Array, Placeholder, Constraint\n\nnum_assets = len(r_i)\nx = Array.create('x', shape=num_assets, vartype='BINARY')\n# Profit generated by each asset individually\nH_linear_profit = 0.0\nfor i in range(num_assets):\n    H_linear_profit += Constraint(\n        r_i[i] * x[i], label='profit({})'.format(i)\n    )\n# Risk obtained from the covariance matrix\nH_quadratic = 0.0\nfor i in range(num_assets):\n    for j in range(i + 1, num_assets):\n        H_quadratic += Constraint(\n            c_ij[i][j] * x[i] * x[j], label='risk({}, {})'.format(i, j)\n        )\n# Constraint (budget)\nH_linear_budget = 0.0\nfor i in range(num_assets):\n    H_linear_budget += Constraint(b_i[i]*x[i], label='slot({})'.format(i))\n# Build model.\ntheta1 = Placeholder('theta1')\ntheta2 = Placeholder('theta2')\ntheta3 = Placeholder('theta3')\nH = - theta1*H_linear_profit + theta2 * H_quadratic + theta3 * (H_linear_budget - B)**2\nmodel = H.compile()\nData comes already in the model but we can select and play around with the regularization parameters to find a good balance in case of having no intuition by all means.\n# Set the Lagrange multipliers\ntheta1=0.5 \ntheta2=0.3\ntheta3=0.2\nfeed_dict = {'theta1': theta1, 'theta2' : theta2, 'theta3' : theta3}\n\n# Transform to QUBO.\nqubo, offset = model.to_qubo(feed_dict=feed_dict)\nimport dimod\n\nsampler = dimod.ExactSolver()\n\nresponse = sampler.sample_qubo(qubo)\n\nprint(response.slice(10))\n\n  x[0] x[1] x[2] x[3] x[4] x[5] x[6]       energy num_oc.\n0    1    1    1    1    0    1    1 -7977.664565       1\n1    1    0    0    0    1    1    0 -7976.446948       1\n2    0    1    1    1    1    0    1 -7972.776279       1\n3    0    0    1    1    1    1    1 -7970.392709       1\n4    1    0    0    1    1    0    0 -7963.990385       1\n5    0    1    1    0    1    1    1 -7961.841598       1\n6    1    1    0    1    0    1    1 -7953.494598       1\n7    1    0    0    0    1    0    1 -7949.231201       1\n8    0    1    0    1    1    1    1 -7948.440829       1\n9    1    1    0    0    1    0    0 -7941.697424       1\n['BINARY', 10 rows, 10 samples, 7 variables]\nsolution = []\nfor k,v in response.first.sample.items():\n    solution.append(v)\ne_cost = np.round(np.sum(np.multiply(b_i,solution)),2)\nprint(f'The effective cost of this investment is {e_cost} for the budget limit of {B}')\n\nThe effective cost of this investment is 205 for the budget limit of 198.0\nSeems like a hard problem to be solved. Most of the energy values are really close around the global optima. Play around with the regularization parameters to check the difference in results.\nNow we can take our example to D-Wave‚Äôs samples and see what we get. Make sure you copy your token into the code below.\nimport minorminer\nfrom dwave.system.samplers import DWaveSampler\nfrom dwave.system.composites import FixedEmbeddingComposite\n\n# Instanciate Sampler\ndwave_sampler = DWaveSampler(token=\"&lt;your-token&gt;\")\n\n# Construct a problem\nbqm = dimod.BinaryQuadraticModel(qubo, dimod.BINARY)\n\n# Get the edge list\ntarget_edgelist = dwave_sampler.edgelist\n\n# And source edge list on the BQM quadratic model\nsource_edgelist = list(bqm.quadratic)\n\n# Find the embeding\nembedding = minorminer.find_embedding(source_edgelist, target_edgelist)\nsampler = FixedEmbeddingComposite(dwave_sampler, embedding)\n# DWave sampler parameters\namode = 'histogram'\nallowed_ta = dwave_sampler.properties['annealing_time_range'] #microsecond\nnum_reads=100\nta = 10 # microseconds\nresponse = sampler.sample_qubo(qubo, num_reads=num_reads, annealing_time=ta, answer_mode=amode)\nprint(response.slice(10))\n\n  x[0] x[1] x[2] x[3] x[4] x[5] x[6]       energy num_oc. chain_b.\n0    1    1    1    1    0    1    1 -7977.664565       4      0.0\n1    1    0    0    0    1    1    0 -7976.446948       1 0.142857\n2    1    0    0    0    1    1    0 -7976.446948       5      0.0\n3    0    1    1    1    1    0    1 -7972.776279       3      0.0\n4    0    0    1    1    1    1    1 -7970.392709       1      0.0\n5    1    0    0    1    1    0    0 -7963.990385       5      0.0\n6    0    1    1    0    1    1    1 -7961.841598       2      0.0\n7    1    1    0    1    0    1    1 -7953.494598       2      0.0\n8    1    0    0    0    1    0    1 -7949.231201       3      0.0\n9    1    1    0    0    1    0    0 -7941.697424       6      0.0\n['BINARY', 10 rows, 32 samples, 7 variables]\nYou will see D-Wave already provides a lot of information about how to implement some of those canonical examples we went through previously.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>D-Wave and its ecosystem</span>"
    ]
  },
  {
    "objectID": "parts/algorithms/usingexistingframeworks.html#portfolio-optimization",
    "href": "parts/algorithms/usingexistingframeworks.html#portfolio-optimization",
    "title": "1¬† D-Wave and its ecosystem",
    "section": "",
    "text": "D-Wave examples",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>D-Wave and its ecosystem</span>"
    ]
  },
  {
    "objectID": "parts/algorithms/usingexistingframeworks.html#max-cut",
    "href": "parts/algorithms/usingexistingframeworks.html#max-cut",
    "title": "1¬† D-Wave and its ecosystem",
    "section": "1.2 Max Cut",
    "text": "1.2 Max Cut\n\nimport networkx as nx\n\n# Create empty graph\nG = nx.Graph()\n\n# Add edges to the graph (also adds nodes)\nG.add_edges_from([(1,2),(1,3),(2,4),(3,4),(3,5),(4,5)])\n\nnx.draw(G)\n\n\n\n\n\n\n\n\n\nfrom collections import defaultdict\n\n# Initialize our Q matrix\nQ = defaultdict(int)\n\n# Update Q matrix for every edge in the graph\nfor i, j in G.edges:\n    Q[(i,i)]+= -1\n    Q[(j,j)]+= -1\n    Q[(i,j)]+= 2\n\n\nfrom dwave.system.samplers import DWaveSampler\nfrom dwave.system.composites import EmbeddingComposite\n\n# Set up QPU parameters\nchainstrength = 8\nnumruns = 10\n\n# Run the QUBO on the solver from your config file\nsampler = EmbeddingComposite(DWaveSampler(token=\"&lt;your-token\"))\nresponse = sampler.sample_qubo(Q,\n                               chain_strength=chainstrength,\n                               num_reads=numruns,\n                               label='Example - Maximum Cut')\n\n\nprint('-' * 60)\nprint('{:&gt;15s}{:&gt;15s}{:^15s}{:^15s}'.format('Set 0','Set 1','Energy','Cut Size'))\nprint('-' * 60)\nfor sample, E in response.data(fields=['sample','energy']):\n    S0 = [k for k,v in sample.items() if v == 0]\n    S1 = [k for k,v in sample.items() if v == 1]\n    print('{:&gt;15s}{:&gt;15s}{:^15s}{:^15s}'.format(str(S0),str(S1),str(E),str(int(-1*E))))\n\n------------------------------------------------------------\n          Set 0          Set 1    Energy        Cut Size    \n------------------------------------------------------------\n         [2, 3]      [1, 4, 5]     -5.0             5       \n         [1, 4]      [2, 3, 5]     -5.0             5       \n\n\nA pretty important piece of information here is that even though we started with \\(2^N\\) potential solutions, finally we only got 4 that yield the same energy value. Therefore, our quantum state is the superposition of all those optimal solutions, therefore the ground state.\n\nfrom matplotlib import pyplot as plt\n\nlut = response.first.sample\n\n# Interpret best result in terms of nodes and edges\nS0 = [node for node in G.nodes if not lut[node]]\nS1 = [node for node in G.nodes if lut[node]]\ncut_edges = [(u, v) for u, v in G.edges if lut[u]!=lut[v]]\nuncut_edges = [(u, v) for u, v in G.edges if lut[u]==lut[v]]\n\n# Display best result\npos = nx.spring_layout(G)\nnx.draw_networkx_nodes(G, pos, nodelist=S0, node_color='r')\nnx.draw_networkx_nodes(G, pos, nodelist=S1, node_color='c')\nnx.draw_networkx_edges(G, pos, edgelist=cut_edges, style='dashdot', alpha=0.5, width=3)\nnx.draw_networkx_edges(G, pos, edgelist=uncut_edges, style='solid', width=3)\nnx.draw_networkx_labels(G, pos);",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>D-Wave and its ecosystem</span>"
    ]
  },
  {
    "objectID": "parts/algorithms/otherconsiderations.html",
    "href": "parts/algorithms/otherconsiderations.html",
    "title": "Other considerations",
    "section": "",
    "text": "Non-binary variables\nmost of what we saw relates to binary optimization but there are two things we may need or want to work on as well:\nMany problems require continuous variables so in order to be solvable by quantum computers, or at least Quantum Annealers, we might need to transform that numerical precision into bins.\nSo, if we take the example of the portfolio optimization problem, assuming a percentage of the available money goes to different assets (not just binary decisions if we invest or not) we will need to reformulate it, so imaging that instead of\n\\[\n\\max \\sum_i r_ix_i - \\sum_i \\sum_j c_{ij}x_ix_j \\\\\n\\text{ s. t. } \\sum_i b_ix_i \\le B\n\\] where \\(x_i \\in \\{0, 1\\}\\), you need to define a variable \\(w_i \\in \\mathbb{R}\\). In order for that to work on a quantum computer, you will need to define your problem so that\n\\[\nw_i = \\sum_{p=0}^{K} \\frac{1}{2^p} x_{i,p}\n\\] so that \\(w_i\\) is the linear combination of weights \\(\\{\\frac{1}{2^0}, \\frac{1}{2^1}, \\frac{1}{2^2}, \\dots, \\frac{1}{2^p} \\}\\). We will need to define the precision we would like and that also affects the number of qubits required to solve our problem. If the binary case required one qubit per asset, now we need \\(p\\) qubits per asset, which might be challenging for large problems.",
    "crumbs": [
      "Optimization",
      "Other considerations"
    ]
  },
  {
    "objectID": "parts/algorithms/otherconsiderations.html#beyond-qubo",
    "href": "parts/algorithms/otherconsiderations.html#beyond-qubo",
    "title": "Other considerations",
    "section": "Beyond QUBO",
    "text": "Beyond QUBO\nThe limitation to quadratic terms might not be suitable for all soft of problems. We may want to extend our problem so that for example, covers a three-parti relationship\n\\[\n\\max \\sum_i a_ix_i - \\sum_i \\sum_j b_{ij}x_i, x_j + \\sum_i \\sum_j \\sum_k c_{ijk}x_ix_jx_k.\n\\]\nThat extra three-party relation adds a whole new level of complexity as it requires either a three party gate (like de Toffoli gate) in systems that do not go beyond two-qubit gates. So we will need to decompose this interaction into several new two body terms, exploding the resources required. This are defined as Higher-Order Unconstainer Binary Optimization (HUBO) problems.",
    "crumbs": [
      "Optimization",
      "Other considerations"
    ]
  },
  {
    "objectID": "parts/algorithms/trainingcircuits.html",
    "href": "parts/algorithms/trainingcircuits.html",
    "title": "Tuning circuits",
    "section": "",
    "text": "Finding the right set of parameters, like the right scheduling function in the case of annealers, can be a real challenge. In particular when there is little knowledge about the problem evolution itself.\nThe Adiabatic theorem states that the gap has to be small enough so that no transition happens between the ground state and any higher energy level state. But what does it mean small enough? How can we design the shape of the scheduling function so that acts in the right places during the evolution?\nThat takes us into a field that has gon wild: Training Quantum Circuits or using Parameterized Quantum Circuits (PQCs) and optimization techniques to find the right parameter setups. But first we need to generalize beyond what a specific machine can do, this will be our first challenge and the first benefit on going abstract with gate-based computers.",
    "crumbs": [
      "Tuning circuits"
    ]
  },
  {
    "objectID": "parts/algorithms/digitizedaqc.html",
    "href": "parts/algorithms/digitizedaqc.html",
    "title": "Introduction to digitization",
    "section": "",
    "text": "Trotter-Suzuki decomposition\nDigitized quantum computers work indeed in a different manner. Hamiltonians are less important from a program perspective. All we need is the set of instructions to be provided for them to run. Of course, these instructions cannot come out of the blue, so we still apply previous principles but with the flexibility of being able to tweak how the initial quantum state evolves. We are not limited by a given algorithm or device configuration, as there exist no default setup other than the initial state of the system (\\(|0\\rangle^n\\)). It is up to our design how it will work and how much juice we squeeze out of the machine.\nAs an example, we can still benefit from the AQC paradigm and implement a time dependent Hamiltonian that solves our problem like before. One critical aspect though is the fact that digitized quantum computers work in cycles and actions will need to be discretized (bounded in time) in order to implement our wanted state evolution. But it gives us more room to change some of the parameters machines like D-Wave‚Äôs ones have fixed for us. That way, we may more options to explore.\nApproximating a time-dependent Hamiltonian can be challenging as working with continuous values on a cycle driven machine (digital machine) requires us to discretize the time evolution into piecewise linear steps.\nIf we consider out unitary evolution for the Hamiltonian \\(U(T,t_0)\\) we could take same approach and approximate its results by extending its definition to discrete intervals.\n\\[\nU(T,t_0) = U(t_n, t_{n-1})U(t_{n-1}, t_{n-2}) \\dots U(t_1, t_0)\n\\]\nThe smaller the step the better we will approximate ideal continuous case. Take also into consideration that \\(\\delta t_i = t_{i+1} - t_i\\) do not have to be equally spaced for all \\(i\\)s even though it does help for generalization purposes.\nIn quantum mechanics there exists a concreta example of this effect called the Suzuki-Trotter expansion or Trotterization [2]. If the Hamiltonian has the shape \\(e^{A+B}\\) its limit can be defined\n\\[\ne^{A+B} = \\lim_{n\\rightarrow\\inf} \\left( e^{\\frac{A}{n}}e^{\\frac{B}{n}}\\right)^n\n\\]\nMeaning we can alternate the effect of a decomposed Hamiltonian (\\(H = H_A + H_B\\)) divided by a discrete number of steps so that the smaller the step size the better we will approximate the ideal case of a constant time evolution.",
    "crumbs": [
      "Tuning circuits",
      "Introduction to digitization"
    ]
  },
  {
    "objectID": "parts/algorithms/digitizedaqc.html#trotter-suzuki-decomposition",
    "href": "parts/algorithms/digitizedaqc.html#trotter-suzuki-decomposition",
    "title": "Introduction to digitization",
    "section": "",
    "text": "Piecewise linear function",
    "crumbs": [
      "Tuning circuits",
      "Introduction to digitization"
    ]
  },
  {
    "objectID": "parts/algorithms/digitizedaqc.html#a-practical-example",
    "href": "parts/algorithms/digitizedaqc.html#a-practical-example",
    "title": "Introduction to digitization",
    "section": "A practical example",
    "text": "A practical example\nLet‚Äôs go step by step. Our digitized annealing algorithm will be composed by three main blocks as we already explained before.\n\n\n\n\nDigitized Quantum Annealing\n\n\n\nThe mixing cycle is where the mixture between our initial Hamiltonian and problem Hamiltonian happen according to the scheduling function.\nSome support material\nOne critical aspect is to be able to initialize the system to the ground state of our initial Hamiltonian (\\(|+\\rangle\\) from our previous example). This can be easily achieved by Hadamard operations placed on each qubit.\nfrom qiskit import QuantumCircuit\n\ninit_state = QuantumCircuit(3)\nfor qi in init_state.qubits:\n    init_state.h(qi)\n\ninit_state.draw('mpl')\n\n\n\n\nInitial state (superposition of all possible basis states)\n\n\n\nThen, we need to produce the combination of our two hamiltonians. We will continue with previously seen examples. First the initial hamiltonian:\n\\[\nH_A = -\\sum_i^n \\sigma_{x_i}\n\\]\nIn this case its effect is conditioned by the scheduling function. Let‚Äôs rewrite the previous time dependent Hamiltonian such that a single scheduling function defines the mixture.\n\\[\nH(t) = (1-\\lambda (t))\\left( -\\sum_i^n \\sigma_{x_i} \\right) + \\lambda (t) H_p\n\\]\nThat way we could introduce a parameterized quantum circuit in which a progressively decaying parameter \\(\\theta\\) would match the effect of \\(1-\\lambda (t)\\).\nfrom qiskit import QuantumCircuit\nfrom qiskit.circuit import Parameter\n\nlt = Parameter(\"$(1-\\\\lambda (t))$\")\ndt = Parameter(\"dt\")\n\nHa = QuantumCircuit(3)\n\nfor qi in Ha.qubits:\n    Ha.rx(2*dt*lt, qi)\n\nHa.draw('mpl')\n\n\n\n\nInitial Hamiltonian (whose solution is the previous circuit block)\n\n\n\nAnd for our target Hamiltonian, if we select previous Ising type of Hamiltonian, we may only need to introduce the parameters affecting our specific instance of the problem.\nfrom qiskit import QuantumCircuit\nfrom qiskit.circuit import Parameter\n\nlt = Parameter(\"$\\\\lambda (t)$\")\ndt = Parameter(\"dt\")\n\nh = []\nfor i in range(3):\n    h.append(Parameter(f\"$h_{i}$\"))\n\nJ = {}\nfor i in range(3):\n    for j in range(i+1, 3):\n        J[(i,j)] = Parameter(f\"$J_{j}^{i}$\")\n\nHp = QuantumCircuit(3)\n\nfor i in range(3):\n    Hp.rz(2*dt*lt*h[i], i)\n\nfor i in range(3):\n    for j in range(i+1, 3):\n        Hp.rzz(2*dt*lt*J[(i, j)], i, j)\n\nHp.draw('mpl')\n\n\n\n\nProblem Hamiltonian (the one we want to obtain the ground state from)\n\n\n\nWith these three pieces we can build our digitized version for an adiabatic computation. Just by selecting the resolution of the \\(\\lambda (t)\\) function (often called the number of trotter steps) we can build the whole end to end circuit.\nfrom qiskit import QuantumCircuit\n\nqc = QuantumCircuit(3)\n\n# Init state\nqc = qc.compose(init_state)\n\n# Trotter steps\nfor dtval in [0.333, 0.6666, 0.999]:\n    qc = qc.compose(Ha.bind_parameters({dt: 0.333, ltA: (1-dtval)}))\n\n    params = {dt: 0.333, ltB: dtval}\n    for i in range(3):\n        params[h[i]] = 0.3\n        for j in range(i+1, 3):\n            params[J[(i,j)]] = 0.3\n    qc = qc.compose(Hp.bind_parameters(params))\n\nqc.draw('mpl', fold = 150)\n\n\n\n\nDigitized annealing circuit\n\n\n\nWe have built our digitized version of the Adiabatic Quantum Computing paradigm. Now, we can tweak it as we want, looking into optimal parameter sets. Change the scheduling function or any other option so that we can implement it on a gate-based quantum computer to check if this outperforms our colleagues from D-Wave. The goal still remains to obtain the ground-state or minimum energy state for the problem Hamiltonian after the whole protocol is performed.",
    "crumbs": [
      "Tuning circuits",
      "Introduction to digitization"
    ]
  },
  {
    "objectID": "parts/algorithms/examplesfordaqc.html",
    "href": "parts/algorithms/examplesfordaqc.html",
    "title": "Digitized AQC",
    "section": "",
    "text": "We will use our previous example on MaxCut just to take it simple.\n\nimport networkx as nx\n\n# Create empty graph\nG = nx.Graph()\n\n# Add edges to the graph (also adds nodes)\nG.add_edges_from([(1,2),(1,3),(2,4),(3,4),(3,5),(4,5)])\n\nnx.draw(G)\n\n\n\n\n\n\n\n\n\nfrom collections import defaultdict\n\n# Initialize our Q matrix\nQ = defaultdict(int)\n\n# Update Q matrix for every edge in the graph\nfor i, j in G.edges:\n    Q[(i,i)]+= -1\n    Q[(j,j)]+= -1\n    Q[(i,j)]+= 2\n\n\nfrom dimod import ExactSolver\n\n# Set up QPU parameters\nchainstrength = 8\nnumruns = 10\n\n# Run the QUBO on the solver from your config file\nsampler = ExactSolver()\nresponse = sampler.sample_qubo(Q)\n\n\nprint('-' * 60)\nprint('{:&gt;15s}{:&gt;15s}{:^15s}{:^15s}'.format('Set 0','Set 1','Energy','Cut Size'))\nprint('-' * 60)\nfor sample, E in response.data(fields=['sample','energy']):\n    S0 = [k for k,v in sample.items() if v == 0]\n    S1 = [k for k,v in sample.items() if v == 1]\n    print('{:&gt;15s}{:&gt;15s}{:^15s}{:^15s}'.format(str(S0),str(S1),str(E),str(int(-1*E))))\n\n------------------------------------------------------------\n          Set 0          Set 1    Energy        Cut Size    \n------------------------------------------------------------\n      [1, 4, 5]         [2, 3]     -5.0             5       \n      [2, 3, 5]         [1, 4]     -5.0             5       \n         [1, 4]      [2, 3, 5]     -5.0             5       \n         [2, 3]      [1, 4, 5]     -5.0             5       \n         [2, 5]      [1, 3, 4]     -4.0             4       \n      [2, 3, 4]         [1, 5]     -4.0             4       \n      [1, 2, 5]         [3, 4]     -4.0             4       \n         [1, 5]      [2, 3, 4]     -4.0             4       \n         [3, 4]      [1, 2, 5]     -4.0             4       \n      [1, 3, 4]         [2, 5]     -4.0             4       \n   [1, 2, 3, 5]            [4]     -3.0             3       \n      [1, 3, 5]         [2, 4]     -3.0             3       \n         [3, 5]      [1, 2, 4]     -3.0             3       \n   [1, 2, 4, 5]            [3]     -3.0             3       \n         [4, 5]      [1, 2, 3]     -3.0             3       \n      [2, 4, 5]         [1, 3]     -3.0             3       \n            [4]   [1, 2, 3, 5]     -3.0             3       \n         [2, 4]      [1, 3, 5]     -3.0             3       \n      [1, 2, 4]         [3, 5]     -3.0             3       \n         [1, 3]      [2, 4, 5]     -3.0             3       \n            [3]   [1, 2, 4, 5]     -3.0             3       \n      [1, 2, 3]         [4, 5]     -3.0             3       \n      [3, 4, 5]         [1, 2]     -2.0             2       \n   [1, 3, 4, 5]            [2]     -2.0             2       \n            [2]   [1, 3, 4, 5]     -2.0             2       \n            [5]   [1, 2, 3, 4]     -2.0             2       \n   [2, 3, 4, 5]            [1]     -2.0             2       \n            [1]   [2, 3, 4, 5]     -2.0             2       \n   [1, 2, 3, 4]            [5]     -2.0             2       \n         [1, 2]      [3, 4, 5]     -2.0             2       \n[1, 2, 3, 4, 5]             []      0.0             0       \n             [][1, 2, 3, 4, 5]      0.0             0       \n\n\n\nfrom qiskit.visualization import plot_histogram\n\nsolution = {\n    '01101': 0.25,\n    '10010': 0.25,\n    '10011': 0.25,\n    '01100': 0.25,\n}\n\nplot_histogram(solution)\n\n\n\n\n\n\n\n\nOk, we don‚Äôt always know the solution in advance but it is important for educational purposes. Let‚Äôs take the three pieces from our digitized AQC and built block by block, starting by the quantum state that is the ground state of our \\(H_{init}=-\\sum_n \\sigma_i^n\\)\n\nfrom qiskit import QuantumCircuit\n\nsize = G.number_of_nodes()\n\ninitial_state = QuantumCircuit(size)\n\n# Hadamard gate\nfor qb_i in initial_state.qubits:\n    initial_state.h(qb_i)\ninitial_state.draw('mpl')\n\n\n\n\n\n\n\n\nThis is simple and we can check the output of that actual circuit. Check out how the distribution resolution changes depending on the shots parameters.\n\nfrom qiskit_aer import AerSimulator\n\nqc = QuantumCircuit(size, size)\nqc = qc.compose(initial_state)\n\n# Measures added\nqc.measure(range(size), range(size))\n\n# execute the quantum circuit\nbackend = AerSimulator()\nresult = backend.run(qc, shots=10000).result()\ncounts  = result.get_counts(qc)\n\nplot_histogram(counts)\n\n\n\n\n\n\n\n\n\nimport qiskit.quantum_info as qi\n\npsi = qi.Statevector.from_instruction(initial_state)\npsi.draw('latex', prefix='|\\\\psi\\\\rangle = ')\n\n\\[|\\psi\\rangle = 0.1767766953 |00000\\rangle+0.1767766953 |00001\\rangle+0.1767766953 |00010\\rangle+0.1767766953 |00011\\rangle+0.1767766953 |00100\\rangle+0.1767766953 |00101\\rangle + \\ldots +0.1767766953 |11011\\rangle+0.1767766953 |11100\\rangle+0.1767766953 |11101\\rangle+0.1767766953 |11110\\rangle+0.1767766953 |11111\\rangle\\]\n\n\n\nfrom qiskit.visualization import plot_bloch_multivector\n\nplot_bloch_multivector(psi)\n\n\n\n\n\n\n\n\nOk so the starting point is just simply all potential solutions: \\(|+\\rangle^{\\otimes n}\\)\nNow we can add the blocks for \\(H_{init}\\) and \\(H_{problem}\\).\n\nfrom qiskit.circuit import Parameter\n\n# Create a parameter to instantiate\nbeta = Parameter(\"$\\\\beta$\")\n\nmixer = QuantumCircuit(size,size)\nfor i in range(size):\n    mixer.rx(2*beta*(-1), i)\n    \nmixer.draw('mpl')\n\n\n\n\n\n\n\n\nThis following code is a particular version of the MaxCut instance as in a fully informed Ising model we would need to add \\(Z\\) rotations depending on the weight of the node and \\(ZZ\\) rotations accoding to coupling strengh of each edge. But given that we have a sparse matrix, we can take advantage of that.\n\nimport numpy as np\n\nH_problem = np.zeros((size, size))\n\nfor k, v in Q.items():\n    if not isinstance(k, int):\n        H_problem[k[0]-1][k[1]-1] = v\n\nH_problem\n\narray([[-2.,  2.,  2.,  0.,  0.],\n       [ 0., -2.,  0.,  2.,  0.],\n       [ 0.,  0., -3.,  2.,  2.],\n       [ 0.,  0.,  0., -3.,  2.],\n       [ 0.,  0.,  0.,  0., -2.]])\n\n\n\nfrom qiskit.circuit import Parameter\n\n# Create a parameter to instantiate\ngamma = Parameter(\"$\\\\gamma$\")\n\nproblem = QuantumCircuit(size,size)\n\nfor node in range(size):\n    problem.rz(2*gamma*H_problem[node][node], node)\n\nfor node_i in range(size):\n    for node_j in range(node_i + 1, size):\n        if H_problem[node_i][node_j] &gt; 0:\n            problem.rzz(2*gamma*H_problem[node_i][node_j], node_i, node_j)\nproblem.draw('mpl')\n\n\n\n\n\n\n\n\nWe have the three main blocks for setting our digitized version of the problem. Now we need to define a composition for those blocks where:\n\n\\(\\beta\\) : (1-\\(\\lambda(t)\\))/\\(n\\)\n\\(\\gamma\\) : (\\(\\lambda(t)\\))/\\(n\\)\n\nWbeing \\(n\\) the number of trotter steps. We can do a first approach with a simple stepwise linear approach.\n\nmaxcut = QuantumCircuit(size,size)\nmaxcut.compose(initial_state, inplace=True)\n\ndt = 0.1\n\nfor lt in np.arange(0.1, 1.1, dt):\n    maxcut.compose(problem.assign_parameters({gamma: lt}), inplace=True)\n    maxcut.compose(mixer.assign_parameters({beta: (1-lt)}), inplace=True)\nmaxcut.measure(range(size), range(size))\nmaxcut.draw('mpl', fold=500)\n\n\n\n\n\n\n\n\n\n# execute the quantum circuit\nresult = backend.run(maxcut, shots=10000).result()\ncounts  = result.get_counts(maxcut)\n\nplot_histogram(counts)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nenergy = []\nsuccess_probability = [] \n\nfor bit_string in counts:\n    solution = [int(char) for char in bit_string]\n    energy.append(np.dot(np.transpose(solution), np.dot(H_problem, solution)))\n    success_probability.append(counts[bit_string]/10000)\n\nplt.bar(energy, height=success_probability, label=\"Digitized AQC\")\nplt.show()\n\n\n\n\n\n\n\n\nLet‚Äôs increase the schedule so that slower evolution is considered.\n\nmaxcut = QuantumCircuit(size,size)\nmaxcut.compose(initial_state, inplace=True)\n\ndt = 0.01\n\nfor lt in np.arange(0.1, 1.1, dt):\n    maxcut.compose(problem.assign_parameters({gamma: lt}), inplace=True)\n    maxcut.compose(mixer.assign_parameters({beta: (1-lt)}), inplace=True)\nmaxcut.measure(range(size), range(size))\nmaxcut.draw('mpl', fold=500)\n\n\n\n\n\n\n\n\n\nresult = backend.run(maxcut, shots=10000).result()\ncounts  = result.get_counts(maxcut)\n\nplot_histogram(counts)\n\n\n\n\n\n\n\n\n\nenergy = []\nsuccess_probability = [] \n\nfor bit_string in counts:\n    solution = [int(char) for char in bit_string]\n    energy.append(np.dot(np.transpose(solution), np.dot(H_problem, solution)))\n    success_probability.append(counts[bit_string]/10000)\n\nplt.bar(energy, height=success_probability, label=\"Digitized AQC\")\nplt.show()\n\n\n\n\n\n\n\n\nWe could select a much more efficient scheduling function, but we should look into the energy evolution, the gap‚Ä¶ let‚Äôs take a wild guess.\n\ndef scheduling_function(t, T):\n\n    return np.sin((np.pi/2)*np.sin((np.pi * t)/(2*T))**2)**2\n\n\nT = 1.0\ndt = 0.1\n\ntimeline = np.arange(0.1, T, dt)\n\nval = [scheduling_function(t, T) for t in timeline]\n\n\nplt.plot(timeline, val, label=\"Schedule\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nmaxcut = QuantumCircuit(size,size)\nmaxcut.compose(initial_state, inplace=True)\n\nT = 1.0\ndt = 0.1\n\ntimeline = np.arange(0.1, T, dt)\n\nfor t in np.arange(0.1, T, dt):\n    lambda_t = scheduling_function(t, T)\n    maxcut.compose(problem.assign_parameters({gamma: dt*lambda_t}), inplace=True)\n    maxcut.compose(mixer.assign_parameters({beta: dt*(1-lambda_t)}), inplace=True)\nmaxcut.measure(range(size), range(size))\nmaxcut.draw('mpl', fold=500)\n\n\n\n\n\n\n\n\n\nresult = backend.run(maxcut, shots=10000).result()\ncounts  = result.get_counts(maxcut)\n\nplot_histogram(counts)\n\n\n\n\n\n\n\n\n\nenergy = []\nsuccess_probability = [] \n\nfor bit_string in counts:\n    solution = [int(char) for char in bit_string]\n    energy.append(np.dot(np.transpose(solution), np.dot(H_problem, solution)))\n    success_probability.append(counts[bit_string]/10000)\n\nplt.bar(energy, height=success_probability, label=\"Digitized AQC\")\nplt.show()\n\n\n\n\n\n\n\n\nGetting close‚Ä¶\n\nmaxcut = QuantumCircuit(size,size)\nmaxcut.compose(initial_state, inplace=True)\n\nT = 5.0\ndt = 0.05\n\ntimeline = np.arange(0.1, T, dt)\n\nfor t in np.arange(0.1, T, dt):\n    lambda_t = scheduling_function(t, T)\n    maxcut.compose(problem.assign_parameters({gamma: dt*lambda_t}), inplace=True)\n    maxcut.compose(mixer.assign_parameters({beta: dt*(1-lambda_t)}), inplace=True)\nmaxcut.measure(range(size), range(size))\nmaxcut.draw('mpl', fold=500)\n\n\n\n\n\n\n\n\n\nresult = backend.run(maxcut, shots=10000).result()\ncounts  = result.get_counts(maxcut)\n\nplot_histogram(counts)\n\n\n\n\n\n\n\n\n\nenergy = []\nsuccess_probability = [] \n\nfor bit_string in counts:\n    solution = [int(char) for char in bit_string]\n    energy.append(np.dot(np.transpose(solution), np.dot(H_problem, solution)))\n    success_probability.append(counts[bit_string]/10000)\n\nplt.bar(energy, height=success_probability, label=\"Digitized AQC\")\n\nplt.show()\n\n\n\n\n\n\n\n\nLet‚Äôs play around a bit with the time parameters and see if we get a good concentration of -5 energy states. Ideally our sampling from the ground state should be below \\(2^5 = 32\\) samples, otherwise, brute forcing would be more efficient in this cases. But in absence of knowledge we should guess for a while.\nWe could rely on techniques known to be used to find parameters in a model. Yeah, machine learning techniques.",
    "crumbs": [
      "Tuning circuits",
      "Digitized AQC"
    ]
  },
  {
    "objectID": "parts/algorithms/variational.html",
    "href": "parts/algorithms/variational.html",
    "title": "The variational approach",
    "section": "",
    "text": "Computing gradients\nIt might have caught your attention that in our previous example there were some parameters set based on the scheduling function and the evolution of the Suzuki-Trotter approximation. But what if wou could harness the potential of classical optimization methods to find a better parametrization?\nIn fact, we could potentially remove some of those blocks that may not add too much, producing the approximation for the functional they encode in a more compact way. This also benefits when going to the hardware. The shallower the circuit, the less noise it gets accumulated.\nThe variational principle states a problem can be solved by using calculus of variations, which requires finding functions that optimize the values (parameters) that depend on those very same functions. This formal structure is extensively used when training machine learning models as those can be thought as parameterized functions whose target can be minimized by varying the weights (parameters) it is defined by.\nVariational Algorithm make use of these three basic principles:\nThese procedures therefore require the circuit to be evaluated (measured in some manner) and parameters changed ideally approximating a target value. This process is quite known for those coming from classical machine learning and therefore you can benefit from that ecosystem. Specifically, optimization techniques.\nThe easiest ones are gradient-free optimization techniques. Those explore the solution space on a way such that no derivatives of the actual function (our circuit in our case) is required.\nA different approach is to extend those gradient based optimizers given that quantum circuits do provide a way in which their derivative can be approximated.\nIn order to fit the ideal set of parameters minimizing a cost function, gradient based approaches are quite popular as being aware of the direction in which this cost function is minimized it may shorten the time required for the method to find it target Quantum circuits have a similar capacity, we can free up the parameters of a given circuit and optimize based on a cost function which in most cases can be represented by the expectation value over some observable. For this, we would need to understand how these derivatives can be computed‚Ä¶ at scale.\nfrom qiskit import QuantumCircuit\nfrom qiskit.circuit import Parameter\n\na = Parameter('a')\n\nqc = QuantumCircuit(1)\nqc.ry(a, 0)\nqc.draw()\n\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\nq: ‚î§ Ry(a) ‚îú\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nThis is a simple rotation over the Y axis of the bloch sphere. So we know that if we measure the observable X of a given state produced by this rotation we get:\n\\[\n\\langle Y(a) | X | Y(a) \\rangle = \\sin(\\pi a)\n\\]\nimport numpy as np\nfrom qiskit.quantum_info import Statevector\n \ninit = QuantumCircuit(1)\nstate = Statevector(init)\n\nval = 0.3\ncirc = qc.assign_parameters({a : np.pi*0.3})\neval = state.evolve(circ)\nprint(eval)\n\nStatevector([0.89100652+0.j, 0.4539905 +0.j],\n            dims=(2,))\nfrom qiskit.quantum_info import Pauli\n\n# Target hamiltonian\nop = Pauli('X')\n\nprint(f\"&lt;Ry|X|Ry&gt; : {eval.expectation_value(oper=op)}\")\nprint(f\"Sin function: {np.sin(np.pi * val)}\")\n\n&lt;Ry|X|Ry&gt; : 0.8090169943749473\nSin function: 0.8090169943749475\nThen we know we could calculate its derivative as \\(\\pi\\cos(\\pi a)\\). This may not be an option for when we go to hardware and we make the circuit much more complicated in terms of gates (in particular multiqubit gates). Enter numerical resolution of derivatives!\nWe can produce an approximation to our target by leveraging finite differences for numerical approximation, taking into consideration the limit:\n\\[\nf^{'}(a) = \\lim_{h \\rightarrow 0} \\frac{f(a + h) - f(a)}{h}\n\\]\nwhich essentially only requires two evaluations of our function.\nstate = Statevector(init)\n\nval = 0.3\nh = 0.001\n\n# Get the expectation value for f(x)\ncirc = qc.assign_parameters({a : np.pi*0.3})\neval = state.evolve(circ)\nexp_val = eval.expectation_value(oper=op)\n\n# Get the expectation value for f(x+h)\ncirc = qc.assign_parameters({a : np.pi*(0.3+h)})\neval = state.evolve(circ)\nexp_val_eps = eval.expectation_value(oper=op)\n\nprint('Finite difference:', (exp_val_eps - exp_val) / h)\nprint('Cosine formula:   ', np.pi * np.cos(np.pi * val))\n\nFinite difference: 1.8425864574260764\nCosine formula:    1.8465818304904567\nWith a varying number of observables and compositions, this is what automatic differentiation frameworks can provide by tracking values and benefitting from the composable nature of the numerical approximations. Sometimes we know the exact mathematical formula, but in the case of QC that‚Äôs usually not the case. One of the most basic methods to approximate it is called ‚Äúfinite differences‚Äù. Given a one variable function \\(f(x)\\) we can check what‚Äôs the value of the gradient by calculating its values upon small perturbations: \\(f(x+\\epsilon)\\) and \\(f(x‚àí\\epsilon)\\) where \\(\\epsilon\\) is just a very small number.\n\\[\n\\Delta_{\\theta}f(x;\\theta) = \\frac{1}{2}\\left[ f\\left(x;\\theta+\\frac{\\pi}{2}\\right) - f\\left(x;\\theta+\\frac{\\pi}{2}\\right)\\right]\n\\]\nFrameworks like Pennylane specialize in this field, providing direct access to gradients when our circuit is defined.\nimport pennylane as qml\n\ndev = qml.device(\"default.qubit\", wires=2)\n\n@qml.qnode(dev)\ndef circuit(weights):\n    qml.RX(weights[0], wires=0)\n    qml.RY(weights[1], wires=1)\n    qml.CNOT(wires=[0, 1])\n    qml.RX(weights[2], wires=1)\n    return qml.expval(qml.PauliZ(1))\nimport pennylane.numpy as np\n\nweights = np.array([0.1, 0.2, 0.3], requires_grad=True)\n\nqml.drawer.use_style(\"pennylane\")\nqml.draw_mpl(circuit)(weights);\ncircuit(weights)\n\ntensor(0.9316158, requires_grad=True)\nqml.gradients.param_shift(circuit)(weights)\n\ntensor([-0.09347337, -0.18884787, -0.28818254], requires_grad=True)\nOne of the problems with calculating gradients this way is that now we have to evaluate our cost function in the range of 3 times more for each iteration and number of variables (as we will need to fix all of them except one to compute the gradient at each variable value). More in detail explanation and relevant references can be found here.\nIn many cases we will see how non-gradient based methods are made available to avoid the extensive usage of quantum hardware.",
    "crumbs": [
      "Tuning circuits",
      "The variational approach"
    ]
  },
  {
    "objectID": "parts/algorithms/variational.html#expectation-value",
    "href": "parts/algorithms/variational.html#expectation-value",
    "title": "The variational approach",
    "section": "Expectation value",
    "text": "Expectation value\nThe guiding light when training circuit using the variational principle is to accurately define the objective and cost functions. What are we looking for? How can we measure the progress? In general, it is understood that the minimization of energy, looking for the quantum state that renders a minimal expectation value, can be tractable and a good measure of improvement. this, the expectation value described as\n\\[\n\\langle \\psi |H|\\psi\\rangle\n\\]\nbeing \\(H\\) the target hamiltonian or a hamiltonian characterizing the problem we would like to solve and \\(|\\psi\\rangle\\) the quantum state we would obtain out of the circuit, we would have a way to evaluate our progress. We could also formulate it as\n\\[\n\\langle H \\rangle_{\\psi} = \\sum_j \\lambda_j |\\langle \\psi|\\lambda_j\\rangle|^2.\n\\]\nIt represents the summation of all eigenvalues conditioned by their probability of outcome given state \\(|\\psi\\rangle\\). That characterizes the mathematical nature of this value and it is evident the minimum value is reached when the there is a perfect overlap between \\(|\\psi\\rangle\\) and the minimum value eigenstate (\\(H|\\lambda_0\\rangle = \\lambda_0|\\lambda_0\\rangle\\)).\nIn principle, if you can get an infinite number of steps and choose angles which exactly mimic the adiabatic path, you will get the right results. Therefore, the main limitation is defining a circuit and find the set of parameters that actually do the job. But, already mentioned in the literature:\n\n‚ÄúNo one knows if a quantum computer running a quantum algorithm will be able to outperform a classical computer on a combinatorial search problem.‚Äù\n\nVariational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA) are probably the two most known techniques in this regime so we will explore them to better understand what are their pros and cons. You will see how most of the foundation already established by the digitization of the annealing procedure will pay off while trying to understand those state-of-the-art techniques.",
    "crumbs": [
      "Tuning circuits",
      "The variational approach"
    ]
  },
  {
    "objectID": "parts/algorithms/vqe.html",
    "href": "parts/algorithms/vqe.html",
    "title": "Variational Quantum Eigensolver",
    "section": "",
    "text": "Variational Quantum Eigensolver (VQE) proposed some years back (Peruzzo et al. 2014) does a upper bound approximation to the minimum energy of a given Hamiltonian. The ground state of a Hamiltonian represents the system state rendering the minium energy level (let‚Äôs call it \\(E_0\\)). If we decompose the eigenvalues and eigenvectors to our target Hamiltonian we would obtain the whole energy spectrum and see which is actually the quantum state associate to the minimum energy value.\n\\[\nH|\\psi_{\\lambda}\\rangle = E_{\\lambda}|\\psi_{\\lambda}\\rangle\n\\]\nThis is useful when, for example, looking to the state that minimizes system energy in chemistry. But of course if our problem is mapped to a Hamiltonian and its minimum energy is the outcome of our cost function minimization, then the obtained quantum state will therefore render as the solution to our problem.\nThus, this way to approximate the minimum energy of the system could be quite useful is it requires less computation than the actual calculation of the eigenstates of our system.\nWe do know that for a set of parameters on a parameterized ansatz, the outcome will be a target state \\(|\\psi\\rangle\\) and that we can compute the energy level of a given state as \\(\\langle \\psi | H |\\psi\\rangle = E(\\psi)\\). If this state is driven by a parameterized quantum circuit then we could modify the parameters looking for different states (\\(|\\phi\\rangle\\)) hopefully in the direction so that \\(E(\\phi) \\lt E(\\psi)\\).\nBy variationally iterating over potential states we could approximate this state and get our result. Let‚Äôs do a simple example, let take for example following Hamiltonian\n\\[\nH = \\left[\n\\begin{array}{cc}\n3 & 1 \\\\\n1 & -1\n\\end{array}\n\\right]\n\\]\nwhich can be decomposed into a linear combination of Pauli operators \\(2Z + X + I\\). Or a linear decomposition of hamiltonians \\(H_1 = 2Z\\), \\(H_2 = X\\) and \\(H_3 = I\\), hence \\(H = H_1+H_2+H_3\\). We will get back to this.\nVQE only requires an ansatz expressive enough so that target state can be mapped but nothing related to our target Hamiltonian. Therefore, our ansatz could be as simple as a \\(R_y(\\theta)\\) rotation.\nOne thing that should be relevant is the idea of measurement and basis. For \\(H_2\\) we might neet to add additional rotation operations over our initial circuit so that we can properly measure it.\nLet‚Äôs start with \\(\\theta = 0\\). Our generated state would be of the shape:\n\\[\nR_y(0)|0\\rangle =\n\\left[\n\\begin{array}{cc}\n1 & 0 \\\\\n0 & 1\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n1 \\\\\n0\n\\end{array}\n\\right] = |0\\rangle\n\\]\nTherefore the expectation value for \\(H_1\\) equals:\n\\[\n\\langle 0 | H_1 |0\\rangle = \\left[\n\\begin{array}{cc}\n1 & 0\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{cc}\n2 & 0 \\\\\n0 & -2\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n1 \\\\\n0\n\\end{array}\n\\right] = 2\n\\]\n\\(H_3 = 1\\) but evaluating \\(H_2\\) may require a basis change as we would need to measure it corresponding to the \\(\\{|+\\rangle, |-\\rangle\\}\\) basis. Therefore, we would need to add a \\(R_y(-\\pi/2)\\) to our circuit.\n\\[\nR_y(-\\pi/2)|0\\rangle =\n\\left[\n\\begin{array}{cc}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n1 \\\\\n0\n\\end{array}\n\\right] = -|1\\rangle = |\\psi\\rangle\n\\]\nso the expectation value can be computed as\n\\[\n\\langle \\psi | H_2 | \\psi\\rangle = \\left[\n\\begin{array}{cc}\n0 & 1\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{cc}\n0 & 1 \\\\\n1 & 0\n\\end{array}\n\\right]\\left[\n\\begin{array}{c}\n0 \\\\\n-1\n\\end{array}\n\\right] = 0\n\\]\nTherefore the overall expectation value for \\(H\\) over \\(|0\\rangle\\) is 3. We could iterate over all potential values of \\(\\theta\\) until we find the actual state that gives us the lowest value but that would also consume quite some time. The important fact here is that, given the basis change we might need to evaluate our ansatz per parameter iteration more than once if the expectation value needs to be obtained at each iteration in order to know the direction in which those parameters should be changed.\nHybrid workload\nThe way to explore the options is by setting a cost function that will help us drive the value into the right direction. Here, expectation value is of great help as we do know it will provide an approximation to the ground state of the system (depending on the amount of time we let it iterate).\nThis is when traditional or classical optimization routines come handy. We could employ existing methods for classical optimization such as Stochastic Gradient Descent (SDG), Powel‚Äôs method or Nelder-Mead method looking for efficiently traversing the potential solution space for all values \\(\\theta\\) could take.\nAlso, given previous decomposition we could easily see how this calculations could be performed in parallel maximizing the usage of resources both from the classical and quantum side.\n\n\n\n\nVQE algorithm\n\n\n\nA key benefit for VQE is that it does not require any specific shape for our ansatz, so if a flexible enough option is selected we could benefit those who could better fit into our hardware connectivity topology to obtain the results.\n\n\n\n\nPeruzzo, Alberto, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J Love, Al√°n Aspuru-Guzik, and Jeremy L O‚Äôbrien. 2014. ‚ÄúA Variational Eigenvalue Solver on a Photonic Quantum Processor.‚Äù Nature Communications 5 (1): 4213.",
    "crumbs": [
      "Tuning circuits",
      "Variational Quantum Eigensolver"
    ]
  },
  {
    "objectID": "parts/algorithms/optimizingcircuits.html",
    "href": "parts/algorithms/optimizingcircuits.html",
    "title": "Optimizing circuits",
    "section": "",
    "text": "We will learn how to set up those parameters by training an ansatz and using classical optimization techniques. And we will continue with our previous example of the MaxCut problem.\n\nimport networkx as nx\nimport numpy as np\n\n# Create empty graph\nG = nx.Graph()\n\n# Add edges to the graph (also adds nodes)\nG.add_edges_from([(1,2),(1,3),(2,4),(3,4),(3,5),(4,5)])\n\nnx.draw(G)\n\n\n\n\n\n\n\n\n\nimport dimod\nfrom collections import defaultdict\n\n# Initialize our Q matrix\nQ = defaultdict(int)\n\n# Update Q matrix for every edge in the graph\nfor i, j in G.edges:\n    Q[(i,i)]+= -1\n    Q[(j,j)]+= -1\n    Q[(i,j)]+= 2\n\nh, j, offset = dimod.qubo_to_ising(Q)\nj\n\n{(1, 2): 0.5, (1, 3): 0.5, (2, 4): 0.5, (3, 4): 0.5, (3, 5): 0.5, (4, 5): 0.5}\n\n\nThe Hamiltonian comes in the shape we previously described that can also be represented as:\n\nfrom qiskit.quantum_info import SparsePauliOp\n\nH_op = SparsePauliOp.from_list(\n    [\n        (\"ZZIII\", 0.5),\n        (\"ZIZII\", 0.5),\n        (\"IZIZI\", 0.5),\n        (\"IIZIZ\", 0.5),\n        (\"IIIZZ\", 0.5),\n    ]\n)\n\nprint(f\"Number of qubits: {H_op.num_qubits}\")\n\nNumber of qubits: 5\n\n\n\nH_op.to_matrix()\n\narray([[2.5+0.j, 0. +0.j, 0. +0.j, ..., 0. +0.j, 0. +0.j, 0. +0.j],\n       [0. +0.j, 0.5+0.j, 0. +0.j, ..., 0. +0.j, 0. +0.j, 0. +0.j],\n       [0. +0.j, 0. +0.j, 0.5+0.j, ..., 0. +0.j, 0. +0.j, 0. +0.j],\n       ...,\n       [0. +0.j, 0. +0.j, 0. +0.j, ..., 0.5+0.j, 0. +0.j, 0. +0.j],\n       [0. +0.j, 0. +0.j, 0. +0.j, ..., 0. +0.j, 0.5+0.j, 0. +0.j],\n       [0. +0.j, 0. +0.j, 0. +0.j, ..., 0. +0.j, 0. +0.j, 2.5+0.j]],\n      shape=(32, 32))\n\n\n\nfrom numpy.linalg import eig\n\neigenvalues, eigenvectors = eig(H_op.to_matrix())\n\nmin_idx = np.argmin(eigenvalues)\nprint(f\"Reference value: {eigenvalues[min_idx] + offset:.5f}\")\n\nReference value: -4.50000+0.00000j\n\n\n\neigenvectors[min_idx]\n\narray([0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j,\n       0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n       0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n       0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j])\n\n\nSo we want the complete solution, the one that captures the degeneracy of our eigenvector, meaning there are several solutions for the original MaxCut problem. We would like to select an expressive enough ansatz and for this, we can rely on the implemented ones within Qiskit‚Äôs circuit library.\n\n# Use RealAmplitudes circuit to create all kind of states\nfrom qiskit.circuit.library import real_amplitudes\n\nansatz = real_amplitudes(num_qubits=H_op.num_qubits, reps=2)\nansatz.decompose().draw('mpl')\n\n\n\n\n\n\n\n\nLet‚Äôs do an initial guess and see what the outcome is.\n\nnp.random.seed(6)\n\n# Parameters are rotation angles, remember\ninitial_point = np.random.uniform(-np.pi, np.pi, ansatz.num_parameters)\n\n\nfrom qiskit_aer import StatevectorSimulator\n\nqc = ansatz.copy()\nfor idx, param in enumerate(ansatz.parameters):\n    qc = qc.assign_parameters({param: initial_point[idx]})\nqc = qc.decompose()\n\nbackend = StatevectorSimulator()\nresult = backend.run(qc).result()\npsi  = result.get_statevector(qc)\n\n\nfrom qiskit.visualization import plot_state_qsphere\n\nplot_state_qsphere(psi)\n\n\n\n\n\n\n\n\nCannot see anything there. Let‚Äôs check the expectation value we get for that state and our Hamiltonian.\n\nfrom qiskit_aer.primitives import Estimator\n\nestimator = Estimator()\nexpectation_value = estimator.run(qc, H_op).result().values\nprint('Exp. val.:', expectation_value + offset)\n\nExp. val.: [-3.31054688]\n\n\nWell, quite close. Probably we could do some smart guesses so that we can minimize this expectation value and therefore find a close enough state.\n\ndef compute_expectation(params):\n    \n    \"\"\"\n    Computes expectation value based on measurement results\n    \n    Args:\n        params: Parameters to be used\n        \n    Returns:\n        float: Expectation value\n    \"\"\"\n\n    qc = ansatz.copy()\n    for idx, param in enumerate(ansatz.parameters):\n        qc = qc.assign_parameters({param: params[idx]})\n    qc = qc.decompose()\n        \n    return estimator.run(qc, H_op).result().values + offset\n\n\ncompute_expectation(initial_point)\n\narray([-3.31640625])\n\n\nConsidering it the initial point, we could use our function to iterate over it and select gradient-free optimizer to try to find a better parameterization.\n\nfrom scipy.optimize import minimize\n\n# Minimize using COBYLA optimizer\nres = minimize(compute_expectation, initial_point, method='COBYLA')\nres\n\n message: Return from COBYLA because the trust region radius reaches its lower bound.\n success: True\n  status: 0\n     fun: -4.453125\n       x: [ 4.267e+00 -1.233e+00 ...  1.684e+00  2.044e+00]\n    nfev: 122\n   maxcv: 0.0\n\n\n\ncompute_expectation(res.x)\n\narray([-4.4140625])\n\n\nCool, so we could get some better parameters based on the minimization of the expectation value. Let‚Äôs check the actual final state.\n\nqc = ansatz.copy()\nfor idx, param in enumerate(ansatz.parameters):\n    qc = qc.assign_parameters({param: res.x[idx]})\nqc = qc.decompose()\n\nbackend = StatevectorSimulator()\nresult = backend.run(qc).result()\npsi  = result.get_statevector(qc)\n\nplot_state_qsphere(psi)\n\n\n\n\n\n\n\n\nNot ideal but close enough.\n\nfrom qiskit import QuantumCircuit\nfrom qiskit_aer import QasmSimulator\nfrom qiskit.visualization import plot_histogram\n\n# Solution\nsolution = {\n    '01101': 0.25,\n    '10010': 0.25,\n    '10011': 0.25,\n    '01100': 0.25,\n}\n\ncirc = QuantumCircuit(5, 5)\ncirc = circ.compose(qc)\ncirc.measure(range(circ.num_qubits), range(circ.num_qubits))\n\n# Solutions need to be reversed when comparing against Qiskit results\ncirc.reverse_bits()\n\nbackend = QasmSimulator()\nresult = backend.run(circ).result()\ncounts  = result.get_counts(circ)\n\nplot_histogram([counts, solution], legend=[\"VQE\", \"Ideal\"])\n\n\n\n\n\n\n\n\nNot bad in terms of expectation value but probably we could find better options for the ansatz so that it becomes easier to find the target state. Any guesses?\nWell lets then do it the ML (machine learning) way and try the available options:\n\nChange the repetition parameter: ..., reps=2)\nTry with other options in Qiskit https://docs.quantum.ibm.com/api/qiskit/circuit_library",
    "crumbs": [
      "Tuning circuits",
      "Optimizing circuits"
    ]
  },
  {
    "objectID": "parts/algorithms/qaoa.html",
    "href": "parts/algorithms/qaoa.html",
    "title": "Quantum Approximate Optimization Algorithm",
    "section": "",
    "text": "Quantum Approximate Optimization Algorithm (QAOA) inherits its structure from the concept of Adiabatic Quantum Computing simply digitizing the steps required to drive the initial state into the solution of the problem to be solved (Farhi, Goldstone, and Gutmann 2014). It also can be seen as a particular case of VQE where the ansatz is already defined by the quantum annealing scheme. If we carefully analyze the composition of the annealing algorithm we can identify three main blocks that essentially compose the QAOA circuit.\n\n\n\n\nDigitized Quantum Annealing\n\n\n\nThe initial and final Hamiltonian blocks (\\(H_i\\) and \\(H_f\\)) are the ones that get propagated according to the trotterization, the digitization of the mixing cycle. The challenge was defining the best possible scheduling function so that the evolution is absent of any transition of the ground state.\nConsidering some of the previous concepts, we could free-up the scheduling function and come up with a better selection of parameters that minimizes the length of this evolution. If we take a large \\(n\\) in our Suzuki-Trotter approximation we could move beyond the coherent threshold of a machine and setting static time-lapses for the whole evolution might not be the best approach to approximate the target evolution (which we do not fully know). One crucial point researchers focus on is generating the shallowest version of this evolution as the shorter it gets less error gets accumulated (see Noise and errors). We refer as the depth of the circuit to the maximal length it reaches considering every set of gates that can be executed within the same execution cycle as the unit depth.\nIf we could enter some placeholders and check how the circuit behaves for a different set of parameters we could maybe find a better solution than the canonical set of equally spaced steps that places the evolution at critical points of our scheduling curve.\n\n\n\nScheduling curve\n\n\nSo, by putting some parameters to the mixing blocks we get\n\\[\n|\\gamma, \\beta\\rangle = U(H_B,\\beta_p)U(H_C, \\gamma_p) \\dots U(H_B,\\beta_1)U(H_C, \\gamma_1)|s\\rangle\n\\]\nwhere \\(|s\\rangle\\) is our starting state and \\(U(H_m, \\theta_m) = e^{-i\\theta_mH_m}\\) is the unitary transformation parameterized for each step for a total of \\(p\\) steps.\nWe could setup a free-form circuit so that we could change the values for those rotation angles according to a different criteria than the one used before.\nfrom qiskit import QuantumCircuit\n\ninit_state = QuantumCircuit(3)\nfor qi in init_state.qubits:\n    init_state.h(qi)\n\n# Initialization\nlayers = 1\nqc = QuantumCircuit(3)\n\n# Init state\nqc = qc.compose(init_state)\n\n# Trotter steps\nfor layer_idx in range(layers):\n    # Init hamiltonian\n    qc = qc.compose(Hi(layer_idx))\n\n    # Final hamiltonian\n    qc = qc.compose(Hf(layer_idx))\n\nqc.draw('mpl', fold = 150)\n\n\n\n\nOne layer of Quantum Approximate Optimization Algorithm (QAOA)\n\n\n\nThis is the basis of the Quantum Approximate Optimization Algorithm (QAOA). We only require to select the number of steps (also called layers when) in order to produce our template circuit. Then it comes the time to select the values that should replace placeholder parameters \\(\\gamma\\) and \\(\\beta\\).\nThe selection of both Hamiltonians, as if we did not have a good initial Hamiltonian, our selection of parameters could make as fall into a higher energy level eigenstate (\\(|\\lambda_1\\rangle\\)) and by continuously applying our unitary associated to the target Hamiltonian we would get stuck in the same quantum state forever (\\(U|\\psi_{\\lambda_1}\\rangle = E_{\\lambda_1}|\\psi_{\\lambda_1}\\rangle\\)). That is how our initial Hamiltonian throughout the whole evolution allows us to scape those local minima looking for the actual ground truth.\n\n\n\n\nFarhi, Edward, Jeffrey Goldstone, and Sam Gutmann. 2014. ‚ÄúA Quantum Approximate Optimization Algorithm.‚Äù arXiv Preprint arXiv:1411.4028.",
    "crumbs": [
      "Tuning circuits",
      "Quantum Approximate Optimization Algorithm"
    ]
  },
  {
    "objectID": "parts/algorithms/qaoaexample.html",
    "href": "parts/algorithms/qaoaexample.html",
    "title": "QAOA in partice",
    "section": "",
    "text": "As we have seen before, QAOA take the best of both worlds (ideally) considering the ansatz out of the AQC but variationaly training it to obtain the best scheduling function out of the free parameters.\nContinuing with previous example‚Ä¶.\n\nimport networkx as nx\n\n# Create empty graph\nG = nx.Graph()\n\n# Add edges to the graph (also adds nodes)\nG.add_edges_from([(1,2),(1,3),(2,4),(3,4),(3,5),(4,5)])\n\nnx.draw(G)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom collections import defaultdict\n\n# Initialize our Q matrix\nQ = defaultdict(int)\n\n# Update Q matrix for every edge in the graph\nfor i, j in G.edges:\n    Q[(i,i)]+= -1\n    Q[(j,j)]+= -1\n    Q[(i,j)]+= 2\n\n# Give it also a matrix shape to ease readability\nsize = G.number_of_nodes()\nH_problem = np.zeros((size, size))\n\nfor k, v in Q.items():\n    if not isinstance(k, int):\n        H_problem[k[0]-1][k[1]-1] = v\n\nH_problem\n\narray([[-2.,  2.,  2.,  0.,  0.],\n       [ 0., -2.,  0.,  2.,  0.],\n       [ 0.,  0., -3.,  2.,  2.],\n       [ 0.,  0.,  0., -3.,  2.],\n       [ 0.,  0.,  0.,  0., -2.]])\n\n\nFor this simple example, brute-forcing the solution can be done to have a reference for it.\n\nimport numpy as np\n\ndef objective_value(x: np.ndarray, w: np.ndarray) -&gt; float:\n    \"\"\"Compute the value of a cut.\n    Args:\n        x: Binary string as numpy array.\n        w: Adjacency matrix.\n    Returns:\n        Value of the cut.\n    \"\"\"\n    return np.dot(np.dot(x.T, w), x)\n\ndef bitfield(n: int, L: int) -&gt; list[int]:\n    \"\"\" Get the binary representation\"\"\"\n    result = np.binary_repr(n, L)\n    return [int(digit) for digit in result]  # [2:] to chop off the \"0b\" part\n\n# use the brute-force way to generate the oracle\nL = G.number_of_nodes()\nmax = 2**L\nsol = np.inf\nfor i in range(max):\n    cur = bitfield(i, L)\n\n    cur_v = objective_value(np.array(cur), H_problem)\n    if cur_v &lt; sol:\n        sol = cur_v\n\nprint(f'Objective value computed by the brute-force method is {sol}')\n\nObjective value computed by the brute-force method is -5.0\n\n\nThe main challenge would be understanding if this one is the only solution in our example\n\nx = [0, 1, 1, 0, 0]\nprint(np.dot(np.dot(np.transpose(x), H_problem), x))\n\n-5.0\n\n\n\nx = [0, 1, 1, 0, 1]\nprint(np.dot(np.dot(np.transpose(x), H_problem), x))\n\n-5.0\n\n\nIn fact, for this example we know there are more than one optimal solutions‚Ä¶\n\nfrom qiskit.visualization import plot_histogram\n\nsolution = {\n    '01101': 0.25,\n    '10010': 0.25,\n    '10011': 0.25,\n    '01100': 0.25,\n}\n\nplot_histogram(solution)\n\n\n\n\n\n\n\n\nWe would like to fully understand our solution landscape. Quantum Computing may help us by mapping the solution to a more complex superposed state that reveals all potential solutions to our problem.\nWe will need to build a Hamiltonian that maps our problem.\n\nfrom qiskit.quantum_info import SparsePauliOp\n\nH_op = SparsePauliOp.from_list(\n    [\n        # h\n        (\"ZIIII\", -2),\n        (\"IZIII\", -2),\n        (\"IIZII\", -3),\n        (\"IIIZI\", -3),\n        (\"IIIIZ\", -2),\n        # j\n        (\"ZZIII\", 2),\n        (\"ZIZII\", 2),\n        (\"IZIZI\", 2),\n        (\"IIZIZ\", 2),\n        (\"IIIZZ\", 2),\n    ]\n)\n\nprint(f\"Number of qubits: {H_op.num_qubits}\")\n\nNumber of qubits: 5\n\n\nIn this case, we will start with the existing Qiskit functionality and do a drill-down to highlight what is done behind the scenes. Qiskit provides a ready to be used QAOA implementation that will only require the and optimizer and a way to extract information out of our quantum device (either Estimator or Sampler primitive).\n\nfrom qiskit_algorithms.minimum_eigensolvers import QAOA\nfrom qiskit_algorithms.optimizers import COBYLA\nfrom qiskit.primitives import Sampler\n\noptimizer = COBYLA()\nsampler = Sampler()\n\nqaoa = QAOA(sampler, optimizer, reps=2)\nresult = qaoa.compute_minimum_eigenvalue(H_op)\n\n/tmp/ipykernel_2814/2458342227.py:6: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n  sampler = Sampler()\n\n\n\nprint(result)\n\n{   'aux_operators_evaluated': None,\n    'best_measurement': {   'bitstring': '01001',\n                            'probability': np.float64(0.1814935021057395),\n                            'state': 9,\n                            'value': np.complex128(-10+0j)},\n    'cost_function_evals': np.int64(217),\n    'eigenstate': {0: np.float64(0.034487189731118), 1: np.float64(0.054067046798144), 2: np.float64(0.009474562791103), 3: np.float64(0.004253202351383), 4: np.float64(0.009474562791103), 5: np.float64(0.004253202351383), 6: np.float64(0.001764549253085), 7: np.float64(0.000391658667689), 8: np.float64(0.106769805392883), 9: np.float64(0.181429324261667), 10: np.float64(0.007725223317258), 11: np.float64(0.003813741874719), 12: np.float64(0.020821941121875), 13: np.float64(0.009776640615382), 14: np.float64(0.001128640306842), 15: np.float64(0.000435494106586), 16: np.float64(0.106769805392883), 17: np.float64(0.181429324261667), 18: np.float64(0.020821941121875), 19: np.float64(0.009776640615382), 20: np.float64(0.007725223317258), 21: np.float64(0.003813741874719), 22: np.float64(0.001128640306842), 23: np.float64(0.000435494106586), 24: np.float64(0.071449503222534), 25: np.float64(0.131698339917453), 26: np.float64(0.004247388742489), 27: np.float64(0.002923324102737), 28: np.float64(0.004247388742489), 29: np.float64(0.002923324102736), 30: np.float64(0.000355010817305), 31: np.float64(0.000188123622825)},\n    'eigenvalue': np.float64(-6.608541505502073),\n    'optimal_circuit': &lt;qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x7f63d1238200&gt;,\n    'optimal_parameters': {   ParameterVectorElement(Œ≤[0]): np.float64(-5.253521670148454),\n                              ParameterVectorElement(Œ≤[1]): np.float64(0.180061375972522),\n                              ParameterVectorElement(Œ≥[0]): np.float64(-2.407113109996151),\n                              ParameterVectorElement(Œ≥[1]): np.float64(-4.648727647165913)},\n    'optimal_point': array([-5.25352167,  0.18006138, -2.40711311, -4.64872765]),\n    'optimal_value': np.float64(-6.608541505502073),\n    'optimizer_evals': None,\n    'optimizer_result': &lt;qiskit_algorithms.optimizers.optimizer.OptimizerResult object at 0x7f63d11b7650&gt;,\n    'optimizer_time': 1.3257575035095215}\n\n\n\nfrom qiskit.visualization import plot_histogram\n\ncounts = {}\nfor key in result.eigenstate:\n    key_str = format(key, 'b').zfill(5)\n    counts[key_str] = result.eigenstate[key]\n\nplot_histogram(counts)\n\n\n\n\n\n\n\n\n\nfrom qiskit.quantum_info import Statevector\nfrom qiskit.result import QuasiDistribution\n\ndef sample_most_likely(state_vector: QuasiDistribution | Statevector) -&gt; np.ndarray:\n    \"\"\"Compute the most likely binary string from state vector.\n    Args:\n        state_vector: State vector or quasi-distribution.\n\n    Returns:\n        Binary string as an array of ints.\n    \"\"\"\n    if isinstance(state_vector, QuasiDistribution):\n        values = list(state_vector.values())\n    else:\n        values = state_vector\n    n = int(np.log2(len(values)))\n    k = np.argmax(np.abs(values))\n    x = bitfield(k, n)\n    x.reverse()\n    return np.asarray(x)\n\nx = sample_most_likely(result.eigenstate)\n\nprint(x)\nprint(f'Objective value computed by QAOA is {objective_value(x, H_problem)}')\n\n[1 0 0 1 0]\nObjective value computed by QAOA is -5.0\n\n\n\nfrom qiskit_algorithms.optimizers import SLSQP\n\noptimizer = SLSQP()\n\nqaoa = QAOA(sampler, optimizer, reps=4)\nresult = qaoa.compute_minimum_eigenvalue(H_op)\nprint(result)\n\n{   'aux_operators_evaluated': None,\n    'best_measurement': {   'bitstring': '01001',\n                            'probability': np.float64(0.3335484852497862),\n                            'state': 9,\n                            'value': np.complex128(-10+0j)},\n    'cost_function_evals': 512,\n    'eigenstate': {0: np.float64(0.000567976910891), 1: np.float64(0.010395471339937), 2: np.float64(0.001960100225017), 3: np.float64(0.002211881581672), 4: np.float64(0.001960100225017), 5: np.float64(0.002211881581672), 6: np.float64(0.003184753941229), 7: np.float64(0.000306623065639), 8: np.float64(0.104361231155337), 9: np.float64(0.331954102047278), 10: np.float64(0.003273224737679), 11: np.float64(0.001009301898733), 12: np.float64(0.00415962544192), 13: np.float64(0.001848449390641), 14: np.float64(0.003342989508428), 15: np.float64(0.000174098143206), 16: np.float64(0.104361231155337), 17: np.float64(0.331954102047278), 18: np.float64(0.00415962544192), 19: np.float64(0.001848449390641), 20: np.float64(0.003273224737679), 21: np.float64(0.001009301898733), 22: np.float64(0.003342989508428), 23: np.float64(0.000174098143206), 24: np.float64(0.005929621082761), 25: np.float64(0.06556042732176), 26: np.float64(0.001926901515645), 27: np.float64(0.000386459767714), 28: np.float64(0.001926901515645), 29: np.float64(0.000386459767714), 30: np.float64(0.000769191879235), 31: np.float64(6.9203632005e-05)},\n    'eigenvalue': np.float64(-8.44544120489131),\n    'optimal_circuit': &lt;qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x7f63cefc94f0&gt;,\n    'optimal_parameters': {   ParameterVectorElement(Œ≤[1]): np.float64(-4.796007346263453),\n                              ParameterVectorElement(Œ≥[0]): np.float64(17.14803815689715),\n                              ParameterVectorElement(Œ≤[3]): np.float64(3.510874613219901),\n                              ParameterVectorElement(Œ≤[2]): np.float64(-14.757927314120892),\n                              ParameterVectorElement(Œ≥[1]): np.float64(-12.480458014455676),\n                              ParameterVectorElement(Œ≤[0]): np.float64(-0.8091502287506132),\n                              ParameterVectorElement(Œ≥[2]): np.float64(1.5740114699074574),\n                              ParameterVectorElement(Œ≥[3]): np.float64(-0.15132280017011113)},\n    'optimal_point': array([ -0.80915023,  -4.79600735, -14.75792731,   3.51087461,\n        17.14803816, -12.48045801,   1.57401147,  -0.1513228 ]),\n    'optimal_value': np.float64(-8.44544120489131),\n    'optimizer_evals': None,\n    'optimizer_result': &lt;qiskit_algorithms.optimizers.optimizer.OptimizerResult object at 0x7f63ceeeee70&gt;,\n    'optimizer_time': 3.860093832015991}\n\n\n\ncounts = {}\nfor key in result.eigenstate:\n    key_str = format(key, 'b').zfill(5)\n    counts[key_str] = result.eigenstate[key]\n\nplot_histogram(counts)\n\n\n\n\n\n\n\n\n\nx = sample_most_likely(result.eigenstate)\n\nprint(x)\nprint(f'Objective value computed by QAOA is {objective_value(x, H_problem)}')\n\n[1 0 0 1 0]\nObjective value computed by QAOA is -5.0\n\n\n\noptimizer = SLSQP(\n    maxiter=100000\n)\n\nqaoa = QAOA(sampler, optimizer, reps=1)\nresult = qaoa.compute_minimum_eigenvalue(H_op)\nprint(result)\n\n{   'aux_operators_evaluated': None,\n    'best_measurement': {   'bitstring': '01001',\n                            'probability': np.float64(0.0698075523256496),\n                            'state': 9,\n                            'value': np.complex128(-10+0j)},\n    'cost_function_evals': 26,\n    'eigenstate': {0: np.float64(0.019292175353761), 1: np.float64(0.03794801227606), 2: np.float64(0.017486038101845), 3: np.float64(0.023866030830428), 4: np.float64(0.017486038101845), 5: np.float64(0.023866030830428), 6: np.float64(0.082312378660836), 7: np.float64(0.052074726718877), 8: np.float64(0.019249954831121), 9: np.float64(0.065298195573385), 10: np.float64(0.02964271134614), 11: np.float64(0.045117067837659), 12: np.float64(0.076393669933859), 13: np.float64(0.001372379713675), 14: np.float64(0.000185786025329), 15: np.float64(0.000180414581744), 16: np.float64(0.019249954831121), 17: np.float64(0.065298195573385), 18: np.float64(0.076393669933859), 19: np.float64(0.001372379713675), 20: np.float64(0.02964271134614), 21: np.float64(0.045117067837659), 22: np.float64(0.000185786025329), 23: np.float64(0.000180414581744), 24: np.float64(0.03278249051786), 25: np.float64(0.002407684974611), 26: np.float64(0.043803191234692), 27: np.float64(0.011297036913139), 28: np.float64(0.043803191234692), 29: np.float64(0.011297036913139), 30: np.float64(0.003861863238562), 31: np.float64(0.101535714413403)},\n    'eigenvalue': np.float64(-0.27045469760299),\n    'optimal_circuit': &lt;qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x7f63d1104680&gt;,\n    'optimal_parameters': {   ParameterVectorElement(Œ≤[0]): np.float64(5.086601140552871),\n                              ParameterVectorElement(Œ≥[0]): np.float64(-2.796093372068505)},\n    'optimal_point': array([ 5.08660114, -2.79609337]),\n    'optimal_value': np.float64(-0.27045469760299),\n    'optimizer_evals': None,\n    'optimizer_result': &lt;qiskit_algorithms.optimizers.optimizer.OptimizerResult object at 0x7f63cca03e00&gt;,\n    'optimizer_time': 0.08787417411804199}\n\n\n\ncounts = {}\nfor key in result.eigenstate:\n    key_str = format(key, 'b').zfill(5)\n    counts[key_str] = result.eigenstate[key]\n\nplot_histogram(counts)\n\n\n\n\n\n\n\n\n\nx = sample_most_likely(result.eigenstate)\n\nprint(x)\nprint(f'Objective value computed by QAOA is {objective_value(x, H_problem)}')\n\n[1 1 1 1 1]\nObjective value computed by QAOA is 0.0\n\n\nLet‚Äôs check what is going on behind it. We can check the ansatz that QAOA creates.\n\nqaoa.ansatz.draw('mpl', fold=150)\n\n\n\n\n\n\n\n\n\nqaoa.ansatz.decompose().draw('mpl', fold=150)\n\n\n\n\n\n\n\n\n\nqaoa.ansatz.decompose(reps=2).draw('mpl', fold=150)\n\n\n\n\n\n\n\n\nAs we can see the ansatz is build following the Adiabatic Quantum Computing scheme where a block of \\(X\\) rotations is applied, and alternated with the problem hamiltonian (\\(Z + ZZ\\) interactions) as many times as repetitions are performed.\n\nqaoa = QAOA(sampler, optimizer, reps=4)\nresult = qaoa.compute_minimum_eigenvalue(H_op)\nprint(result)\n\n{   'aux_operators_evaluated': None,\n    'best_measurement': {   'bitstring': '01001',\n                            'probability': np.float64(0.3300305947965866),\n                            'state': 9,\n                            'value': np.complex128(-10+0j)},\n    'cost_function_evals': 276,\n    'eigenstate': {0: np.float64(0.000326083070787), 1: np.float64(0.030874239739588), 2: np.float64(0.003596121268256), 3: np.float64(7.9842042882e-05), 4: np.float64(0.003596121268256), 5: np.float64(7.9842042882e-05), 6: np.float64(0.003143853130805), 7: np.float64(0.001069151640852), 8: np.float64(0.089224465509067), 9: np.float64(0.329987742024022), 10: np.float64(0.000690642170396), 11: np.float64(0.000563049296698), 12: np.float64(0.022174384047733), 13: np.float64(0.001638536501227), 14: np.float64(0.001017387094548), 15: np.float64(0.000827745974191), 16: np.float64(0.089224465509067), 17: np.float64(0.329987742024022), 18: np.float64(0.022174384047733), 19: np.float64(0.001638536501227), 20: np.float64(0.000690642170396), 21: np.float64(0.000563049296698), 22: np.float64(0.001017387094548), 23: np.float64(0.000827745974191), 24: np.float64(0.000994955774486), 25: np.float64(0.050016364594514), 26: np.float64(0.006451999456021), 27: np.float64(0.000328663309232), 28: np.float64(0.006451999456021), 29: np.float64(0.000328663309232), 30: np.float64(0.000244093807393), 31: np.float64(0.000170100853031)},\n    'eigenvalue': np.float64(-8.487502215227986),\n    'optimal_circuit': &lt;qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x7f63cef0cd10&gt;,\n    'optimal_parameters': {   ParameterVectorElement(Œ≤[1]): np.float64(2.575641377957819),\n                              ParameterVectorElement(Œ≤[0]): np.float64(-1.0376199112606443),\n                              ParameterVectorElement(Œ≤[2]): np.float64(-4.682755548711262),\n                              ParameterVectorElement(Œ≥[3]): np.float64(2.7129228630835955),\n                              ParameterVectorElement(Œ≥[0]): np.float64(-3.3069272714649256),\n                              ParameterVectorElement(Œ≥[2]): np.float64(-3.696398969575387),\n                              ParameterVectorElement(Œ≤[3]): np.float64(3.550167314322061),\n                              ParameterVectorElement(Œ≥[1]): np.float64(-4.692076718933664)},\n    'optimal_point': array([-1.03761991,  2.57564138, -4.68275555,  3.55016731, -3.30692727,\n       -4.69207672, -3.69639897,  2.71292286]),\n    'optimal_value': np.float64(-8.487502215227986),\n    'optimizer_evals': None,\n    'optimizer_result': &lt;qiskit_algorithms.optimizers.optimizer.OptimizerResult object at 0x7f63ccafa720&gt;,\n    'optimizer_time': 2.0383241176605225}\n\n\n\nqaoa.ansatz.draw('mpl', fold=150)\n\n\n\n\n\n\n\n\n\nqaoa.ansatz.decompose().decompose().draw('mpl', fold=150)\n\n\n\n\n\n\n\n\nObtained solution distribution really points out to the solutions that provide solutions to our problem, superposed in a state that is the one minimizing the outcome of the circuit with respect to the expectation value of the hamitlonian.\n\ncounts = {}\nfor key in result.eigenstate:\n    key_str = format(key, 'b').zfill(5)\n    counts[key_str] = result.eigenstate[key]\n\nplot_histogram(counts)\n\n\n\n\n\n\n\n\nTherefore, is we are able to encode our problem into an Ising Hamiltonian it looks like we could easily design a circuit to obtain the results from the potential solution landscape. We may need to set up the repetitions needed in order to map the optimal parameters to our variationaly trained scheduling function.\n\nqaoa.ansatz.depth()\n\n10\n\n\nSmall problems will render shallow circuits, but we need to be aware that introducing many repetitions of those blocks or layers, it will require more operations to be squeezed in, the lifespan of the cubits will need to be longer and the more operations we add, more noise it will enter into our final state.",
    "crumbs": [
      "Tuning circuits",
      "QAOA in partice"
    ]
  },
  {
    "objectID": "parts/algorithms/extras.html",
    "href": "parts/algorithms/extras.html",
    "title": "Final remarks",
    "section": "",
    "text": "VQE alternatives\nEven though both techniques(VQE and QAOA) are quite recent, most of the community adopted them due to the limitations existing hardware had. These techniques, thanks to the variational approach, allowed for shallower circuits to be implemented and improve upon full length algorithms at the cost of running several iterations until the optimization routine finishes.\nThe adoption of these techniques lead to new problems as well. Similar to Neural Networks, challenges when scaling the training of those techniques at larger sizes came in the shape of barren-plateus (McClean et al. 2018), regions in the solution space where gradient based approaches cannot be provided with a direction in which the problem could be optimized. This also renders on a high-locality in certain instances that may make our variational training fall into local minima without the ability to get out of those.\nThat is why, if possible, global optimization techniques are recommended to be used so that certain global exploratory activity is also present while local optimization is done over the solution surface.\nHardware efficient ansatz is one of those device-aware techniques that tries to minimize the overhead when going from the theoretical representation to the practical implementation (Leone et al. 2024). By already considering the device topology, one can design an ansatz that is suited to the device more than the problem itself.\nAdapt-VQE (Grimsley et al. 2019) is modification of the VQE algorithm making use of a pool of operators. It evaluates their contribution on finding the lowest energy state (ground state) by measuring the gradients (or contribution). The trick here is that the ansatz keeps changing from iteration to iteration looking for a specific set of operators that benefits the convergence towards the ground state. A simple example on how to use it in Qiskit can be found https://qiskit.org/ecosystem/nature/howtos/adapt_vqe.html\nTetris-VQE (Anastasiou et al. 2024) defines an even more complex setup built on top of previous method. By growing the ansatz evaluating the gradient but also accounting for the free space available in the remaining qubits, Tetris-VQE generated ansatz tend to be much more compact than Adapt-VQE ones.",
    "crumbs": [
      "Tuning circuits",
      "Final remarks"
    ]
  },
  {
    "objectID": "parts/algorithms/extras.html#vqe-alternatives",
    "href": "parts/algorithms/extras.html#vqe-alternatives",
    "title": "Final remarks",
    "section": "",
    "text": "Adapt-VQE",
    "crumbs": [
      "Tuning circuits",
      "Final remarks"
    ]
  },
  {
    "objectID": "parts/algorithms/extras.html#qaoa",
    "href": "parts/algorithms/extras.html#qaoa",
    "title": "Final remarks",
    "section": "QAOA",
    "text": "QAOA\nLike VQE, QAOA has been put to a test and several improvements have been propose to tackle detected inefficiencies or simply try to boost its performance.\nWarm-Started QAOA (Egger, Mareƒçek, and Woerner 2021) proposes classically solving the relaxed version fo the problem so that it is used to warm-start the problem (replacing our initial state towards the one of the relaxed solution). An interesting guide can be found here https://qiskit.org/ecosystem/optimization/tutorials/10_warm_start_qaoa.html.\nBy following a Reinforcement learning approach one can try to find the optimal policy by which ideal parameters can be proposed a priori avoiding any gradient based approach that may lead to localized solutions (Wauters et al. 2020). On the bad side of this approach, reinforcement learning requires quite some computing time to try-fail-learn the ideal parameters to be used.\n\n\n\n\nRL-QAOA",
    "crumbs": [
      "Tuning circuits",
      "Final remarks"
    ]
  },
  {
    "objectID": "parts/algorithms/extras.html#tackling-large-problems",
    "href": "parts/algorithms/extras.html#tackling-large-problems",
    "title": "Final remarks",
    "section": "Tackling large problems",
    "text": "Tackling large problems\nSo far, we were able to tackle small-ish problems, we could tackle larger problems of course but there is certain limitations due to the number of qubits available in the devices (in the range of 127-156 for IBM, for example).\nThe main challenge will come from the hardware implementation and availability of devices capable of mapping the problem to their actual chip architecture. In most cases we have talked about interacting terms between all qubits in our system but in fact, depending on the device we use to solve our problem, we might find not all qubits are connected between them. This is the common case for most of the superconducting chips. That forces us to find ways around those limitations when pairing our problem to specific hardware.\nBut we could also find ways to decompose the problem, make it smaller in some sense and enable the usage of smaller devices to tackle large problems. then, could those problems also be solved by classical solvers? Given the size is smaller they could also benefit from that. Some interesting results have already shown the competence of these algorithms in comparison with state-of-the-art classical solvers (Ponce et al. 2025).\n\n\n\n\nDecomposed QAOA (https://arxiv.org/abs/2306.00494)\n\n\n\nIn general, the systems we often work with have a hard limitation on how big our problems can be. Some options have appeared but classical solvers can also handle pretty well small size problems so we may find there is no huge gain in many cases. Nevertheless, we will explore what can be done.\n\n\n\n\nAnastasiou, Panagiotis G, Yanzhu Chen, Nicholas J Mayhall, Edwin Barnes, and Sophia E Economou. 2024. ‚ÄúTETRIS-ADAPT-VQE: An Adaptive Algorithm That Yields Shallower, Denser Circuit Ans√§tze.‚Äù Physical Review Research 6 (1): 013254.\n\n\nEgger, Daniel J, Jakub Mareƒçek, and Stefan Woerner. 2021. ‚ÄúWarm-Starting Quantum Optimization.‚Äù Quantum 5: 479.\n\n\nGrimsley, Harper R, Sophia E Economou, Edwin Barnes, and Nicholas J Mayhall. 2019. ‚ÄúAn Adaptive Variational Algorithm for Exact Molecular Simulations on a Quantum Computer.‚Äù Nature Communications 10 (1): 3007.\n\n\nLeone, Lorenzo, Salvatore FE Oliviero, Lukasz Cincio, and Marco Cerezo. 2024. ‚ÄúOn the Practical Usefulness of the Hardware Efficient Ansatz.‚Äù Quantum 8: 1395.\n\n\nMcClean, Jarrod R, Sergio Boixo, Vadim N Smelyanskiy, Ryan Babbush, and Hartmut Neven. 2018. ‚ÄúBarren Plateaus in Quantum Neural Network Training Landscapes.‚Äù Nature Communications 9 (1): 4812.\n\n\nPonce, Moises, Rebekah Herrman, Phillip C Lotshaw, Sarah Powers, George Siopsis, Travis Humble, and James Ostrowski. 2025. ‚ÄúGraph Decomposition Techniques for Solving Combinatorial Optimization Problems with Variational Quantum Algorithms.‚Äù Quantum Information Processing 24 (2): 60.\n\n\nWauters, Matteo M, Emanuele Panizon, Glen B Mbeng, and Giuseppe E Santoro. 2020. ‚ÄúReinforcement-Learning-Assisted Quantum Optimization.‚Äù Physical Review Research 2 (3): 033446.",
    "crumbs": [
      "Tuning circuits",
      "Final remarks"
    ]
  },
  {
    "objectID": "parts/qml/index.html",
    "href": "parts/qml/index.html",
    "title": "Quantum Machine Learning",
    "section": "",
    "text": "Well, there is some hype around this topic. Hopefully, how we have arrived to propose Quantum Machine Learning can be easily understood having gone trough previous sections. If we do have the ability to train quantum circuits, why not use them as predictors to classification tasks or as regression model to provide us with better estimators?\nIf you are not entirely familiar with the concept, classical Machine Learning methods are composed of tunable functions and some data characterizing a phenomena. Canonical examples of this are, customer data and churn labels, weather conditions and likelihood of a traffic accident, etc. These are complex systems to characterize or model, it is hard to get a formulation describing the dynamics of the phenomena.\nBut we could learn some basic patterns linking variables to targets using an expressive enough function. When we would like to predict a condition or a class for a set of evidences (data), we call those classification tasks. When a percentage or a numeric value needs to be predicted, these are often called regression tasks. These two belong to the category of supervised machine learning. When no label or target variable is defined, quite often data segmentation or clustering is performed trying to better understand the common factors between available data points, this is called unsupervised machine learning. There you go, one paragraph and you are already an AI expert üòä\nThis is the minimum basis we will need to better understand the next bit: Quantum Machine learning (QML). Mixing Quantum Computing and Machine Learning gives birth to this field, even though there is no strict meaning on how this mixture is performed. We found three common recipes, though:\n\nMachine Learning models for better Quantum Computing\nQuantum Computers aiding Machine Learning tasks\nQuantum Computers acting as Machine Learning models\n\n\n\n\n\nData and computing quadrant\n\n\n\nPutting some focus in the last two, there are classical machine steps where data needs to be filtered or selected where QC techniques may apply (M√ºcke et al. 2023). Using Quantum Computers to find the right set of parameters on classical models (Date, Arthur, and Pusey-Nazzaro 2021). Or adaptations of Variational Quantum Algorithms (VQA) where the functional is approximated by a Parameterized Quantum Circuit (PQC) or ansatz named after the analogous classical field of Neural Networks (Schuld, Sinayskiy, and Petruccione 2014). There is plenty of hybridations we can discuss but we would like to focus on those cases where meaningful and practical approaches can be implemented‚Ä¶ for the moment.\nWe will learn some basic notions on the main changes we will require to adopt these techniques, but a great chunk comes from the good practices already set by classical machine learning so we might skip the explanations already covered about those.\n\n\n\n\n\n\nVideo recommendation\n\n\n\n\n\nIf interested, I find this talk on the topic by Nathan Wiebe really interesting, check it out.\n\n\n\n\n\n\n\n\nDate, Prasanna, Davis Arthur, and Lauren Pusey-Nazzaro. 2021. ‚ÄúQUBO Formulations for Training Machine Learning Models.‚Äù Scientific Reports 11 (1): 10029.\n\n\nM√ºcke, Sascha, Raoul Heese, Sabine M√ºller, Moritz Wolter, and Nico Piatkowski. 2023. ‚ÄúFeature Selection on Quantum Computers.‚Äù Quantum Machine Intelligence 5 (1): 11.\n\n\nSchuld, Maria, Ilya Sinayskiy, and Francesco Petruccione. 2014. ‚ÄúThe Quest for a Quantum Neural Network.‚Äù Quantum Information Processing 13: 2567‚Äì86.",
    "crumbs": [
      "Quantum Machine Learning"
    ]
  },
  {
    "objectID": "parts/qml/embedding.html",
    "href": "parts/qml/embedding.html",
    "title": "Data embedding",
    "section": "",
    "text": "A key aspect in both classical and quantum machine learning is that these are purely evidence based approaches. We need to have data in order to reverse engineer the dynamics of a given system or relationship between collected evidences and target values of the task.\nMost companies harvest a huge amount of data but this data is classical for now. Our quantum devices can only work with quantum states as we saw previously. We also know that when measuring any outcome of a quantum device, a mapping is done from quantum to classical regime where plenty of information is lost\n\\[\n|0\\rangle \\rightarrow 0 \\quad \\& \\quad |1\\rangle \\rightarrow 1,\n\\]\nFor the machine to be aware of our classical data we should find the opposite action, we would need to introduce our data into the quantum device creating a state that is meaningful for our task. Most likely it will also require a more complex representation than simple bits, as we will be using different data representations in our classical datasets (boolean, categorical and continuous variables among others).\nA special type of circuits exists meant for this particular step. This action is performed by feature maps, and the name comes from the mapping action between the original data space \\(\\in \\mathbb{R}^n\\) to a new feature space of different dimension and complexity (\\(\\in \\mathbb{C}^m\\)).\nMany techniques in the classical domain use this technique, mapping into a higher-dimensional space so that data becomes easy to separable. This is mostly performed by the definition of kernels in techniques like Support Vector Machines.\n\n\n\n\nData in high space (https://en.wikipedia.org/wiki/Kernel_method)\n\n\n\nAnd in some other cases, an intentional mapping to lower dimensional space is done to remove information redundancy and synthesize the information inside the data. Something similar to bottleneck layers on neural networks trying to define a lower dimensional latent space.\n\n\n\n\nLatent mapping using Encoder-Decoder architectures\n\n\n\nOften, these techniques require a good understanding from the classical side in order to create the best mapping possible to the latent space. It should be able to accurately synthesize the information removing all redundant information from the original data space.\nSo, by definition, our mapping into the quantum regime belongs to the first family as it is in generally assumed the higher dimensional nature of quantum formalism. But each type of mapping may benefit different objectives as we previously stated.",
    "crumbs": [
      "Quantum Machine Learning",
      "Data embedding"
    ]
  },
  {
    "objectID": "parts/qml/featuremaps.html",
    "href": "parts/qml/featuremaps.html",
    "title": "Feature maps",
    "section": "",
    "text": "Basis encoding\nFeature maps are the way in which classical data gets transformed into quantum states. A good point to start is to use existing templates from libraries like Pennylane.\nBasis encoding might be one of the most simplistic ways to map classical data in quantum states. It requires our data to be composed by integers and maps its binary representation to the basis state that maps the same binary string: \\(n = 4, \\quad b_n = 1001, \\quad |b_n\\rangle = |1001\\rangle\\).\n\\[\n|X\\rangle = \\frac{1}{\\sqrt{M}}\\sum_{m=1}^M |x^m \\rangle\n\\]\nIt is not often used as it limits the data that can be embedded and requires quite some resources from the quantum side which are not fully utilized.\nimport pennylane as qml\nfrom qiskit.visualization import array_to_latex\n\ndev = qml.device('default.qubit', wires=3)\n\n@qml.qnode(dev)\ndef feature_map(feature_vector):\n    qml.BasisEmbedding(features=feature_vector, wires=range(3))\n    return qml.state()\n\nX = [1,1,1]\n\narray_to_latex(array=feature_map(X).T, prefix='| \\\\psi \\\\rangle = ', max_size=(10,10))\n\n\\[\n| \\psi \\rangle =\n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 1  \\\\\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Quantum Machine Learning",
      "Feature maps"
    ]
  },
  {
    "objectID": "parts/qml/featuremaps.html#angle-encoding",
    "href": "parts/qml/featuremaps.html#angle-encoding",
    "title": "Feature maps",
    "section": "Angle encoding",
    "text": "Angle encoding\nAngle encoding takes benefit from the ability to embed rotation angles into our gates so that we can introduce our numeric data as the rotation angle of a given qubit. Example, \\(x = [x_1, x_2,...,x_n]\\) would be mapped to\n\\[\n(R_x(x_1) \\otimes R_x(x_2) \\otimes \\dots \\otimes R_x(x_n))|0\\rangle^{\\otimes n}\n\\]\ndata must be in the regime where those rotations take effect (\\([0, 2\\pi]\\)) and may be more suitable if a cyclic nature is already present. Normalization of data may be relevant preprocessing step in this case.\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\ndev = qml.device('default.qubit', wires=3)\n\n@qml.qnode(dev)\ndef feature_map(feature_vector):\n    qml.AngleEmbedding(features=feature_vector, wires=range(3), rotation='Z')\n    return qml.state()\n\nX = [2*np.pi,0,np.pi]\n\nprint(qml.draw(feature_map, level=\"device\")(X))\n\n0: ‚îÄ‚îÄRZ(6.28)‚îÄ‚î§ ‚ï≠State\n1: ‚îÄ‚îÄRZ(0.00)‚îÄ‚î§ ‚îúState\n2: ‚îÄ‚îÄRZ(3.14)‚îÄ‚î§ ‚ï∞State",
    "crumbs": [
      "Quantum Machine Learning",
      "Feature maps"
    ]
  },
  {
    "objectID": "parts/qml/featuremaps.html#amplitude-encoding",
    "href": "parts/qml/featuremaps.html#amplitude-encoding",
    "title": "Feature maps",
    "section": "Amplitude encoding",
    "text": "Amplitude encoding\nAmplitude encoding takes the probability amplitude of elements on an array and matches those to the position the where placed at. Assuming our data is composed by \\(n\\) values (\\(X = [x_0, x_1,...,x_n]\\) ) its representation would take the shape\n\\[\n|\\phi\\rangle = \\frac{1}{\\sqrt{\\sum_k x_k^2}} \\sum_{k=0}^{2 -1} x_k|k\\rangle\n\\]\n\nimport pennylane as qml\n\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev)\ndef feature_map(f):\n    qml.AmplitudeEmbedding(features=f, wires=range(2))\n    return qml.state()\n\nfeature_map(f=[1/2, 1/2, 1/2, 1/2])\n\narray([0.5+0.j, 0.5+0.j, 0.5+0.j, 0.5+0.j])",
    "crumbs": [
      "Quantum Machine Learning",
      "Feature maps"
    ]
  },
  {
    "objectID": "parts/qml/featuremaps.html#iqp-circuits",
    "href": "parts/qml/featuremaps.html#iqp-circuits",
    "title": "Feature maps",
    "section": "IQP circuits",
    "text": "IQP circuits\nBased on some works in the field (Havl√≠ƒçek et al. 2019), IQP circuit family was proposed as a potential good transformation given its hardness when it comes to classical simulation. If there is something quantum that adds to the mixture, this circuits mush be a good way to have those intrinsically added.\n\nZZ feature map\nThis is a particular version of the Pauli feature map that we will be able to find in Qiskit. It performs the same rotations we saw in the angle encoding but with an extra \\(ZZ\\) interaction between data points to capture the correlating observations to further boost their representation. We can also perform several repetitions of the mapping providing a more complex iteration.\nGiven that it uses \\(Z\\) rotations it is common to see Hadamard gates preceding those rotations so that by moving our \\(|0\\rangle\\) states to \\(|+\\rangle\\) will make sure those rotations have some meaningful effect.\n\nfrom qiskit.circuit.library import ZZFeatureMap\n\nfeature_map = ZZFeatureMap(3)\nfeature_map.draw('mpl')\n\n\n\n\n\n\n\n\n\nfeature_map.decompose().draw('mpl', fold=-1)\n\n\n\n\n\n\n\n\nThe general case allows to define a complete specification of Pauli rotations and entanglement layout so that one can customize the feature map to its specifications.\n\nfrom qiskit.circuit.library import PauliFeatureMap\n\nfeature_map = PauliFeatureMap(3, paulis=[\"X\", \"YZ\"], entanglement='linear')\nfeature_map.decompose().draw('mpl', fold=-1)\n\n\n\n\n\n\n\n\nWhy is this mapping good? Or bad? How many repetitions need to be used? Well, indeed, some of the approaches have no particular rational behind and are much more experimental. We can always look into the literature relevant papers exploring the qualities of each feature map trying to find the one that could be beneficial for our project (Sim, Johnson, and Aspuru-Guzik 2019).\n\n\n\n\nHavl√≠ƒçek, Vojtƒõch, Antonio D. C√≥rcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. 2019. ‚ÄúSupervised Learning with Quantum-Enhanced Feature Spaces.‚Äù Nature 567 (7747): 209‚Äì12. https://doi.org/10.1038/s41586-019-0980-2.\n\n\nSim, Sukin, Peter D Johnson, and Al√°n Aspuru-Guzik. 2019. ‚ÄúExpressibility and Entangling Capability of Parameterized Quantum Circuits for Hybrid Quantum-Classical Algorithms.‚Äù Advanced Quantum Technologies 2 (12): 1900070.",
    "crumbs": [
      "Quantum Machine Learning",
      "Feature maps"
    ]
  },
  {
    "objectID": "parts/qml/kernels.html",
    "href": "parts/qml/kernels.html",
    "title": "Kernels",
    "section": "",
    "text": "Quantum Kernel\nBefore going into details of what quantum kernels might be let‚Äôs discuss the simplest classifier one could build. If we re provided with one number we could build a classifier saying if its negative or positive. Or if we go to a \\(\\mathcal{R}^2\\) example we would like to know if our samples belong to the pink or blue category.\nWe could try to draw the line that places our samples into the corresponding category \\(f(x,y) = w_x x + w_y y + b\\). Actually there could be more than one option for this problem to be solved.\nWe would like to find the hyperplane that better splits the two groups, meaning that the margin from the line to the data points is maximal. Something like\n\\[\n    \\min \\frac{1}{2}\\|w\\|^2 + C\\sum_j \\epsilon_j\n\\]\n\\[\n    \\text{s.t.}\\quad y_j(w.x_j + b) \\ge 1 - \\epsilon_j, \\quad \\epsilon_j \\ge 0\n\\]\nbeing \\(C\\) a hyperparameter of our choosing. The main problem about this setup is that it can only perform linear separations. We could perform the kernel trick which consist of mapping the data space into a higher dimensional space in order to compute the inner product of the samples without actual computation of the coordinates so that a linear separation would render a low-dimensional non-linear classifier.\nKernel functions are mappers of members of a non-empty set (\\(x \\in \\mathbb{X}\\)) into a real value\n\\[\nK(x_i, x_j) \\ge 0\n\\]\nbeing able to map those to a distance value (but not equivalent to distance measurement) between the two samples being evaluated.\nEssentially, given that there exists a way to map data points into a quantum feature space and the inner product operation can be performed in similar manner by undoing the actions of the first feature map, we could build a quantum kernel of the form\n\\[\nk(x_i, x_j) = \\langle\\psi(x_i)|\\psi(x_j)\\rangle\n\\]\nconsidering \\(\\psi(x) = \\Psi(x)|0\\rangle\\) where \\(\\Psi(x)\\) is a quantum feature map like the ones presented before. By taking its conjugate\n\\[\nk(x_i, x_j) = \\langle 0 |\\Psi^{\\dagger}(x)|\\Psi(x_j)|0\\rangle\n\\]\nwhich can be easily performed by flipping the original circuit backwards and adding it to the initial feature map.\nfrom qiskit.circuit.library import ZZFeatureMap\n\nfeature_map = ZZFeatureMap(2)\n\nkernel = feature_map.compose(feature_map.inverse())\nkernel.decompose().draw('mpl', fold = 150)\nBy computing the number of times we get \\(|0\\rangle^n\\) after this execution we can get the closeness of these to feature map prepared states. If orthogonal states are evaluated (ex. \\(\\langle 0 | 1\\rangle\\)) then we can assume their states to be as distant as possible within the Hilbert space.\nThis way we could easily compute the fidelity between states on a higher dimensional feature space by just running this exercise of each sample pair (\\(O(N^2)\\)) in what it is called a Fidelity Kernel (Hubregtsen et al. 2022).\nThen, adding a classical classification method that using that measure of distance can separate between both classes, we would obtain a hybrid quantum-classical method. In fact, by chaining a Support Vector Machine classifier we would have obtained the Quantum Support Vector Classifier (Rebentrost, Mohseni, and Lloyd 2014).",
    "crumbs": [
      "Quantum Machine Learning",
      "Kernels"
    ]
  },
  {
    "objectID": "parts/qml/kernels.html#quantum-kernel",
    "href": "parts/qml/kernels.html#quantum-kernel",
    "title": "Kernels",
    "section": "",
    "text": "from qiskit_machine_learning.kernels import FidelityQuantumKernel\nfrom qiskit.algorithms.state_fidelities import ComputeUncompute\nfrom qiskit_machine_learning.algorithms import QSVC\nfrom qiskit.primitives import Sampler\n\n# How to compute the fidelity between the states \nfidelity = ComputeUncompute(sampler=Sampler())\n\n# Feature map and quantum Kernel\nfeature_map = ZZFeatureMap(2)\nnew_kernel = FidelityQuantumKernel(feature_map=feature_map, fidelity=fidelity)\n\n# QKernel + SVC\nqsvc = QSVC(quantum_kernel=new_kernel)\nqsvc.fit(features, labels)\nqsvc.score(features, labels)\n\n\n\n\nQuantum Support Vector Classifier (SQVC)\n\n\n\n\n\n\n\nHubregtsen, Thomas, David Wierichs, Elies Gil-Fuster, Peter-Jan H. S. Derks, Paul K. Faehrmann, and Johannes Jakob Meyer. 2022. ‚ÄúTraining Quantum Embedding Kernels on Near-Term Quantum Computers.‚Äù Physical Review A 106 (4). https://doi.org/10.1103/physreva.106.042431.\n\n\nRebentrost, Patrick, Masoud Mohseni, and Seth Lloyd. 2014. ‚ÄúQuantum Support Vector Machine for Big Data Classification.‚Äù Physical Review Letters 113 (13). https://doi.org/10.1103/physrevlett.113.130503.",
    "crumbs": [
      "Quantum Machine Learning",
      "Kernels"
    ]
  },
  {
    "objectID": "parts/qml/challenges.html",
    "href": "parts/qml/challenges.html",
    "title": "Summary",
    "section": "",
    "text": "Challenges remain from previous exercises. Circuit depth can be significantly increased also with these approaches, although QML allows for free form ansatz creation instead of problem specific ones like QAOA. That makes possible to adapt the ansatz so that it better fits specific hardware coupling maps like with hardware aware VQE.\nStill, defining the advantage of QML over ML can be difficult. We must consider the complexities of classical to quantum data conversion bottlenecks and maturity of quantum devices as well as the extra payload for over the network training and queuing when using quantum hardware. This is the reason why some vendors like AWS and IBM defined hybrid workloads and session-blocking to ease remote training. Others, like Xanadu and Google, released functional and easy to use efficient local QML focused frameworks (Pennylane and TensorFlow Quantum) but apart from the limitations one may face on going beyond 30 qubits seems like are more for educational purposes than any other thing.\nLastly, some interesting use cases mixing both regimes, quantum and classical have emerged which would reveal the true advantage of QC at this stage. Quantum-aided ML steps have shown interesting results as it is the case of Quantum Feature Selection (M√ºcke et al. 2023), which deals with the pre-selection of relevant information from the dataset maximizing information and reducing noise. Quantum-inspired techniques such as Tensor Networks actually provide really impressive results using just classical resources.\nWe have seen how the evolution from Adiabatic Quantum Computing, though trainable circuits up to Quantum Machine Learning has a rational behind it for overcoming existing limitations at each step. As long as we focus on solving issues, progress is guaranteed and who knows, maybe some day, quantum advantage.\n\n\n\n\nM√ºcke, Sascha, Raoul Heese, Sabine M√ºller, Moritz Wolter, and Nico Piatkowski. 2023. ‚ÄúFeature Selection on Quantum Computers.‚Äù Quantum Machine Intelligence 5 (1): 11.",
    "crumbs": [
      "Quantum Machine Learning",
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aharonov, Dorit, Wim Van Dam, Julia Kempe, Zeph Landau, Seth Lloyd, and\nOded Regev. 2008. ‚ÄúAdiabatic Quantum Computation Is Equivalent to\nStandard Quantum Computation.‚Äù SIAM Review 50 (4):\n755‚Äì87.\n\n\nAlbash, Tameem, and Daniel A. Lidar. 2018. ‚ÄúAdiabatic Quantum\nComputation.‚Äù Reviews of Modern Physics 90 (1). https://doi.org/10.1103/revmodphys.90.015002.\n\n\nAnastasiou, Panagiotis G, Yanzhu Chen, Nicholas J Mayhall, Edwin Barnes,\nand Sophia E Economou. 2024. ‚ÄúTETRIS-ADAPT-VQE: An Adaptive\nAlgorithm That Yields Shallower, Denser Circuit\nAns√§tze.‚Äù Physical Review Research 6 (1):\n013254.\n\n\nAugustino, Brandon, Giacomo Nannicini, Tam√°s Terlaky, and Luis Zuluaga.\n2025. ‚ÄúSolving the Semidefinite Relaxation of QUBOs in Matrix\nMultiplication Time, and Faster with a Quantum Computer.‚Äù https://arxiv.org/abs/2301.04237.\n\n\nBiamonte, Jacob, and Ville Bergholm. 2017. ‚ÄúTensor Networks in a\nNutshell.‚Äù https://arxiv.org/abs/1708.00006.\n\n\nBillionnet, Alain, and Sourour Elloumi. 2007. ‚ÄúUsing a Mixed\nInteger Quadratic Programming Solver for the Unconstrained Quadratic 0-1\nProblem.‚Äù Mathematical Programming 109: 55‚Äì68.\n\n\nCai, Jun, William G. Macready, and Aidan Roy. 2014. ‚ÄúA Practical\nHeuristic for Finding Graph Minors.‚Äù https://arxiv.org/abs/1406.2741.\n\n\nChuang, Isaac L, Neil Gershenfeld, and Mark Kubinec. 1998.\n‚ÄúExperimental Implementation of Fast Quantum Searching.‚Äù\nPhysical Review Letters 80 (15): 3408.\n\n\nCirac, Juan I, and Peter Zoller. 1995. ‚ÄúQuantum Computations with\nCold Trapped Ions.‚Äù Physical Review Letters 74 (20):\n4091.\n\n\nCrain, Stephen, Clinton Cahall, Geert Vrijsen, Emma E Wollman, Matthew D\nShaw, Varun B Verma, Sae Woo Nam, and Jungsang Kim. 2019.\n‚ÄúHigh-Speed Low-Crosstalk Detection of a 171Yb+ Qubit Using\nSuperconducting Nanowire Single Photon Detectors.‚Äù\nCommunications Physics 2 (1): 97.\n\n\nDate, Prasanna, Davis Arthur, and Lauren Pusey-Nazzaro. 2021.\n‚ÄúQUBO Formulations for Training Machine Learning Models.‚Äù\nScientific Reports 11 (1): 10029.\n\n\nDiVincenzo, David P. 1997. ‚ÄúTopics in Quantum Computers.‚Äù\nIn Mesoscopic Electron Transport, 657‚Äì77. Springer.\n\n\nEgger, Daniel J, Jakub Mareƒçek, and Stefan Woerner. 2021.\n‚ÄúWarm-Starting Quantum Optimization.‚Äù Quantum 5:\n479.\n\n\nFarhi, Edward, Jeffrey Goldstone, and Sam Gutmann. 2000. ‚ÄúA\nNumerical Study of the Performance of a Quantum Adiabatic Evolution\nAlgorithm for Satisfiability.‚Äù arXiv Preprint\nQuant-Ph/0007071.\n\n\n‚Äî‚Äî‚Äî. 2014. ‚ÄúA Quantum Approximate Optimization Algorithm.‚Äù\narXiv Preprint arXiv:1411.4028.\n\n\nGokhale, Pranav, Teague Tomesh, Martin Suchara, and Frederic T. Chong.\n2021. ‚ÄúFaster and More Reliable Quantum SWAPs via Native\nGates.‚Äù https://arxiv.org/abs/2109.13199.\n\n\nGrimsley, Harper R, Sophia E Economou, Edwin Barnes, and Nicholas J\nMayhall. 2019. ‚ÄúAn Adaptive Variational Algorithm for Exact\nMolecular Simulations on a Quantum Computer.‚Äù Nature\nCommunications 10 (1): 3007.\n\n\nHavl√≠ƒçek, Vojtƒõch, Antonio D. C√≥rcoles, Kristan Temme, Aram W. Harrow,\nAbhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. 2019.\n‚ÄúSupervised Learning with Quantum-Enhanced Feature Spaces.‚Äù\nNature 567 (7747): 209‚Äì12. https://doi.org/10.1038/s41586-019-0980-2.\n\n\nHubregtsen, Thomas, David Wierichs, Elies Gil-Fuster, Peter-Jan H. S.\nDerks, Paul K. Faehrmann, and Johannes Jakob Meyer. 2022.\n‚ÄúTraining Quantum Embedding Kernels on Near-Term Quantum\nComputers.‚Äù Physical Review A 106 (4). https://doi.org/10.1103/physreva.106.042431.\n\n\nKoch, Jens, Terri M Yu, Jay Gambetta, Andrew A Houck, David I Schuster,\nJohannes Majer, Alexandre Blais, Michel H Devoret, Steven M Girvin, and\nRobert J Schoelkopf. 2007. ‚ÄúCharge-Insensitive Qubit Design\nDerived from the Cooper Pair Box.‚Äù Physical Review A‚ÄîAtomic,\nMolecular, and Optical Physics 76 (4): 042319.\n\n\nLeone, Lorenzo, Salvatore FE Oliviero, Lukasz Cincio, and Marco Cerezo.\n2024. ‚ÄúOn the Practical Usefulness of the Hardware Efficient\nAnsatz.‚Äù Quantum 8: 1395.\n\n\nLevine, Harry, Alexander Keesling, Giulia Semeghini, Ahmed Omran, Tout T\nWang, Sepehr Ebadi, Hannes Bernien, et al. 2019. ‚ÄúParallel\nImplementation of High-Fidelity Multiqubit Gates with Neutral\nAtoms.‚Äù Physical Review Letters 123 (17): 170503.\n\n\nLucas, Andrew. 2014. ‚ÄúIsing Formulations of Many NP\nProblems.‚Äù Frontiers in Physics 2: 5.\n\n\nMcClean, Jarrod R, Sergio Boixo, Vadim N Smelyanskiy, Ryan Babbush, and\nHartmut Neven. 2018. ‚ÄúBarren Plateaus in Quantum Neural Network\nTraining Landscapes.‚Äù Nature Communications 9 (1): 4812.\n\n\nMontanaro, Ashley. 2020. ‚ÄúQuantum Speedup of Branch-and-Bound\nAlgorithms.‚Äù Physical Review Research 2 (1): 013056.\n\n\nM√ºcke, Sascha, Raoul Heese, Sabine M√ºller, Moritz Wolter, and Nico\nPiatkowski. 2023. ‚ÄúFeature Selection on Quantum Computers.‚Äù\nQuantum Machine Intelligence 5 (1): 11.\n\n\nMyerson, AH, DJ Szwer, SC Webster, DTC Allcock, MJ Curtis, G Imreh, JA\nSherman, DN Stacey, AM Steane, and DM Lucas. 2008. ‚ÄúHigh-Fidelity\nReadout of Trapped-Ion Qubits.‚Äù Physical Review Letters\n100 (20): 200502.\n\n\nNakamura, Y, Yu A Pashkin, and Jaw Shen Tsai. 2001. ‚ÄúRabi\nOscillations in a Josephson-Junction Charge Two-Level System.‚Äù\nPhysical Review Letters 87 (24): 246601.\n\n\nNielsen, Michael A, and Isaac L Chuang. 2010. Quantum Computation\nand Quantum Information. Cambridge university press.\n\n\nPeruzzo, Alberto, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi\nZhou, Peter J Love, Al√°n Aspuru-Guzik, and Jeremy L O‚Äôbrien. 2014.\n‚ÄúA Variational Eigenvalue Solver on a Photonic Quantum\nProcessor.‚Äù Nature Communications 5 (1): 4213.\n\n\nPonce, Moises, Rebekah Herrman, Phillip C Lotshaw, Sarah Powers, George\nSiopsis, Travis Humble, and James Ostrowski. 2025. ‚ÄúGraph\nDecomposition Techniques for Solving Combinatorial Optimization Problems\nwith Variational Quantum Algorithms.‚Äù Quantum Information\nProcessing 24 (2): 60.\n\n\nRebentrost, Patrick, Masoud Mohseni, and Seth Lloyd. 2014.\n‚ÄúQuantum Support Vector Machine for Big Data\nClassification.‚Äù Physical Review Letters 113 (13). https://doi.org/10.1103/physrevlett.113.130503.\n\n\nSchuld, Maria, Ilya Sinayskiy, and Francesco Petruccione. 2014.\n‚ÄúThe Quest for a Quantum Neural Network.‚Äù Quantum\nInformation Processing 13: 2567‚Äì86.\n\n\nShor, Peter W. 1994. ‚ÄúAlgorithms for Quantum Computation: Discrete\nLogarithms and Factoring.‚Äù In Proceedings 35th Annual\nSymposium on Foundations of Computer Science, 124‚Äì34. Ieee.\n\n\nSim, Sukin, Peter D Johnson, and Al√°n Aspuru-Guzik. 2019.\n‚ÄúExpressibility and Entangling Capability of Parameterized Quantum\nCircuits for Hybrid Quantum-Classical Algorithms.‚Äù Advanced\nQuantum Technologies 2 (12): 1900070.\n\n\nWauters, Matteo M, Emanuele Panizon, Glen B Mbeng, and Giuseppe E\nSantoro. 2020. ‚ÄúReinforcement-Learning-Assisted Quantum\nOptimization.‚Äù Physical Review Research 2 (3): 033446.\n\n\nWeidenfeller, Johannes, Lucia C. Valor, Julien Gacon, Caroline Tornow,\nLuciano Bello, Stefan Woerner, and Daniel J. Egger. 2022. ‚ÄúScaling\nof the Quantum Approximate Optimization Algorithm on Superconducting\nQubit Based Hardware.‚Äù Quantum 6 (December): 870. https://doi.org/10.22331/q-2022-12-07-870.\n\n\nWocjan, Pawel, and Shengyu Zhang. 2006. ‚ÄúSeveral Natural\nBQP-Complete Problems.‚Äù https://arxiv.org/abs/quant-ph/0606179.\n\n\nWolf, Ronald de. 2023. ‚ÄúQuantum Computing: Lecture Notes.‚Äù\nhttps://arxiv.org/abs/1907.09415.\n\n\nYanofsky, Noson S, and Mirco A Mannucci. 2008. Quantum Computing for\nComputer Scientists. Cambridge University Press.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "libraries.html",
    "href": "libraries.html",
    "title": "Appendix A ‚Äî Software packages",
    "section": "",
    "text": "QuTip: Quantum Toolbox for simulating the dynamics of quantum systems.\nQiskit: IBM‚Äôs development kit, the indisputable standard for gate-based quantum software development.\nOcean SDK: D-Wave proprietary SDK. Really useful for problem posing and creating the first instances of our particular instances before moving to annealers, gate-based devices or whatever might be the final solver.\nQuEra‚Äôs Bloqade for simulating Neutral Atom platforms both in Julia and Python\nAmazon Braket for simulation and sending jobs to Amazon Braket service.\nPennylane SDK for Quantum Machine Learning, among other topics.\nQUIMB for those wanting to get deep into Tensor Network stuff.\nsQUlearn focused on QML with great documentation\nTensorFlow Quantum looking into interoperability between the classical and quantum world.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Software packages</span>"
    ]
  },
  {
    "objectID": "hybridsolvers.html",
    "href": "hybridsolvers.html",
    "title": "Appendix B ‚Äî Hybrid solvers",
    "section": "",
    "text": "Hybrid Annealers\nEven though we talked about Quantum Computers and how we can use them to solve specific problems, complex problems; what percentage of the problem is solved by each device (classical and quantum) depends on the specific problem we would like to solve.\nThink about Shor‚Äôs algorithm. It revolutionized the field and worried a couple of CSO/CISOs in the way. But it only changes one little step from the conventional Prime Factorization algorithm. Thanks to that tep there is a significant speedup in the algorithm taking it down from exponential order to polynomial one.\nA more detailed explanation can be found in IBM‚Äôs documentation but this type of hybrid approaches can really benefit from NISQ era hardware in other regimes like optimization.\nOur approach to optimization is already limited to the QUBO formulation but this is an imposed restrictions given our hardware. We may want to find solutions to problems whose variables can be integers, for example. This type of problems are classically solved by using branch and bound strategies. Taken a problem of the shape\n\\[\n\\max_{x\\in\\mathbb{R}^n} f(x) \\\\\n\\quad  \\text{s.t.  } c(c) \\le 0 \\\\\n    \\quad \\quad x_i \\in \\mathbb{Z}, i \\in I\n\\]\nwhere \\(f\\) and \\(c\\) are nonlinear and non-convex functions. This makes our target problem a non-convex mixed-integer nonlinear problem (MINLP). Having guarantee that we not only reach a solution but the solution is indeed a demanding task.\nGood news is that non-convex QUBOs can be made convex as follows\n\\[\n\\max_{x\\in\\{0,1\\}^n} x^T (Q + diag(s))x + s^Tx\n\\]\nwhere \\(S\\) refers to a shift to be obtained. Different methods can be found to obtain said shift, such as solving and auxiliary Semidefinite Optimization Problem (SDP) that returns larges eigenvalue of \\(Q\\) (Billionnet and Elloumi 2007). This shift computation can also be tackled by quantum computers mixing classical and quantum devices in different ways (Montanaro 2020; Augustino et al. 2025). Finding the right balance between classical and quantum resources should definitely boost processes without requiring the whole fault-tolerant service at scale.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Hybrid solvers</span>"
    ]
  },
  {
    "objectID": "hybridsolvers.html#hybrid-annealers",
    "href": "hybridsolvers.html#hybrid-annealers",
    "title": "Appendix B ‚Äî Hybrid solvers",
    "section": "",
    "text": "D-Wave‚Äôs Hybrid solvers\nD-Wave was one of the first to leverage this complex workflows involving classical and quantum resources and came with a practical solution anybody used to their service can use. They released a set of hybrid solvers. These solvers are particularly convenient when instead of Binary Quadratic Models we would like to formulate our problem as:\n\nConstrained Quadratic Models: problems that might include real, integer and/or binary variables and one or more constraints.\nDiscrete Quadratic Models: problems containing discrete variables in their core variable formulation.\n\nAs you can see, there is no silver bullet, but being aware of all the posibilites technology brings may make a difference when facing our business use cases for that extra percentage that makes us competitive.\n\n\nQuantagonia\nDue to their change on pricing strategy, we can no longer mess around with their devices as much as we would like to so, an interesting alternative is the service provided by Quantagonia.\n\n\n\nQuantagonia Dashboard\n\n\n\n\n\n\nAugustino, Brandon, Giacomo Nannicini, Tam√°s Terlaky, and Luis Zuluaga. 2025. ‚ÄúSolving the Semidefinite Relaxation of QUBOs in Matrix Multiplication Time, and Faster with a Quantum Computer.‚Äù https://arxiv.org/abs/2301.04237.\n\n\nBillionnet, Alain, and Sourour Elloumi. 2007. ‚ÄúUsing a Mixed Integer Quadratic Programming Solver for the Unconstrained Quadratic 0-1 Problem.‚Äù Mathematical Programming 109: 55‚Äì68.\n\n\nMontanaro, Ashley. 2020. ‚ÄúQuantum Speedup of Branch-and-Bound Algorithms.‚Äù Physical Review Research 2 (1): 013056.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Hybrid solvers</span>"
    ]
  }
]